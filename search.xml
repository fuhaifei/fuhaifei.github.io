<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>博客建成</title>
    <url>/2021/07/01/%E5%8D%9A%E5%AE%A2%E5%BB%BA%E6%88%90/</url>
    <content><![CDATA[<h3 id="为什么要搭建这个博客"><a href="#为什么要搭建这个博客" class="headerlink" title="为什么要搭建这个博客"></a>为什么要搭建这个博客</h3><ol>
<li>项目不会做，不知道干啥，就来搞点没用的</li>
<li>想把笔记放在网站上，起到保存和督促的作用</li>
</ol>
<h3 id="当前博客的计划"><a href="#当前博客的计划" class="headerlink" title="当前博客的计划"></a>当前博客的计划</h3><ol>
<li>每周末把这周的笔记整理发布到网站上</li>
<li>其他感想之类的废话，还有待思考</li>
</ol>
<p>总结：闲的没事干，整点歪门邪道</p>
]]></content>
      <categories>
        <category>日常扯淡</category>
      </categories>
  </entry>
  <entry>
    <title>DataLoaders和DataSets使用总结</title>
    <url>/2021/07/09/DL%E7%BC%96%E7%A8%8B/DataLoaders%E5%92%8CDataSets%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="DataLoaders和DataSets使用总结"><a href="#DataLoaders和DataSets使用总结" class="headerlink" title="DataLoaders和DataSets使用总结"></a>DataLoaders和DataSets使用总结</h1><p>通过 <code>torch.utils.data.Dataset</code>类定义数据集，通过<code>torch.utils.data.DataLoader</code>与<code>Dataset</code>配合定义数据加载方式</p>
<p><code>torch.utils.data.Dataset</code>主要参数:</p>
<ol>
<li>dataset 指定构造的数据集，一般是数据+label的元组</li>
<li>shuffle 指定是否打乱</li>
<li>sampler 指定采样器，采取某种方式遍历数据（顺序，随机等）</li>
<li>collate_fn (callable, optional):将数据形成一个batch的tensor（在某些时候需要自定义）<span id="more"></span>
以加载minist数据集为例，基本的使用流程：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先加载数据集（pytorch现有的数据集） 返回的是DataSet对象</span></span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"><span class="comment"># 然后构造DataLoader对象 返回的是可迭代的DataLoader对象</span></span><br><span class="line">train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 通过for循环遍历进行训练</span></span><br><span class="line"><span class="keyword">for</span> batch_features,batch_labels <span class="keyword">in</span> train_iter:</span><br></pre></td></tr></table></figure>
<p>基本遍历原理</p>
<ul>
<li>通过sampler抽样index(默认为顺序抽样)，根据抽出的index从dataset中获取元素（getitem），调用自身的collate_fn方法，将原始数据转化为batch形式的tensor，返回。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># dataloader遍历的next方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__next__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.num_workers == <span class="number">0</span>:  </span><br><span class="line">            indices = <span class="built_in">next</span>(self.sample_iter)  <span class="comment"># Sampler</span></span><br><span class="line">            batch = self.collate_fn([self.dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices]) <span class="comment"># Dataset</span></span><br><span class="line">            <span class="keyword">if</span> self.pin_memory:</span><br><span class="line">                batch = _utils.pin_memory.pin_memory_batch(batch)</span><br><span class="line">            <span class="keyword">return</span> batch</span><br><span class="line"><span class="comment"># dataset 以[] 形式访问的方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br></pre></td></tr></table></figure>
<h2 id="1-自定义数据集"><a href="#1-自定义数据集" class="headerlink" title="1. 自定义数据集"></a>1. 自定义数据集</h2><h4 id="1-1-继承dataset类"><a href="#1-1-继承dataset类" class="headerlink" title="1.1 继承dataset类"></a>1.1 继承dataset类</h4><p>需要重写<code>init(),len(),getitem()</code>方法，以word2vec中负采样数据集生成为例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义数据读取类，可结合dataloader进行数据批量读取</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span>(<span class="params">torch.utils.data.Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, centers, contexts, negatives</span>):</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(centers) == <span class="built_in">len</span>(contexts) == <span class="built_in">len</span>(negatives)</span><br><span class="line">        self.centers = centers</span><br><span class="line">        self.contexts = contexts</span><br><span class="line">        self.negatives = negatives</span><br><span class="line">	<span class="comment"># 返回当前行数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> (self.centers[index], self.contexts[index], self.negatives[index])</span><br><span class="line">    <span class="comment"># 返回数据长度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.centers)</span><br></pre></td></tr></table></figure>
<h4 id="1-2-定义collate-fn方法"><a href="#1-2-定义collate-fn方法" class="headerlink" title="1.2 定义collate_fn方法"></a>1.2 定义collate_fn方法</h4><p>默认的collate_fn方法，必须保证每个训练数据的长度相同，通过自定义collate_fn方法解决这一问题</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_collate_fn</span>(<span class="params">data</span>):</span></span><br><span class="line">	<span class="comment"># 保证所有数据一致的统一长度</span></span><br><span class="line">    max_len = <span class="number">500</span></span><br><span class="line">    center_words = []</span><br><span class="line">    context_negatives = []</span><br><span class="line">    masks = []</span><br><span class="line">    labels = []</span><br><span class="line">    <span class="keyword">for</span> center, contexts, negatives <span class="keyword">in</span> data:</span><br><span class="line">        cur_len = <span class="built_in">len</span>(contexts) + <span class="built_in">len</span>(negatives)</span><br><span class="line">        center_words.append([center])</span><br><span class="line">        <span class="comment"># 对数据进行截断或者延长</span></span><br><span class="line">        <span class="keyword">if</span> cur_len &gt; max_len:</span><br><span class="line">        	context_negatives.append((contexts + negatives)[<span class="number">0</span>:max_len])</span><br><span class="line">        	labels.append([<span class="number">1</span>] * <span class="built_in">len</span>(contexts) + [<span class="number">0</span>] * (max_len - <span class="built_in">len</span>(contexts)))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">        	context_negatives.append(contexts + negatives + [<span class="number">0</span>] * (max_len - cur_len))</span><br><span class="line">        	labels.append([<span class="number">1</span>] * <span class="built_in">len</span>(contexts) + [<span class="number">0</span>] * (max_len - <span class="built_in">len</span>(contexts)))</span><br><span class="line">    <span class="keyword">return</span>  torch.tensor(center_words), torch.tensor(context_negatives)</span><br><span class="line"><span class="comment"># 使用即可</span></span><br><span class="line">train_iter = Data.DataLoader(MyDataset(all_centers, all_contexts, all_negatives), collate_fn = batchify, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="2-自定义sampler"><a href="#2-自定义sampler" class="headerlink" title="2.自定义sampler"></a>2.自定义sampler</h2><p>待补充</p>
]]></content>
      <categories>
        <category>ML/DL编程总结</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch Scatter和gather函数使用</title>
    <url>/2021/07/09/DL%E7%BC%96%E7%A8%8B/Scatter%E5%92%8Cgather%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h3 id="Scatter函数"><a href="#Scatter函数" class="headerlink" title="Scatter函数"></a>Scatter函数</h3><blockquote>
<p>scatter_(<em>dim</em>, <em>index</em>, <em>src</em>, <em>reduce=None</em>) → Tensor</p>
<p>Writes all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor. For each value in <code>src</code>, its output index is specified by its index in <code>src</code> for <code>dimension != dim</code> and by the corresponding value in <code>index</code> for <code>dimension = dim</code>.</p>
</blockquote>
<p>简单理解就是将 src 张量中的元素散落到 self 张量中，具体选择哪个元素，选择的元素散落到哪个位置由index张量决定，具体的映射规则为:</p>
<figure class="highlight plaintext"><figcaption><span>三维张量为例</span></figcaption><table><tr><td class="code"><pre><span class="line"># 其中 i,j,k 为index张量中元素坐标。</span><br><span class="line">self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0</span><br><span class="line">self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1</span><br><span class="line">self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><ul>
<li><strong>dim(int)</strong> 指index数组元素替代的坐标（dim = 0 替代src中的横坐标）</li>
<li><strong>index (LongTensor)</strong> 可以为空，最大与src张量形状相同</li>
<li><strong>src(Tensor or float)</strong> 源张量</li>
<li><strong>reduce</strong> 聚集函数(src替换元素与self中被替换元素执行的操作，默认是替代，可以进行add,multiply等操作)</li>
</ul>
<p>具体例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>src = torch.arange(<span class="number">1</span>, <span class="number">11</span>).reshape((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>src</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>index = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros(<span class="number">3</span>, <span class="number">5</span>, dtype=src.dtype).scatter_(<span class="number">0</span>, index, src)</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure>
<p>以上例子中scatter函数执行的操作等价于：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 遍历index，在dim = 0 时,替换i</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(index.shape[<span class="number">0</span>]):</span><br><span class="line">	<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(index.shape[<span class="number">1</span>]):</span><br><span class="line">		self[index[i][j]][j] = src[i][j]</span><br></pre></td></tr></table></figure>
<h3 id="gather-函数"><a href="#gather-函数" class="headerlink" title="gather() 函数"></a>gather() 函数</h3><blockquote>
<p>Gathers values along an axis specified by dim.</p>
<p>沿着某条轴对元素进行聚集</p>
</blockquote>
<p>scatter的逆操作，挑选源张量的某些元素，放置到新张量中，具体映射规则与scatter类似：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out[i][j][k] = <span class="built_in">input</span>[index[i][j][k]][j][k]  <span class="comment"># if dim == 0</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][index[i][j][k]][k]  <span class="comment"># if dim == 1</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][j][index[i][j][k]]  <span class="comment"># if dim == 2</span></span><br></pre></td></tr></table></figure>
<h4 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h4><ul>
<li><strong>input</strong> 输入张量</li>
<li><strong>dim</strong> 聚集的轴</li>
<li><strong>index</strong> 聚集的index张量</li>
</ul>
<p>具体例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.gather(t, <span class="number">1</span>, torch.tensor([[<span class="number">0</span>, <span class="number">1</span>]]))</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>
<p>以上例子中gather()函数等价于scatter()函数的操作:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 遍历index，在dim = 1 时,替换j</span><br><span class="line">for i in range(index.shape[0]):</span><br><span class="line">	for j in range(index.shape[1]):</span><br><span class="line">		result[i][j] = input[i][index[i][j]]</span><br></pre></td></tr></table></figure>
<h3 id="scatter-vs-gather"><a href="#scatter-vs-gather" class="headerlink" title="scatter vs gather"></a>scatter vs gather</h3><p>相同点:</p>
<ul>
<li>scatter 和 gather 函数都是根据index数组从 src/input 源张量中选取指定元素</li>
</ul>
<p>不同点：</p>
<ul>
<li>scatter选取元素放置到目标张量中，放置位置由<code>index[i][j]</code>决定</li>
<li>gather选取元素放置组成新的张量，选取位置由<code>index[i][j]</code>决定</li>
</ul>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><h4 id="1-使用scatter函数将向量转化为one-hot形式"><a href="#1-使用scatter函数将向量转化为one-hot形式" class="headerlink" title="1. 使用scatter函数将向量转化为one-hot形式"></a>1. 使用scatter函数将向量转化为one-hot形式</h4><p>以转化为<code>n x 10</code>的n个10维行one-hot向量为例子:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([<span class="number">9</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t.view(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">tensor([[<span class="number">9</span>],</span><br><span class="line">        [<span class="number">8</span>],</span><br><span class="line">        [<span class="number">9</span>],</span><br><span class="line">        [<span class="number">5</span>],</span><br><span class="line">        [<span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>one_hot = torch.zeros(t.shape[<span class="number">0</span>],<span class="number">10</span>).scatter(<span class="number">1</span>,t.view(-<span class="number">1</span>,<span class="number">1</span>),<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ont_hot</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
<p>使用argmax转化回去:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; one_hot.argmax(dim = 1,keepdim = True)</span><br><span class="line">tensor([[9],</span><br><span class="line">        [8],</span><br><span class="line">        [9],</span><br><span class="line">        [5],</span><br><span class="line">        [6],</span><br><span class="line">        [7]])</span><br></pre></td></tr></table></figure>
<h3 id="待补充"><a href="#待补充" class="headerlink" title="待补充"></a>待补充</h3>]]></content>
      <categories>
        <category>ML/DL编程总结</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>model_selection模块进行交叉验证</title>
    <url>/2021/07/09/DL%E7%BC%96%E7%A8%8B/model-selection%E6%A8%A1%E5%9D%97%E8%BF%9B%E8%A1%8C%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/</url>
    <content><![CDATA[<p>为了避免模型过拟合，通过添加<strong>验证集</strong>，模型训练完成后使用<strong>验证集</strong>验证模型的效果后，再对测试集进行测试。</p>
<h3 id="训练集和测试集划分"><a href="#训练集和测试集划分" class="headerlink" title="训练集和测试集划分"></a>训练集和测试集划分</h3><p>使用train_test_split函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">&gt;&gt; x_data, y_data = datasets.load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 划分测试集和训练集</span></span><br><span class="line">&gt;&gt; x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = <span class="number">0.2</span>, random_state = <span class="number">42</span>)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(x_train.shape)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(x_test.shape)</span><br><span class="line">(<span class="number">120</span>, <span class="number">4</span>)</span><br><span class="line">(<span class="number">30</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">&gt;&gt; clf = svm.SVC(kernel=<span class="string">&#x27;linear&#x27;</span>, C=<span class="number">1</span>).fit(X_train, y_train)</span><br><span class="line">&gt;&gt; clf.score(X_test, y_test)</span><br><span class="line"><span class="number">1.0</span></span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h3 id="cross-validation-交叉验证"><a href="#cross-validation-交叉验证" class="headerlink" title="cross validation 交叉验证"></a>cross validation 交叉验证</h3><p><img src="sklearn model_selection模块进行交叉验证/image-20210709165817836.png" alt="image-20210709165817836"></p>
<h4 id="cross-val-score函数进行模型交叉验证评价"><a href="#cross-val-score函数进行模型交叉验证评价" class="headerlink" title="cross_val_score函数进行模型交叉验证评价"></a>cross_val_score函数进行模型交叉验证评价</h4><p>简单使用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">&gt;&gt; clf = svm.SVC(kernel=<span class="string">&#x27;linear&#x27;</span>, C=<span class="number">1</span>, random_state=<span class="number">42</span>)</span><br><span class="line">&gt;&gt; scores = cross_val_score(clf, x_data, y_data, cv = <span class="number">5</span>)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(scores)</span><br><span class="line">[<span class="number">0.96666667</span> <span class="number">1.</span>         <span class="number">0.96666667</span> <span class="number">0.96666667</span> <span class="number">1.</span>        ]</span><br></pre></td></tr></table></figure>
<p>通过scoring参数，自定义评估函数，如果不显示指定，则调用estimator的默认score函数</p>
<p><img src="sklearn model_selection模块进行交叉验证/image-20210604090841624.png" alt="image-20210604090841624"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; scores = cross_val_score(clf, x_data, y_data, cv = <span class="number">5</span>, scoring = <span class="string">&#x27;f1_macro&#x27;</span>)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(scores)</span><br><span class="line">[<span class="number">0.96658312</span> <span class="number">1.</span>         <span class="number">0.96658312</span> <span class="number">0.96658312</span> <span class="number">1.</span>        ]</span><br></pre></td></tr></table></figure>
<p>cv参数不止可以传入k折的折数，还可以传入<strong>cross-validation generator or an iterable</strong>等</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 传入交叉验证生成器</span></span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit</span><br><span class="line">&gt;&gt; cv = ShuffleSplit(n_splits=<span class="number">5</span>, test_size=<span class="number">0.2</span>, random_state= <span class="number">42</span>)</span><br><span class="line">&gt;&gt; cross_val_score(clf, x_data, y_data, cv=cv)</span><br><span class="line">array([<span class="number">1.</span>        , <span class="number">1.</span>        , <span class="number">0.96666667</span>, <span class="number">0.93333333</span>, <span class="number">0.96666667</span>])</span><br><span class="line"><span class="comment"># 传入index生成迭代器（教程代码）</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">custom_cv_2folds</span>(<span class="params">X</span>):</span></span><br><span class="line"><span class="meta">... </span>    n = X.shape[<span class="number">0</span>]</span><br><span class="line"><span class="meta">... </span>    i = <span class="number">1</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">while</span> i &lt;= <span class="number">2</span>:</span><br><span class="line"><span class="meta">... </span>        idx = np.arange(n * (i - <span class="number">1</span>) / <span class="number">2</span>, n * i / <span class="number">2</span>, dtype=<span class="built_in">int</span>)</span><br><span class="line"><span class="meta">... </span>        <span class="keyword">yield</span> idx, idx</span><br><span class="line"><span class="meta">... </span>        i += <span class="number">1</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>custom_cv = custom_cv_2folds(X)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cross_val_score(clf, X, y, cv=custom_cv)</span><br><span class="line">array([<span class="number">1.</span>        , <span class="number">0.973</span>...])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ML/DL编程总结</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>pytroch如何对梯度追踪张量进行inplace操作</title>
    <url>/2021/07/09/DL%E7%BC%96%E7%A8%8B/pytorch%E5%A6%82%E4%BD%95%E5%AF%B9%E6%A2%AF%E5%BA%A6%E8%BF%BD%E8%B8%AA%E5%BC%A0%E9%87%8F%E8%BF%9B%E8%A1%8Cinplace%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h3 id="如何对梯度追踪张量进行inplace操作"><a href="#如何对梯度追踪张量进行inplace操作" class="headerlink" title="如何对梯度追踪张量进行inplace操作"></a>如何对梯度追踪张量进行inplace操作</h3><blockquote>
<p>别忘了梯度追踪张量必须为float</p>
<p>Only Tensors of floating point and complex dtype can require gradients</p>
</blockquote>
<h4 id="问题来源"><a href="#问题来源" class="headerlink" title="问题来源"></a>问题来源</h4><p>当对设置了requires_grad=True的张量进行原地操作时，pytorch会抛出运行错误：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt; X = torch.rand((3,4),requires_grad = True)</span><br><span class="line">&gt;&gt; X.fill_(0)</span><br><span class="line">RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p>什么情况下会进行原地操作（当前遇到的）:</p>
<ul>
<li>模型参数初始化</li>
<li>自定义实现梯度下降法</li>
<li>避免梯度积累，每次训练进行梯度清零</li>
</ul>
<h4 id="如何解决"><a href="#如何解决" class="headerlink" title="如何解决"></a>如何解决</h4><ol>
<li><p>使用 .data 或 .detach() 方法，获得原张量的同内存但不进行梯度追踪的张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="built_in">print</span>(X)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(X.data.requires_grad)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(X.detach().requires_grad)</span><br><span class="line">tensor([[<span class="number">0.2407</span>, <span class="number">0.3222</span>, <span class="number">0.4246</span>, <span class="number">0.3125</span>],</span><br><span class="line">        [<span class="number">0.1386</span>, <span class="number">0.7018</span>, <span class="number">0.1751</span>, <span class="number">0.0617</span>],</span><br><span class="line">        [<span class="number">0.3467</span>, <span class="number">0.5178</span>, <span class="number">0.2557</span>, <span class="number">0.9855</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>使用该不追踪梯度张量替代执行inplace操作</p>
<pre><code># 自定义实现梯度下降
net.weight.detach().sub_(net.weight.grad,alpha = lr)
net.bias.detach().sub_(net.bias.grad,alpha = lr)
</code></pre></li>
<li><p>.data 与 .detach()的区别在于</p>
<ul>
<li><p>.data 的inplace修改自动求导不监控，如果修改了梯度追踪节点的值，可能导致求导结果错误</p>
</li>
<li><p>.detach() 的inplace修改自动求导同样监控，如果修改了梯度追踪节点的值再进行求导，系统会报错</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<pre><code> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 正常使用</span><br><span class="line">&gt;&gt; X = torch.tensor([[5, 1, 9],[6, 5, 7]],requires_grad = True,dtype = torch.float)</span><br><span class="line">&gt;&gt; print(X)</span><br><span class="line">&gt;&gt; Y = X ** 2</span><br><span class="line">&gt;&gt; Y.sum().backward()</span><br><span class="line"># 梯队为2X</span><br><span class="line">&gt;&gt; print(X.grad)</span><br><span class="line">tensor([[5., 1., 9.],</span><br><span class="line">        [6., 5., 7.]], requires_grad=True)</span><br><span class="line">tensor([[10.,  2., 18.],</span><br><span class="line">        [12., 10., 14.]])</span><br></pre></td></tr></table></figure>

 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 使用data</span><br><span class="line">&gt;&gt; X = torch.tensor([[5, 1, 9],[6, 5, 7]],requires_grad = True,dtype = torch.float)</span><br><span class="line">&gt;&gt; print(X)</span><br><span class="line">&gt;&gt;  = X ** 2</span><br><span class="line">&gt;&gt; X.data *= 2</span><br><span class="line">&gt;&gt; Y.sum().backward()</span><br><span class="line">&gt;&gt; print(Y)</span><br><span class="line"># 梯队为(梯度相较于正常的放大了两倍)</span><br><span class="line">&gt;&gt; print(X.grad)</span><br><span class="line">tensor([[5., 1., 9.],</span><br><span class="line">        [6., 5., 7.]], requires_grad=True)</span><br><span class="line">tensor([[25.,  1., 81.],</span><br><span class="line">        [36., 25., 49.]], grad_fn=&lt;PowBackward0&gt;)</span><br><span class="line">tensor([[20.,  4., 36.],</span><br><span class="line">        [24., 20., 28.]])</span><br></pre></td></tr></table></figure>

 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 使用detach()</span><br><span class="line">X = torch.tensor([[5, 1, 9],[6, 5, 7]],requires_grad = True,dtype = torch.float)</span><br><span class="line">print(X)</span><br><span class="line">Y = X ** 2</span><br><span class="line">X.detach().add_(100)</span><br><span class="line">Y.sum().backward()</span><br><span class="line">print(Y)</span><br><span class="line"># 梯队为(梯度相较于正常的放大了两倍)</span><br><span class="line">print(X.grad)</span><br><span class="line">直接报错:</span><br><span class="line">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2, 3]] is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</span><br></pre></td></tr></table></figure>
</code></pre><ul>
<li>detach() 与 detach_()的区别<ul>
<li>detach_()不仅获得未追踪梯度同内存tensor，还将当前节点设置为叶子节点（即求梯度是求到当前节点停止，不在向前继续计算，截断方向传播计算图）</li>
</ul>
</li>
</ul>
<ol>
<li><p>使用pytorch的init模块初始化模块参数</p>
<blockquote>
<p>torch.nn.init+初始化函数名</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正态分布初始化</span></span><br><span class="line">net = nn.Linear(feature_num, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    nn.init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="自动求导相关属性"><a href="#自动求导相关属性" class="headerlink" title="自动求导相关属性"></a>自动求导相关属性</h3><ul>
<li><p><strong>requires_grad</strong> 是否进行梯度追踪</p>
<blockquote>
<p>Is <code>True</code> if gradients need to be computed for this Tensor, <code>False</code> otherwise.</p>
</blockquote>
</li>
<li><p><strong>grad</strong> 存储梯度的tensor数组，未进行反向传播计算时为None，多次计算梯度会进行累加</p>
<blockquote>
<p>This attribute is <code>None</code> by default and becomes a Tensor the first time a call to <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.backward"><code>backward()</code></a> computes gradients for <code>self</code>. The attribute will then contain the gradients computed and future calls to <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.backward"><code>backward()</code></a> will accumulate (add) gradients into it.</p>
</blockquote>
</li>
<li><p><strong>is_leaf</strong>  所有用户创建的梯度追踪结点为叶子节点,只有叶子节点的梯度值会被计算,可通过retrain_grad()获得非叶子节点的梯度</p>
<blockquote>
<p>All Tensors that have <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.requires_grad"><code>requires_grad</code></a> which is <code>False</code> will be leaf Tensors by convention.</p>
<p>For Tensors that have <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.requires_grad"><code>requires_grad</code></a> which is <code>True</code>, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so <code>grad_fn</code> is None.</p>
<p>Only leaf Tensors will have their <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.grad"><code>grad</code></a> populated during a call to <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.backward"><code>backward()</code></a>. To get <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.grad"><code>grad</code></a> populated for non-leaf Tensors, you can use <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.retain_grad"><code>retain_grad()</code></a>.</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>ML/DL编程总结</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch模型定义</title>
    <url>/2021/07/09/DL%E7%BC%96%E7%A8%8B/pytorch%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/</url>
    <content><![CDATA[<h2 id="pytorch模型定义的几种方式"><a href="#pytorch模型定义的几种方式" class="headerlink" title="pytorch模型定义的几种方式"></a>pytorch模型定义的几种方式</h2><p>主要包括 标准继承模式 和 使用常用容器 两种方式</p>
<h3 id="1-继承nn-Module实现模型"><a href="#1-继承nn-Module实现模型" class="headerlink" title="1. 继承nn.Module实现模型"></a>1. 继承nn.Module实现模型</h3><p>通过继承nn模块的Module基类，并实现初始化<strong>init</strong>()以及forward()方法，实现模型定义</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 模型的初始化方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 首先调用父类构造方法</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        <span class="comment"># 定义各层模型，层与层之间的权重</span></span><br><span class="line">        self.hidden = nn.Linear(<span class="number">128</span>, <span class="number">16</span>) <span class="comment"># 隐藏层</span></span><br><span class="line">        self.activate = nn.ReLU()</span><br><span class="line">        self.output = nn.Linear(<span class="number">16</span>, <span class="number">10</span>)  <span class="comment"># 输出层</span></span><br><span class="line">    <span class="comment"># 模型的计算方法  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.activate(self.hidden(x))</span><br><span class="line">        <span class="keyword">return</span> self.output(x)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p>模型的使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">test_input = torch.rand((256,128))</span><br><span class="line">net = Model()</span><br><span class="line">print(net(test_input))</span><br><span class="line">print(net)</span><br><span class="line">输出:</span><br><span class="line">tensor([[ 0.2073, -0.0732,  0.1259,  ...,  0.1715,  0.1540,  0.1089],</span><br><span class="line">        [ 0.2999, -0.0407,  0.1167,  ...,  0.1734,  0.2581,  0.0695],</span><br><span class="line">        [ 0.3761, -0.0145,  0.1217,  ...,  0.1910,  0.2665,  0.0340],</span><br><span class="line">        ...,</span><br><span class="line">        [ 0.3084, -0.1336,  0.1632,  ...,  0.2174, -0.0351,  0.1205],</span><br><span class="line">        [ 0.3323, -0.1693,  0.1411,  ...,  0.1700,  0.1171, -0.0442],</span><br><span class="line">        [ 0.3395,  0.0367, -0.0068,  ...,  0.2706,  0.2510,  0.0017]],</span><br><span class="line">       grad_fn=&lt;AddmmBackward&gt;)</span><br><span class="line">Model(</span><br><span class="line">  (hidden): Linear(in_features=128, out_features=16, bias=True)</span><br><span class="line">  (activate): ReLU()</span><br><span class="line">  (output): Linear(in_features=16, out_features=10, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="2-快速定义模型"><a href="#2-快速定义模型" class="headerlink" title="2. 快速定义模型"></a>2. 快速定义模型</h3><p>pytorch提供了一系列继承自nn.Module的实现类，类如Sequential等用来快速定义模型</p>
<h4 id="1-nn-Sequential"><a href="#1-nn-Sequential" class="headerlink" title="1. nn.Sequential"></a>1. nn.Sequential</h4><blockquote>
<p>A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ordered dict of modules can also be passed in.</p>
</blockquote>
<p>向Sequential中传入一系列的层/其他模型（Module），按照传入的顺序或者OrderedDict中的顺序进行前向传播计算，实现模型定义</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">          (<span class="string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu2&#x27;</span>, nn.ReLU())</span><br><span class="line">        ]))</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line">输出:</span><br><span class="line">Sequential(</span><br><span class="line">  (conv1): Conv2d(<span class="number">1</span>, <span class="number">20</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (relu1): ReLU()</span><br><span class="line">  (conv2): Conv2d(<span class="number">20</span>, <span class="number">64</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (relu2): ReLU()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="2-nn-ModuleList"><a href="#2-nn-ModuleList" class="headerlink" title="2.nn.ModuleList"></a>2.nn.ModuleList</h4><blockquote>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html?highlight=modulelist#torch.nn.ModuleList"><code>ModuleList</code></a> can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"><code>Module</code></a> methods.</p>
</blockquote>
<p>存储module的列表，支持列表的append，insert操作，并可通过坐标形式访问</p>
<ul>
<li>与sequence不同点，在于ModuList就是个List，不支持forward前向传播运算</li>
<li>与List不同点，在于ModuList中所有模型的参数均加入到了反向传播梯度计算监控中</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class MyModule(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyModule, self).__init__()</span><br><span class="line">        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # ModuleList can act as an iterable, or be indexed using ints</span><br><span class="line">        for i, l in enumerate(self.linears):</span><br><span class="line">            x = self.linears[i // 2](x) + l(x)</span><br><span class="line">        return x</span><br><span class="line">net = MyModule()</span><br><span class="line">print(net)</span><br><span class="line">输出:</span><br><span class="line">MyModule(</span><br><span class="line">  (linears): ModuleList(</span><br><span class="line">    (0): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (1): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (2): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (3): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (4): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (5): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (6): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (7): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (8): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (9): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>与列表存储模型不同点比较</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class MyModule1(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyModule1, self).__init__()</span><br><span class="line">        self.linearList = [nn.Linear(i,i) for i in range(1,3)]</span><br><span class="line">class MyModule2(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyModule2, self).__init__()</span><br><span class="line">        self.linearList = nn.ModuleList([nn.Linear(i,i) for i in range(1, 3)])</span><br><span class="line">net1 = MyModule1()</span><br><span class="line">net2 = MyModule2()</span><br><span class="line">print(&#x27;net1:&#x27;)</span><br><span class="line">for parameter in net1.parameters():</span><br><span class="line">    print(parameter)</span><br><span class="line">print(&#x27;net2:&#x27;)</span><br><span class="line">for parameter in net2.parameters():</span><br><span class="line">    print(parameter)</span><br><span class="line">输出:</span><br><span class="line">net1:</span><br><span class="line">net2:</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[0.9031]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([0.8702], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ 0.4465, -0.2073],</span><br><span class="line">        [-0.3850, -0.6185]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([-0.1432, -0.6002], requires_grad=True)</span><br><span class="line"></span><br><span class="line">#### 3. nn.MoudleDict</span><br><span class="line"></span><br><span class="line">类似于Modulist，只是存储形式为Dict</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ML/DL编程总结</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>sklearn主要模块理解</title>
    <url>/2021/07/09/DL%E7%BC%96%E7%A8%8B/sklearn%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<p>主要包括六个模块，其中四个模块为<strong>分类，回归，聚类，降维</strong>的算法模块，两个其他模型，模型选择评估模块和预处理模块。</p>
<img src="/2021/07/09/DL%E7%BC%96%E7%A8%8B/sklearn%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97%E7%90%86%E8%A7%A3/image-20210709164825577.png" class="" title="image-20210709164825577">
<span id="more"></span>
<h3 id="一个数据分析流程中涉及的sklearn模块"><a href="#一个数据分析流程中涉及的sklearn模块" class="headerlink" title="一个数据分析流程中涉及的sklearn模块"></a>一个数据分析流程中涉及的sklearn模块</h3><p>机器学习项目的一般流程为 <strong>数据集获取</strong>-》<strong>数据预处理</strong>-》<strong>训练模型</strong>-》<strong>评估模型</strong>-》<strong>应用模型</strong></p>
<ol>
<li><p>获取数据集，使用dataset模块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"><span class="comment"># 返回一个包含data和target的字典</span></span><br><span class="line">iris_data = iris.data</span><br><span class="line">iris_target = iris.target</span><br></pre></td></tr></table></figure>
<p>包含常用的数据集加载</p>
<img src="/2021/07/09/DL%E7%BC%96%E7%A8%8B/sklearn%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97%E7%90%86%E8%A7%A3/image-20210709164843880.png" class="" title="image-20210709164843880">
</li>
<li><p>数据预处理，使用sklearn的Preprocessing模块</p>
<p>包括常用的归一化、one_hot等处理方式</p>
<img src="/2021/07/09/DL%E7%BC%96%E7%A8%8B/sklearn%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97%E7%90%86%E8%A7%A3/image-20210709164855098.png" class="" title="image-20210709164855098">
</li>
<li><p>划分数据集，使用model_selection模块中的划分函数（不同的训练集测试集划分方式)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_target, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>
<img src="/2021/07/09/DL%E7%BC%96%E7%A8%8B/sklearn%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97%E7%90%86%E8%A7%A3/image-20210709164909443.png" class="" title="image-20210709164909443">
</li>
<li><p>模型选择，直接导入特定模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">clf = svm.SVC(gamma=<span class="number">0.001</span>, C=<span class="number">100.</span>)</span><br><span class="line"><span class="comment"># 训练模型fit</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(clf.get_params())</span><br><span class="line"><span class="comment"># 评价模型</span></span><br><span class="line"><span class="built_in">print</span>(clf.score(X_test,y_test))</span><br></pre></td></tr></table></figure>
</li>
<li><p>评价模型，使用metircs模块进行评价</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">accuracy_score(y_test, clf.predict(X_test))</span><br></pre></td></tr></table></figure>
<img src="/2021/07/09/DL%E7%BC%96%E7%A8%8B/sklearn%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97%E7%90%86%E8%A7%A3/image-20210709164919754.png" class="" title="image-20210709164919754">
</li>
</ol>
]]></content>
      <categories>
        <category>ML/DL编程总结</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>优化算法学习总结</title>
    <url>/2021/07/21/DL%E7%BC%96%E7%A8%8B/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="1-动量梯度下降"><a href="#1-动量梯度下降" class="headerlink" title="1. 动量梯度下降"></a>1. 动量梯度下降</h3><p>为了解决随机梯度梯度下降存在的震荡问题，通过添加动量，平滑动量变化。</p>
<h4 id="什么是指数加权平均"><a href="#什么是指数加权平均" class="headerlink" title="什么是指数加权平均"></a>什么是指数加权平均</h4><p>当前步的函数值不仅取决于当前的输入，还取决于之前的函数值，公式如下</p>
<script type="math/tex; mode=display">
v_t = \beta v_{t-1}+(1-\beta)\theta_t</script><p>通过不断带入展开，可得只包含 <script type="math/tex">\theta</script> 的表达式为</p>
<script type="math/tex; mode=display">
v_t = (1-\beta)\theta_t + (1-\beta)\beta\theta_{t-1} + (1-\beta)\beta^2\theta_{t-2}+....+(1-\beta)\beta^n\theta_{t-n}+\beta^nv_{t-n}</script><img src="/2021/07/21/DL%E7%BC%96%E7%A8%8B/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/image-20210720092013215.png" class="" title="image-20210720092013215">
<p>由于 <script type="math/tex">(1-\frac{1}{n})^n</script> 在n趋向于无穷时等于 <script type="math/tex">e^{-1}</script>。当 <script type="math/tex">\beta</script> 趋向于1时，<script type="math/tex">\beta^{\frac{1}{1-\beta}} = (1-\frac{1}{n})^n</script> 同样等于 <script type="math/tex">e^{-1}</script>,如果将 <script type="math/tex">e^{-1}</script>看作一个可以忽略的数字，则展开式中含有阶数大于等于 <script type="math/tex">\theta^{\frac{1}{1-\theta}}</script> 的相可以忽略。</p>
<ul>
<li>当 <script type="math/tex">\beta = 0.98</script>​时，<script type="math/tex">\beta^{\frac{1}{1-\beta}} = \beta^{20}</script>​ ,所以所有系数包含<script type="math/tex">\beta^n</script>​ n&gt;=20的均省略，原式子等价于前20个<script type="math/tex">\theta</script>​的加权平均和，<strong>即当前梯度等于前20个梯度的加权平均和</strong><span id="more"></span>
</li>
</ul>
<h4 id="动量梯度下降公式"><a href="#动量梯度下降公式" class="headerlink" title="动量梯度下降公式"></a>动量梯度下降公式</h4><p>假设第 <script type="math/tex">t</script> 次训练的输入为 <script type="math/tex">x_t</script> ,学习率为<script type="math/tex">\eta_t</script>,计算得到梯度为 <script type="math/tex">g_t</script> ,并定义速度向量 <script type="math/tex">v_t</script>,初始化为0；动量系数为m</p>
<script type="math/tex; mode=display">
v_t = m * v_{t-1} + \eta * g_t \\
x_t = x_{t-1} - v_t</script><p>其中 <script type="math/tex">0<m<1</script>，当 <script type="math/tex">m=0</script> 时等价于随机梯度下降</p>
<p><strong>通过对历史梯度进行指数加权平均，实现了梯度的平滑变换，确保了相邻梯度方向的一致性，解决了梯度下降的震荡问题。</strong></p>
<h3 id="2-AdaGrad算法"><a href="#2-AdaGrad算法" class="headerlink" title="2. AdaGrad算法"></a>2. AdaGrad算法</h3><p>为解决不同方向梯度大小变化不同，导致学习率设置的问题，adagrad算法根据每个方向梯度大小，不断更新学习率</p>
<h4 id="AdaGrad公式"><a href="#AdaGrad公式" class="headerlink" title="AdaGrad公式"></a>AdaGrad公式</h4><p>首先定义向量 <script type="math/tex">s_t</script> ,初始化为0，每次迭代累加梯度的内积，递归公式为</p>
<script type="math/tex; mode=display">
s_t = s_{t-1} + g_i * g_i</script><p>更新目标参数是，使用  <script type="math/tex">s_t</script> 更新学习率大小,其中 <script type="math/tex">\epsilon</script> 为为保持数值稳定性添加的常数</p>
<script type="math/tex; mode=display">
x_t = x_{t-1} - \frac{\eta}{\sqrt{s_t+\epsilon}}*g_t</script><p><strong>AdaGrad算法能够根据梯度大小设置学习率，梯度越大，学习率越小，但是由于 $s_t$ 为累加形式，学习率随着训练会不断减小，到后期学习速率可能过慢</strong></p>
<h3 id="3-RMSprop算法"><a href="#3-RMSprop算法" class="headerlink" title="3. RMSprop算法"></a>3. RMSprop算法</h3><p>为了解决AdAGrad算法后期，学习率过小可能无法找到最优解的问题，在计算 $s_t$ 时引入加权平均</p>
<h4 id="RMSprop公式"><a href="#RMSprop公式" class="headerlink" title="RMSprop公式"></a>RMSprop公式</h4><p>引入加权平均后的 $s_t$ 计算公式如下，避免了学习率的不断减小</p>
<script type="math/tex; mode=display">
s_t = \theta s_{t-1} + (1-\theta)g_i * g_i</script><h3 id="4-AdaDelta算法"><a href="#4-AdaDelta算法" class="headerlink" title="4. AdaDelta算法"></a>4. AdaDelta算法</h3><p>舍弃了学习率这一参数，从另一个角度解决AdaGrad后期学习率变小，难以找到最优解的问题</p>
<h4 id="AdaDelta公式"><a href="#AdaDelta公式" class="headerlink" title="AdaDelta公式"></a>AdaDelta公式</h4><p>$s_t$递推公式与RMSprop一致</p>
<script type="math/tex; mode=display">
s_t = \theta s_{t-1} + (1-\theta)g_i * g_i</script><p>增加了状态 $\Delta x_t$,计算梯度  $g’_t$ </p>
<script type="math/tex; mode=display">
g'_t = \sqrt{\frac{\Delta x_{t-1} + \epsilon}{s_t + \epsilon}}*g_t</script><p>使用 $g’_t$  更新参数</p>
<script type="math/tex; mode=display">
x_t = x_{t-1} - g'_t</script><p>其中状态 $\Delta x_t$ 的递推公式为</p>
<script type="math/tex; mode=display">
\Delta x_t = \theta \Delta x_{t-1} + (1-\theta)g'_i * g'_i</script><p><strong>与RMSprop算法区别点在于学习率也通过指数加权平均递归计算</strong></p>
<h3 id="5-Adam算法"><a href="#5-Adam算法" class="headerlink" title="5. Adam算法"></a>5. Adam算法</h3><p>Adam算法结合了动量法和RMSprop法，对梯度和$s_t$ 均进行了指数加权平均</p>
<p>$s_t$递推公式($\theta_1$  建议为0.999）</p>
<script type="math/tex; mode=display">
s_t = \theta_1 s_{t-1} + (1-\theta_1)g_i * g_i</script><p>$v_t$ 递推公式($\theta_2$  建议为0.9）</p>
<script type="math/tex; mode=display">
v_t = \theta_2 * v_{t-1} + \theta_2 * g_t</script><p>由于 $s_t$ 和 $v_t$ 在递推初始化时初始化为0，根据递推公式中 $\theta_1$ 和 $\theta_2$  均为接近于1的数，导致在训练初期， $s_t$ 和 $v_t$ 取值与梯度关系不大，更加趋向于0，因此需要增加 <strong>误差修正(bias correction)</strong> 操作</p>
<script type="math/tex; mode=display">
s'_t = \frac{s_t}{1-\theta_1^t}\\
v'_t = \frac{v_t}{1-\theta_2^t}</script><p>使用修正后的两个参数计算新梯度</p>
<script type="math/tex; mode=display">
g'_t =\frac{\eta v'_t}{\sqrt{s'_t} + \epsilon}</script><p>更新参数公式为</p>
<script type="math/tex; mode=display">
x_t = x_{t-1} - g'_t</script><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><img src="/2021/07/21/DL%E7%BC%96%E7%A8%8B/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/image-20210720143613685.png" class="" title="image-20210720143613685">
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
  </entry>
  <entry>
    <title>sklearn数据预处理一般流程</title>
    <url>/2021/07/09/DL%E7%BC%96%E7%A8%8B/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>The <code>sklearn.preprocessing</code> package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.</p>
</blockquote>
<p>sklearn的preprocessing模块提供了一系列包括标准化、数据最大最小缩放处理、正则化、特征二值化和数据缺失值处理在内的数据预处理模块。</p>

<p>基本操作流程为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.创建预处理器 transform</span></span><br><span class="line">test_scaler = StandardScaler()</span><br><span class="line"><span class="comment"># 2. 调用fit函数 计算预处理所需要的相关数据(如StandardScaler会计算mean、var等)</span></span><br><span class="line">test_scaler.fit(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># 3. 调用transform函数对数据进行预处理</span></span><br><span class="line">test_scaler.transform(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># 或者直接合并fit和transform两部操作</span></span><br><span class="line">test_scaler.fit_transform(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h2 id="1-标准化"><a href="#1-标准化" class="headerlink" title="1. 标准化"></a>1. 标准化</h2><h3 id="使用StandardScaler（mean-1-std-0）"><a href="#使用StandardScaler（mean-1-std-0）" class="headerlink" title="使用StandardScaler（mean = 1,std = 0）"></a>使用StandardScaler（mean = 1,std = 0）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">&gt;&gt; <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;&gt; test_array = np.arange(<span class="number">0</span>,<span class="number">12</span>).reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">       [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">       [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]])</span><br><span class="line">&gt;&gt; test_scaler = StandardScaler()</span><br><span class="line">&gt;&gt; test_scaler.fit(test_array)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(test_scaler.var_)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(test_scaler.mean_)</span><br><span class="line">&gt;&gt; test_scaler.transform(test_array, copy = <span class="literal">True</span>)</span><br><span class="line">[<span class="number">10.66666667</span> <span class="number">10.66666667</span> <span class="number">10.66666667</span> <span class="number">10.66666667</span>]</span><br><span class="line">[<span class="number">4.</span> <span class="number">5.</span> <span class="number">6.</span> <span class="number">7.</span>]</span><br><span class="line">array([[-<span class="number">1.22474487</span>, -<span class="number">1.22474487</span>, -<span class="number">1.22474487</span>, -<span class="number">1.22474487</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ],</span><br><span class="line">       [ <span class="number">1.22474487</span>,  <span class="number">1.22474487</span>,  <span class="number">1.22474487</span>,  <span class="number">1.22474487</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="使用MaxMinScaler进行区间缩放（默认0-1）"><a href="#使用MaxMinScaler进行区间缩放（默认0-1）" class="headerlink" title="使用MaxMinScaler进行区间缩放（默认0-1）"></a>使用MaxMinScaler进行区间缩放（默认0-1）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">&gt;&gt; test_array = np.random.uniform(low=-<span class="number">1</span>, high=<span class="number">16</span>, size=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(test_array)</span><br><span class="line">[[<span class="number">10.45968472</span>  <span class="number">6.52372993</span> <span class="number">12.08526458</span>]</span><br><span class="line"> [ <span class="number">9.94529398</span>  <span class="number">6.3849395</span>   <span class="number">6.22910917</span>]</span><br><span class="line"> [<span class="number">12.02516025</span> <span class="number">11.50269044</span>  <span class="number">6.380779</span>  ]</span><br><span class="line"> [<span class="number">14.67759022</span>  <span class="number">0.36908024</span>  <span class="number">1.13677392</span>]]</span><br><span class="line">[<span class="number">0.42262781</span> <span class="number">0.17963625</span> <span class="number">0.18267358</span>]</span><br><span class="line"><span class="comment"># 传入要缩放的区间</span></span><br><span class="line">&gt;&gt; test_scaler = MinMaxScaler((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">&gt;&gt; test_scaler.fit(test_array)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(test_scaler.scale_)</span><br><span class="line">&gt;&gt; test_scaler.transform(test_array)</span><br><span class="line">array([[-<span class="number">0.78260417</span>,  <span class="number">0.1055982</span> ,  <span class="number">1.</span>        ],</span><br><span class="line">       [-<span class="number">1.</span>        ,  <span class="number">0.08066641</span>, -<span class="number">0.06976488</span>],</span><br><span class="line">       [-<span class="number">0.12099067</span>,  <span class="number">1.</span>        , -<span class="number">0.04205881</span>],</span><br><span class="line">       [ <span class="number">1.</span>        , -<span class="number">1.</span>        , -<span class="number">1.</span>        ]])</span><br></pre></td></tr></table></figure>
<h3 id="使用MaxAbsScaler稀疏数据标准化"><a href="#使用MaxAbsScaler稀疏数据标准化" class="headerlink" title="使用MaxAbsScaler稀疏数据标准化"></a>使用MaxAbsScaler稀疏数据标准化</h3><p>为了避免标准化过程中破坏稀疏数据的稀疏性质，使用MaxAbsScaler,根据样本数据除以最大绝对值，实现到[-1, 1]的映射</p>
<h3 id="使用RobustScaler带有离群值的数据标准化"><a href="#使用RobustScaler带有离群值的数据标准化" class="headerlink" title="使用RobustScaler带有离群值的数据标准化"></a>使用RobustScaler带有离群值的数据标准化</h3><h2 id="2-非线性转化"><a href="#2-非线性转化" class="headerlink" title="2.非线性转化"></a>2.非线性转化</h2><p>主要包括概率分布转化（Quantile transforms）和正态变换（Power transforms），用来将原特定分布的特征值映射到另一个特征分布。</p>
<h3 id="使用QuantileTransformer进行均匀分布映射转换"><a href="#使用QuantileTransformer进行均匀分布映射转换" class="headerlink" title="使用QuantileTransformer进行均匀分布映射转换"></a>使用QuantileTransformer进行均匀分布映射转换</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> QuantileTransformer</span><br><span class="line">&gt;&gt; data_x, data_y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line">&gt;&gt; x_train, x_test, y_train, y_test = train_test_split(data_x, data_y,  test_size = <span class="number">0.2</span>, random_state = <span class="number">42</span>)</span><br><span class="line">&gt;&gt; quantileTransformer = QuantileTransformer()</span><br><span class="line">&gt;&gt; x_train_trans = quantileTransformer.fit_transform(x_train)</span><br><span class="line">&gt;&gt; x_test_trans = quantileTransformer.fit_transform(x_test)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(np.percentile(x_train[:, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">25</span>, <span class="number">50</span>, <span class="number">75</span>, <span class="number">100</span>]))</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(np.percentile(x_train_trans[:, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">25</span>, <span class="number">50</span>, <span class="number">75</span>, <span class="number">100</span>]))</span><br><span class="line">[<span class="number">4.3</span>  <span class="number">5.1</span>  <span class="number">5.75</span> <span class="number">6.4</span>  <span class="number">7.7</span> ]</span><br><span class="line">[<span class="number">0.</span>         <span class="number">0.24789916</span> <span class="number">0.5</span>        <span class="number">0.7605042</span>  <span class="number">1.</span>        ]</span><br></pre></td></tr></table></figure>
<h3 id="使用PowerTransformer进行正态分布映射转换"><a href="#使用PowerTransformer进行正态分布映射转换" class="headerlink" title="使用PowerTransformer进行正态分布映射转换"></a>使用PowerTransformer进行正态分布映射转换</h3><p>使用Yeo-Johnson transform和 Box-Cox transform两种变换方式（暂时还不太懂，不列举代码）</p>
<h2 id="3-标准化（Normalization）"><a href="#3-标准化（Normalization）" class="headerlink" title="3.标准化（Normalization）"></a>3.标准化（<strong>Normalization</strong>）</h2><p>直接使用normalize函数，有三种归一化方式 <code>&#123;‘l1’, ‘l2’, ‘max’&#125;, default=’l2’</code> ，坑爹的是默认使用行向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">&gt;&gt; test_array = np.arange(<span class="number">3</span>, <span class="number">15</span>).reshape(<span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(test_array)</span><br><span class="line">&gt;&gt; result = preprocessing.normalize(test_array, norm = <span class="string">&#x27;l1&#x27;</span>, axis = <span class="number">0</span>)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(result)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(result.<span class="built_in">sum</span>(axis = <span class="number">0</span>))</span><br><span class="line">[[ <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>  <span class="number">8</span>]</span><br><span class="line"> [ <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span>]]</span><br><span class="line">[[<span class="number">0.25</span>       <span class="number">0.28571429</span> <span class="number">0.3125</span>     <span class="number">0.33333333</span> <span class="number">0.35</span>       <span class="number">0.36363636</span>]</span><br><span class="line"> [<span class="number">0.75</span>       <span class="number">0.71428571</span> <span class="number">0.6875</span>     <span class="number">0.66666667</span> <span class="number">0.65</span>       <span class="number">0.63636364</span>]]</span><br><span class="line">[<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br></pre></td></tr></table></figure>
<p>或者使用Normalizer类</p>
<h2 id="4-类型转化"><a href="#4-类型转化" class="headerlink" title="4.类型转化"></a>4.类型转化</h2><p>主要包括 onehot和数字顺序编码两种形式，主要涉及OneHotEncoder和OrdinalEncoder</p>
<h2 id="5-遇到继续整理。。。。"><a href="#5-遇到继续整理。。。。" class="headerlink" title="5. 遇到继续整理。。。。"></a>5. 遇到继续整理。。。。</h2><h2 id="自定义转化器"><a href="#自定义转化器" class="headerlink" title="自定义转化器"></a>自定义转化器</h2><h3 id="1-使用FunctionTransformer封装函数为转化器"><a href="#1-使用FunctionTransformer封装函数为转化器" class="headerlink" title="1.使用FunctionTransformer封装函数为转化器"></a>1.使用FunctionTransformer封装函数为转化器</h3><p>没想到怎么传入多个参数的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> FunctionTransformer</span><br><span class="line">&gt;&gt; <span class="function"><span class="keyword">def</span> <span class="title">my_power</span>(<span class="params">x, power = <span class="number">2</span></span>):</span></span><br><span class="line">    x = np.power(x, power);</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line">&gt;&gt; my_transformer = FunctionTransformer(my_power)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(my_transformer)</span><br><span class="line">&gt;&gt; test_array = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(my_transformer.fit_transform(test_array))</span><br><span class="line">FunctionTransformer(func=&lt;function my_power at <span class="number">0x000001988F4543A0</span>&gt;)</span><br><span class="line">[ <span class="number">1</span>  <span class="number">4</span>  <span class="number">9</span> <span class="number">16</span>]</span><br></pre></td></tr></table></figure>
<h3 id="3-继承BaseEstimator-TransformerMixin（自动实现fit-transform）"><a href="#3-继承BaseEstimator-TransformerMixin（自动实现fit-transform）" class="headerlink" title="3. 继承BaseEstimator, TransformerMixin（自动实现fit_transform）"></a>3. 继承BaseEstimator, TransformerMixin（自动实现fit_transform）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line">rooms_ix, bedrooms_ix, population_ix, households_ix = <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span></span><br><span class="line"><span class="comment"># 通过原有属性增加一列属性（来自书籍-机器学习实战）</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CombineAttirbutesAdder</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, add_bedrooms_per_room = <span class="literal">True</span></span>):</span></span><br><span class="line">        self.add_bedrooms_per_room = add_bedrooms_per_room</span><br><span class="line">    <span class="comment"># 提取某些特征，例如归一化处理是求平均值和方差</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        rooms_per_house = X[:, rooms_ix] / X[:, households_ix]</span><br><span class="line">        pepoles_per_house = X[:, population_ix] / X[:, households_ix]</span><br><span class="line">        <span class="keyword">if</span> self.add_bedrooms_per_room:</span><br><span class="line">            bedrooms_per_house = X[:,bedrooms_ix] / X[:, households_ix]</span><br><span class="line">            <span class="keyword">return</span> np.c_[X, rooms_per_house, bedrooms_per_house]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> np.c_[X, rooms_per_house]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ML/DL编程总结</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>sklearn Pipeline 组合多个预处理和预测模型</title>
    <url>/2021/07/09/DL%E7%BC%96%E7%A8%8B/%E7%BB%84%E5%90%88%E5%A4%9A%E4%B8%AA%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<blockquote>
<p> Transformers are usually combined with classifiers, regressors or other estimators to build a composite estimator</p>
</blockquote>
<p>将一系列的数据预处理和模型封装在一起，固定处理数据的一系列步骤，调用一次fit和predict完成整个流程，并可使用grid search对pipeline内参数进行统一调试。</p>
<p>构成规则如下（前面处理数据，最后一步输入模型或者完全处理数据）：</p>
<ul>
<li>所有流水线中的estimator必须为transformer，除了最后一个</li>
<li>最后一个estimator可以是任何类型（trainsformer，classifier）</li>
</ul>
<p>主要函数：</p>
<ul>
<li>union为只包含transformer的pipeline</li>
</ul>

<span id="more"></span>
<h3 id="1-pipeline的基本使用"><a href="#1-pipeline的基本使用" class="headerlink" title="1. pipeline的基本使用"></a>1. pipeline的基本使用</h3><p>使用类似字典的元组链表初始化pipeline，或者使用make_pipeline()函数直接传入estimator列表</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">&gt;&gt; estimators = [(<span class="string">&#x27;reduce_dim&#x27;</span>, PCA()), (<span class="string">&#x27;model&#x27;</span>, SVC())]</span><br><span class="line">&gt;&gt; test_pipeline = Pipeline(estimators)</span><br><span class="line">&gt;&gt; test_pipeline</span><br><span class="line">Pipeline(steps=[(<span class="string">&#x27;reduce_dim&#x27;</span>, PCA()), (<span class="string">&#x27;model&#x27;</span>, SVC())])</span><br></pre></td></tr></table></figure>
<p>可通过数组、字典以及step属性访问每个estimator</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="built_in">print</span>(test_pipeline[<span class="number">0</span>])</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(test_pipeline.steps)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(test_pipeline[<span class="string">&#x27;reduce_dim&#x27;</span>])</span><br><span class="line">PCA()</span><br><span class="line">[(<span class="string">&#x27;reduce_dim&#x27;</span>, PCA()), (<span class="string">&#x27;model&#x27;</span>, SVC())]</span><br><span class="line">PCA()</span><br></pre></td></tr></table></figure>
<p>可使用数组的截断形式，获取子pipeline</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="built_in">print</span>(<span class="built_in">len</span>(test_pipeline))</span><br><span class="line">&gt;&gt; sub_pipeline = test_pipeline[<span class="number">1</span>:]</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(sub_pipeline)</span><br><span class="line"><span class="number">2</span></span><br><span class="line">Pipeline(steps=[(<span class="string">&#x27;model&#x27;</span>, SVC())])</span><br></pre></td></tr></table></figure>
<p>使用<code>&lt;estimator&gt;__&lt;parameter&gt;</code> 即pipeline中estimator名称+参数，设置访问pipeline中某一estimator的参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; test_pipeline.set_params(model__C = <span class="number">10</span>)</span><br><span class="line">&gt;&gt; test_pipeline.get_params()[<span class="string">&#x27;model__C&#x27;</span>]</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="comment"># 与GridSearchCV结合使用</span></span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line">&gt;&gt; param_grid = <span class="built_in">dict</span>(reduce_dim__n_components=[<span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>], clf__C=[<span class="number">0.1</span>, <span class="number">10</span>, <span class="number">100</span>])</span><br><span class="line">&gt;&gt; grid_search = GridSearchCV(test_pipeline, param_grid=param_grid)</span><br><span class="line">&gt;&gt; grid_search</span><br><span class="line">GridSearchCV(estimator=Pipeline(steps=[(<span class="string">&#x27;reduce_dim&#x27;</span>, PCA()),</span><br><span class="line">                                       (<span class="string">&#x27;model&#x27;</span>, SVC(C=<span class="number">10</span>))]),</span><br><span class="line">             param_grid=&#123;<span class="string">&#x27;clf__C&#x27;</span>: [<span class="number">0.1</span>, <span class="number">10</span>, <span class="number">100</span>],</span><br><span class="line">                         <span class="string">&#x27;reduce_dim__n_components&#x27;</span>: [<span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>]&#125;)</span><br></pre></td></tr></table></figure>
<h1 id="待补充。。"><a href="#待补充。。" class="headerlink" title="待补充。。"></a>待补充。。</h1>]]></content>
      <categories>
        <category>ML/DL编程总结</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>lc2212.射箭比赛中的最大比赛</title>
    <url>/2022/03/27/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/lc2212.%E5%B0%84%E7%AE%AD%E6%AF%94%E8%B5%9B%E4%B8%AD%E7%9A%84%E6%9C%80%E5%A4%A7%E6%AF%94%E8%B5%9B/</url>
    <content><![CDATA[<h3 id="2212-射箭比赛中的最大得分"><a href="#2212-射箭比赛中的最大得分" class="headerlink" title="2212. 射箭比赛中的最大得分"></a><a href="https://leetcode-cn.com/problems/maximum-points-in-an-archery-competition/">2212. 射箭比赛中的最大得分</a></h3><p>打周赛遇到的一道非常典型的0-1背包问题，但是因为太久没写过背包问题，写了40分钟才写出来，再温习一遍。</p>
<h4 id="什么是0-1背包？"><a href="#什么是0-1背包？" class="headerlink" title="什么是0-1背包？"></a>什么是0-1背包？</h4><p>一共有N个物品，每个物品有对应的重量weight[i] 和价值value[i]，给定一个固定容量W的背包，问能够放入背包的最大价值是多少？非常经典又相对简单的动态规划问题，关键还是在于定义状态以及状态转移公式。</p>
<p>贪心的角度出发，假设我选取某个物品后，新的贪心目标变为 在剩余物品和空间的条件下，选取最大价值的物品，因此父问题到子问题的转化可以定义为：（按照从左到右一个一个选的思路）</p>
<ul>
<li><strong>父问题</strong>：是否选取第i个物品，使得总价值最大</li>
<li><strong>子问题</strong>：父问题 <strong>选 / 不选 </strong>第i个物品， 背包剩余 <strong>W-w[i] / W</strong>空间，如何在前i-1物品中合适选取使得子问题价值最大</li>
</ul>
<span id="more"></span>
<p>dp状态可以定义为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dp[i][j]: 将前i个物品装到容量为j的背包中的最大价值</span><br></pre></td></tr></table></figure>
<p>状态转移公式为:</p>
<ol>
<li><p>选取第i个物品</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dp[i][j] = v[i] + dp[i][j - w[i]]</span><br></pre></td></tr></table></figure>
</li>
<li><p>不选第i个物品（或者当前容量无法选取第i个物品）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dp[i][j] = dp[i - 1][j] </span><br></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li><p>最后公式为</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dp[i][j] = max(v[i] + dp[i][j - w[i]], dp[i - 1][j])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>复杂度分析</strong>:</p>
<ol>
<li><strong>时间复杂度：</strong>自下而上（前i物品）计算状态，每个物品需要遍历所有的背包容量，<strong>时间复杂度为 O(NW)</strong>，即物品数量乘以背包容量</li>
<li><strong>空间复杂度：</strong> 类似时间复杂度分析，存储所有状态需要 O(NW)，若使用存储压缩，只需要一个长度等于背包容量的数组，但是若要存储解，则无法压缩，<strong>空间复杂度为 O(NW) 或者 O(W)</strong></li>
</ol>
<p><strong>模板伪代码</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># dp数组长度为w+1,多一个为了减少越界判断</span><br><span class="line">dp[0,1,2,3....,w] = 0</span><br><span class="line">for i in [0,1,2,3,4,5,6,7,8,9....n]:</span><br><span class="line">	for j in [w,w - 1,....1]:</span><br><span class="line">		# j &gt; w[i],反向遍历避免状态丢失（因为只用了一维数组存储状态）</span><br><span class="line">		dp[j] = max(dp[j], dp[j−w[i]]+v[i])</span><br></pre></td></tr></table></figure>
<h4 id="射箭问题"><a href="#射箭问题" class="headerlink" title="射箭问题"></a>射箭问题</h4><p>非常明显的0-1背包问题，箭支数量为”背包容量”，占领每个区域所需要的箭为”物品重量“，每个区域的分数为”价值”，转化为0-1背包问题套模板即可，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] maximumBobPoints(<span class="keyword">int</span> numArrows, <span class="keyword">int</span>[] aliceArrows) &#123;</span><br><span class="line">        <span class="comment">// dp状态数组</span></span><br><span class="line">        <span class="keyword">int</span>[] dpArray = <span class="keyword">new</span> <span class="keyword">int</span>[numArrows + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span>[][] solution = <span class="keyword">new</span> <span class="keyword">int</span>[aliceArrows.length][numArrows + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; aliceArrows.length;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = numArrows;j &gt; <span class="number">0</span>;j--)&#123;</span><br><span class="line">                <span class="keyword">if</span>(j &gt; aliceArrows[i] &amp;&amp; i + dpArray[j - aliceArrows[i] - <span class="number">1</span>] &gt; dpArray[j])&#123;</span><br><span class="line">                    dpArray[j] = i + dpArray[j - aliceArrows[i] - <span class="number">1</span>];</span><br><span class="line">                    solution[i][j] = <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//生成结果数组(比较重要的一部分代码)</span></span><br><span class="line">        <span class="keyword">int</span>[] result = <span class="keyword">new</span> <span class="keyword">int</span>[aliceArrows.length];</span><br><span class="line">        <span class="keyword">int</span> curArrows = numArrows;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = aliceArrows.length - <span class="number">1</span>;i &gt; <span class="number">0</span>;i--)&#123;</span><br><span class="line">            <span class="keyword">if</span>(solution[i][curArrows] == <span class="number">1</span>)&#123;</span><br><span class="line">                result[i] = aliceArrows[i] + <span class="number">1</span>;</span><br><span class="line">                curArrows -= aliceArrows[i] + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        result[<span class="number">0</span>] = curArrows;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>力扣刷题</category>
      </categories>
      <tags>
        <tag>0-1背包</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>死亡搁浅通关感想:一场关于‘连接’的快递之旅</title>
    <url>/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/</url>
    <content><![CDATA[<p>最开始从b站老戴的全流程视频里了解到了死亡搁浅这款游戏，听说这个游戏好看不好玩，原计划在b站云通关，但是看了几期视频后，发现游戏所营造的世界观和剧情深深的吸引了我，于是等到了Epic夏促打折入手，这几天终于通关了游戏，心里有一些小感受，写一点东西，算是对死亡搁浅体验的个人总结。</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/20210617193238_1.jpg" class="" title="20210617193238_1">
<span id="more"></span>
<h4 id="我玩到了什么"><a href="#我玩到了什么" class="headerlink" title="我玩到了什么"></a>我玩到了什么</h4><p>说实话网上说死亡搁浅更像电影，不像游戏有一定道理的，全流程体验下来，感觉主要的游戏方式主要就局限于<strong>送快递、打BT、抢劫米尔人</strong>这几种，修公路、建滑索算也勉强算是游戏内容一部分。</p>
<ul>
<li><strong>跑图送快递</strong>，从一个点到另一个点跑图，有车开车，没车硬跑，还要维持身体平衡，比较枯燥，但是小岛配的音乐真的有味道，和场景搭配起来恰到好处，一定程度解决了长距离跑图的枯燥</li>
<li><strong>干BT</strong>，说实话我感觉体验不是很好，虽然武器和怪物随着剧情发展种类都会不断增加，但是实际体验下来还是换汤不换药，基本就是站桩打枪，尤其是背着一堆货物，是真不想打BT</li>
<li><strong>抢劫米尔人</strong>，这部分类似于其他游戏里的暗杀，不能击杀只能靠近用绳索勒晕，或者后期用步枪什么的，体验多了就有点枯燥了，尤其是后面米尔人有枪之后，我基本能绕着走就绕着走（另外我修了三条路，全是靠米尔人的寄存桶资源，谢谢米尔人为美国做出的贡献）</li>
</ul>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/20210721223211_1.jpg" class="" title="20210721223211_1">
<h5 id="体验不好的点"><a href="#体验不好的点" class="headerlink" title="体验不好的点"></a>体验不好的点</h5><blockquote>
<p>你想想，你带着货箱，开着小摩托，吃着隐生虫唱着歌，突然被米尔人给劫了</p>
</blockquote>
<p>游戏难度我选的默认普通，基本不会遇到怪打不过重新加载存档点，或者遣返重生的情况，最多的情况就是被米尔人抢劫了，或者掉河里货丢了这两种情况，尤其是开着摩托，带着一堆货，突然遇到一堆米尔人，他们追我跑，结果开的太快冲进河里，车也没了，货也没了，真的我当时就想把游戏性卸载了，ZTMD难受。</p>
<p>我感觉我游戏体验不好部分主要就是</p>
<ul>
<li><strong>载具手感真差</strong>，拿手柄开摩托车、汽车是真的折磨，弯转不过来，前进后退真的迷，难道为了让我们修公路故意这么做的吗？</li>
<li><strong>传送点没啥用</strong>，后期用fragile的伞给我们整了个传送功能，但是不能带货的传送有啥用？反过来想如果能带货那就没意思了，总而言之传送功能没什么用处。</li>
<li><strong>战斗太单一了</strong>，从头到尾就是打BT，从手榴弹打狮子狗BT，到步枪打希格斯BT,最后的榴弹炮打鲸鱼BT，真的没什么区别，玩到后面真的审美疲劳。</li>
<li><strong>不时需要安慰的bb</strong>，有时候真的稍微摔一下bb就开始哭，这就是照顾孩子的心情吗？一次两次还好，次数多了真的着急</li>
<li><strong>剥洋葱式讲剧情</strong>，不能说是缺点，但是一上来就给你扔进入一个未知的世界，陌生的世界观，玩起来真的晕头转向，不知道自己在干啥，后面一点一点展开就好点了，确实提高了游戏的入门门槛。</li>
</ul>
<h5 id="印象深刻的战斗"><a href="#印象深刻的战斗" class="headerlink" title="印象深刻的战斗"></a>印象深刻的战斗</h5><p>第一个是游戏快接近结束时和希格斯的从现实到冥滩的三百回合大战，在前面的剧情里把希格斯塑造的太无敌，每次出来都是在主角的脸上跳舞，以至于在最后一场和希格斯的肉搏战里，看着把希格斯脸锤得变形是真的解气，就ntm叫希格斯啊！！！</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/Death_Stranding_2021_8_4_15_48_50.png" class="" title="Death_Stranding_2021_8_4_15_48_50">
<p>第二个是拔叔饰演的昂格尔剧情部分的越战冥滩，虽然进入以后还是老一套的打BT，但是场景中枪林弹雨、各种飞机火箭的轰鸣，战场的紧张，肾上腺素喷薄，真的让我感觉仿佛置身于战场之中，我真真切切的被场景所震撼了。</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/20210802145157_1.jpg" class="" title="20210802145157_1">
<h4 id="死亡搁浅讲了什么（剧透警告）"><a href="#死亡搁浅讲了什么（剧透警告）" class="headerlink" title="死亡搁浅讲了什么（剧透警告）"></a>死亡搁浅讲了什么（<strong>剧透警告</strong>）</h4><p>死亡搁浅的出现让人类世界支离破碎，人们虽然居住在一篇大陆，却只能困于自己小小的避难所之中，无法与其他人产生联系，在这样的背景下，主角sam肩负起了连接世界的重任，通过连接一个一个节点的网络，让人们重新连接在一起，随着连接的不断进行，主角渐渐发现的自己的身世和死亡搁浅的真相，那就是人类的第六次灭绝，而自己是解决这一切的关键，是选择破而后立还是负重前行？面对决定人类命运最终的抉择，主角做出了自己的选择。死亡搁浅故事内核在我看来还是<strong>美式个人英雄主义</strong>式的，一人carry全场后，了却世俗，隐于山林。</p>
<h5 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h5><p>游戏从到到尾都在连接开罗尔网络，但我觉得小岛所传达出的出的‘连接’不仅仅局限于物理意义上的连接，更重要的是末世下人与人心之间的连接，游戏不断地用一个一个支线故事调这一点。玛玛与洛克妮的合体，废品商与女朋友的重归于好等等，相比连接开罗尔网络，更重要的是连接破碎的人心。</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/20210803110221_1.jpg" class="" title="20210803110221_1">
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/20210802102701_1.jpg" class="" title="20210802102701_1">
<p>在连接世界的过程中，山姆原本冰冷的心也渐渐的与世界连接在一起，从最开始的密切接触恐惧症，到最后主动与亡人紧紧拥抱，主动接受芙拉吉尔隐生虫，山姆接受了世界，也融入了这个世界。</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/20210802144048_1.jpg" class="" title="20210802144048_1">
<p>在游戏机制上，小岛也设计了玩家与玩家之间的连接，玩家之间可以相互帮忙送货，一起搞基建，相互点赞。从剧情到游戏，真的让我感觉紧紧连接在一起。（经常骑别人的摩托车，丢了也不心疼）</p>
<h5 id="关于死亡"><a href="#关于死亡" class="headerlink" title="关于死亡"></a>关于死亡</h5><blockquote>
<p>世界上只有一种英雄主义,就是看清生活的真相之后依然热爱生活</p>
</blockquote>
<p>游戏的最后，小岛给我们抛出了一个问题，当你面临关于人类命运的选择，你会做出哪个选项？</p>
<ol>
<li><strong>破而后立</strong> 选择灭亡，毁灭是为了更好的重生</li>
<li><strong>负重前行</strong> 选择阻止死亡搁浅的发生，接受这一事实，继续生活</li>
</ol>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/Death_Stranding_2021_8_10_17_52_30.png" class="" title="Death Stranding 2021_8_10 17_52_30">
<p>然而小岛并没有给我们选择的权力，我拿着枪一顿射，发现根本打不死amile，试了好几次才发现只能拥抱amile组织死亡搁浅的发生，最后只能违背自己的愿望，继续活下去，其实我心里更喜欢选项1。或许正如罗曼罗兰所说的：世界上只有一种英雄主义,就是看清生活的真相之后依然热爱生活，负重前行也许是更好的选择。</p>
<h5 id="山姆离去"><a href="#山姆离去" class="headerlink" title="山姆离去"></a>山姆离去</h5><p>其实最后山姆拒绝芙拉吉尔，选择自己走了我是有点难受的，我觉得他就应该和芙拉吉尔在一起，我觉得小岛一直在用山姆对待隐生虫的态度象征两个人的关系，最开始在山洞相遇山姆拒绝芙拉吉尔的隐生虫，后面开始接受，再到主动给芙拉吉尔隐生虫，象征着两人的关系越来越亲密，最后在首都节点城，拒绝芙拉吉尔后，天空也出现了隐生虫，但是山姆放弃了，真的难以接受。</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/image-20210812103152564.png" class="" title="image-20210812103152564">
<p>至于山姆为什么要离去？我觉得首先山姆最牵挂的就是amile、bb以及芙拉吉尔，但是当他完成最终选择，他发现amile留在冥滩，永远回不来了，bb也死了，他所有的牵挂都离他而去。山姆发现自己牵挂的东西最后都不得善终，因此主动放弃了芙拉吉尔，选择自己一个人享受孤独。（忘记截图了，截一张老戴视频里的）</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/image-20210812104025400.png" class="" title="image-20210812104025400">
<h4 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h4><p>死亡搁浅以自己独特的世界观讲了一个好故事，值回票价，如果你没那么重视玩法，喜欢剧情导向的游戏，那么死亡搁浅准没错，最后像吐槽一下，什么时候美利坚能像游戏里塑造的那么正直高大，我觉得不太可能</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/Death_Stranding_2021_8_10_19_49_58.png" class="" title="Death_Stranding_2021_8_10 19_49_58.png">
]]></content>
      <categories>
        <category>玩物丧志</category>
      </categories>
      <tags>
        <tag>玩后感</tag>
      </tags>
  </entry>
  <entry>
    <title>lc437.路径总和III</title>
    <url>/2021/09/30/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/lc437.%E8%B7%AF%E5%BE%84%E6%80%BB%E5%92%8CIII/</url>
    <content><![CDATA[<h3 id="437-路径总和-III"><a href="#437-路径总和-III" class="headerlink" title="437. 路径总和 III"></a><a href="https://leetcode-cn.com/problems/path-sum-iii/">437. 路径总和 III</a></h3><p>前后一共做了三次，每次都想不出来使用前缀和的方法。</p>
<h4 id="思路1：DFS遍历"><a href="#思路1：DFS遍历" class="headerlink" title="思路1：DFS遍历"></a>思路1：DFS遍历</h4><p>先序遍历整个二叉树，遍历到某一结点后，以该节点为子树，查找当前子树中，是否存在一条从根节点出发的路径，满足路径和条件。</p>
<p><strong>复杂度分析</strong>：</p>
<ul>
<li>时间复杂度：与DFS遍历不同的点，在于每到达一个节点都需要重新遍历子树，寻找备选最优解，<strong>时间复杂度为O(N^2)</strong></li>
<li>空间复杂度：两次遍历不影响栈的深度，最大栈深度与DFS相同，<strong>空间复杂度为O(logN)</strong></li>
</ul>
<span id="more"></span>
<p><strong>代码实现：</strong></p>
<p>思路比较简单，实现也比较简单</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getSolution</span><span class="params">(TreeNode curNode ,<span class="keyword">int</span> curSum, <span class="keyword">int</span> targetSum)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(curNode == <span class="keyword">null</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">    curSum += curNode.val;</span><br><span class="line">    <span class="keyword">if</span>(curSum == targetSum)&#123;</span><br><span class="line">        result++;</span><br><span class="line">    &#125;</span><br><span class="line">    result += getSolution(curNode.left, curSum, targetSum) + getSolution(curNode.right, curSum, targetSum);</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">pathSum</span><span class="params">(TreeNode root, <span class="keyword">int</span> targetSum)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//简单的dfs</span></span><br><span class="line">    <span class="keyword">return</span> getSolution(root, <span class="number">0</span>, targetSum) + pathSum(root.left, targetSum) + pathSum(root.right, targetSum);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="思路2：前缀和"><a href="#思路2：前缀和" class="headerlink" title="思路2：前缀和"></a>思路2：前缀和</h4><p>DFS计算路径和只能计算从根路径到当前节点的总和，但是满足目标解的路径不一定从根节点开始，我们可以将不从根节点开始的序列，转化成 两个从根出发序列的差，其中一个序列是另一个序列前缀。</p>
<p>如何实现这种存储，并且方便查询？深度遍历过程中将当前路径和存储到哈希表中，当遍历到某一个节点时，哈希表存储的是从根节点到当前节点的序列的<strong>所有前缀子序列的路径和</strong>,原问题转化为</p>
<script type="math/tex; mode=display">
sum_{seq\_i} = sum_{rootSeq_i} - sum_{preSeq_i}</script><p>在回退时，需要删除再是前缀的序列的前缀和（也就是当前序列）</p>
<p><strong>复杂度分析：</strong></p>
<ul>
<li>时间复杂度：与DFS一致，时间复杂度为树中的节点数目，<strong>时间复杂度为O(N)</strong></li>
<li>空间复杂度：哈希表的最大存储数量为O(logN)，<strong>空间复杂度为O(logN)</strong></li>
</ul>
<p><strong>代码实现：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//前缀和算法，包含当前节点</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">dfs</span><span class="params">(TreeNode curNode,<span class="keyword">int</span> curSum,<span class="keyword">int</span> targetSum,Map&lt;Integer, Integer&gt; preSumMap)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(curNode == <span class="keyword">null</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">    curSum += curNode.val;</span><br><span class="line">    <span class="keyword">if</span>(preSumMap.containsKey(curSum - targetSum) &amp;&amp; preSumMap.get(curSum - targetSum) != <span class="number">0</span>)&#123;</span><br><span class="line">        result += preSumMap.get(curSum - targetSum);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//将当前前缀和添加到hashmap中</span></span><br><span class="line">    <span class="keyword">int</span> times = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(preSumMap.containsKey(curSum))&#123;</span><br><span class="line">        times += preSumMap.get(curSum);</span><br><span class="line">    &#125;</span><br><span class="line">    preSumMap.put(curSum, times);</span><br><span class="line">    result += (dfs(curNode.left, curSum, targetSum, preSumMap) + dfs(curNode.right, curSum, targetSum, preSumMap));</span><br><span class="line">    <span class="comment">//回滚</span></span><br><span class="line">    preSumMap.put(curSum, times - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">pathSum</span><span class="params">(TreeNode root, <span class="keyword">int</span> targetSum)</span> </span>&#123;</span><br><span class="line">    Map&lt;Integer, Integer&gt; preSumMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    preSumMap.put(<span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> dfs(root, <span class="number">0</span>, targetSum, preSumMap);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li><p>初始化应加入根节点的”假”前缀序列(序列和为0)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">preSumMap.put(<span class="number">0</span>, <span class="number">1</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>有序树中可能有负值节点，不能只记录前缀值的否出现，应记录出现次数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> times = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span>(preSumMap.containsKey(curSum))&#123;</span><br><span class="line">	times += preSumMap.get(curSum);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>力扣刷题</category>
      </categories>
      <tags>
        <tag>树</tag>
        <tag>前缀思路</tag>
      </tags>
  </entry>
  <entry>
    <title>lc517.超级洗衣机</title>
    <url>/2021/09/30/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/lc517.%E8%B6%85%E7%BA%A7%E6%B4%97%E8%A1%A3%E6%9C%BA/</url>
    <content><![CDATA[<h3 id="517-超级洗衣机"><a href="#517-超级洗衣机" class="headerlink" title="517. 超级洗衣机"></a><a href="https://leetcode-cn.com/problems/super-washing-machines/">517. 超级洗衣机</a></h3><p>贪心的思路并不难，就是太难想到了</p>
<h4 id="思路1：邻居平分法（我自己的思路）"><a href="#思路1：邻居平分法（我自己的思路）" class="headerlink" title="思路1：邻居平分法（我自己的思路）"></a>思路1：邻居平分法（我自己的思路）</h4><p>类似于最优解的区域法，针对每个洗衣机，将整个数组划分为左边右边两个子区域，计算左右两个区域的衣服总和，并定会存在总数小于或者大于 平均*区域数量 的区域，如果当前洗衣机衣服多，就将多余的衣服分给缺衣服的区域（给了邻居）</p>
<ol>
<li>每遍历一次所有洗衣机等价于一次移动</li>
<li>最后一次遍历所有情况下划分的子区域，均满足 平均*区域数量 的性质</li>
</ol>
<p><strong>复杂度分析：</strong></p>
<ol>
<li>时间复杂度：每次平均都需要从头到尾遍历数组，共需要遍历结果次数的数组，<strong>时间复杂度为O(KN)</strong></li>
<li>空间复杂度：额外开辟了一个存储前缀和的数组，<strong>空间复杂度为O(N)</strong></li>
</ol>
<span id="more"></span>
<p><strong>代码实现：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findMinMoves</span><span class="params">(<span class="keyword">int</span>[] machines)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> average;</span><br><span class="line">        <span class="keyword">int</span> numOFMach = machines.length;</span><br><span class="line">        <span class="keyword">int</span>[] preSum = <span class="keyword">new</span> <span class="keyword">int</span>[numOFMach];</span><br><span class="line">        preSum[<span class="number">0</span>] = machines[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i &lt; numOFMach;i++)&#123;</span><br><span class="line">            preSum[i] = preSum[i - <span class="number">1</span>] + machines[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//判断是否可分</span></span><br><span class="line">        <span class="keyword">if</span>(preSum[numOFMach - <span class="number">1</span>] % numOFMach != <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        average = preSum[numOFMach - <span class="number">1</span>] / numOFMach;</span><br><span class="line">        <span class="keyword">boolean</span> flag;</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            flag = <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; numOFMach;i++)&#123;</span><br><span class="line">                <span class="comment">//如果左边区域小，往左边区域移动</span></span><br><span class="line">                <span class="keyword">if</span>(machines[i] &gt; <span class="number">0</span>)&#123;</span><br><span class="line">                    <span class="keyword">if</span>(i != <span class="number">0</span> &amp;&amp; preSum[i] - machines[i] &lt; i * average)&#123;</span><br><span class="line">                        machines[i] -= <span class="number">1</span>;</span><br><span class="line">                        machines[i - <span class="number">1</span>] += <span class="number">1</span>;</span><br><span class="line">                        preSum[i - <span class="number">1</span>] += <span class="number">1</span>;</span><br><span class="line">                        flag = <span class="keyword">false</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span>(i != numOFMach - <span class="number">1</span> &amp;&amp; preSum[numOFMach - <span class="number">1</span>] - preSum[i] &lt; (numOFMach - i - <span class="number">1</span>) * average)&#123;</span><br><span class="line">                        machines[i] -= <span class="number">1</span>;</span><br><span class="line">                        machines[i + <span class="number">1</span>] += <span class="number">1</span>;</span><br><span class="line">                        preSum[i] -= <span class="number">1</span>;</span><br><span class="line">                        flag = <span class="keyword">false</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//没有平均操作就返回</span></span><br><span class="line">            <span class="keyword">if</span>(flag)&#123;</span><br><span class="line">                <span class="keyword">return</span> result;</span><br><span class="line">            &#125;</span><br><span class="line">            result++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>可惜差两个测试用例，超时</p>
<img src="/2021/09/30/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/lc517.%E8%B6%85%E7%BA%A7%E6%B4%97%E8%A1%A3%E6%9C%BA/image-20210930181801433.png" class="" title="image-20210930181801433">
</li>
</ul>
<h4 id="思路2：官方题解"><a href="#思路2：官方题解" class="headerlink" title="思路2：官方题解"></a>思路2：官方题解</h4><p>基于区域的最优，主要是三个点</p>
<ol>
<li><p>如果将数组划分为前后两个区域，如果是一个多衣服，一个少衣服，要达到平均状态最少也要将</p>
<p>多衣服的区域多的衣服，转移到少衣服区域，也就是 <strong>最少的次数至少也是多衣服区域多出来的衣服</strong></p>
</li>
<li><p>单个区域内要达到平均，最多衣服的洗衣机，要转移成平均，意味着 <strong>最少的次数至少是最多衣服数量洗衣机转移的衣服数量</strong></p>
</li>
<li><p>这种转移可以并行进行，也就得到了最终的贪心策略-<strong>寻找多最多的区域或者某个洗衣机</strong></p>
</li>
</ol>
<p><strong>复杂度分析：</strong></p>
<ol>
<li>时间复杂度：只需要遍历一次数组计算上面两个状态，<strong>时间复杂度为O(N)</strong></li>
<li>空间复杂度：不需要额外空间，<strong>空间复杂度为O(1)</strong></li>
</ol>
<p><strong>代码实现：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findMinMoves</span><span class="params">(<span class="keyword">int</span>[] machines)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//首先求和计算平均值</span></span><br><span class="line">    <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> average;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; machines.length;i++)&#123;</span><br><span class="line">        sum += machines[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(sum % machines.length != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//计算平均数，并用sum作为前i个元素所需要移入或移出的个数</span></span><br><span class="line">    average = sum / machines.length;</span><br><span class="line">    sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> tempNeed;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; machines.length;i++)&#123;</span><br><span class="line">        tempNeed = machines[i] - average;</span><br><span class="line">        sum += tempNeed;</span><br><span class="line">        result = Math.max(Math.abs(sum),Math.max(tempNeed, result));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>力扣刷题</category>
      </categories>
      <tags>
        <tag>贪心算法</tag>
      </tags>
  </entry>
  <entry>
    <title>lc59.两数相除</title>
    <url>/2021/10/17/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/lc59.%E4%B8%A4%E6%95%B0%E7%9B%B8%E9%99%A4/</url>
    <content><![CDATA[<h3 id="29-两数相除"><a href="#29-两数相除" class="headerlink" title="29. 两数相除"></a><a href="https://leetcode-cn.com/problems/divide-two-integers/">29. 两数相除</a></h3><p>难点在于处理不能用long解决溢出问题</p>
<h4 id="思路1：“二进制”减法-没有满足越界要求"><a href="#思路1：“二进制”减法-没有满足越界要求" class="headerlink" title="思路1：“二进制”减法(没有满足越界要求)"></a>思路1：“二进制”减法(没有满足越界要求)</h4><p>不能用除法，最简单的思路就是被除数(dividend)不断地减除数(divisor)，直到减到剩余余数，相减的次数就是结果。该方法的问题就是时间复杂度太高，</p>
<ul>
<li>二进制优化，首先找到最大的n使得 $divisor <em> 2^n &lt; dividend$，每次减去 $ divisor </em> 2^n， divisor <em> 2^{n-1} ，divisor </em> 2^{n-2}  ……$,直到减到 $ divisor $</li>
<li>实际上就是将原来的逐个减去，变为二进制减去（找商的二进制表示），由于任何数都能由二进制表示，所以该方法必定有解</li>
<li>我的实现方法<strong>越界无法规避</strong>，只能用Long</li>
</ul>
<p><strong>复杂度分析：</strong></p>
<p>没有分析的必要，由于只有32位整数，二进制一共只需要移动32次，时间复杂度与空间复杂度均为O(1)</p>
<p><strong>代码实现：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">divide</span><span class="params">(<span class="keyword">int</span> dividend, <span class="keyword">int</span> divisor)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> result  = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">boolean</span> isNegative = (dividend &lt; <span class="number">0</span> &amp; divisor &gt; <span class="number">0</span>) || (dividend &gt; <span class="number">0</span> &amp; divisor &lt; <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">long</span> temp = <span class="number">1</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//首先绝对值求解</span></span><br><span class="line">    <span class="keyword">long</span> denominator = (<span class="keyword">long</span>)Math.abs((<span class="keyword">long</span>)dividend);</span><br><span class="line">    <span class="keyword">long</span> numerator = (<span class="keyword">long</span>)Math.abs((<span class="keyword">long</span>)divisor);</span><br><span class="line">    <span class="comment">//首先找到最大元素</span></span><br><span class="line">    <span class="keyword">while</span>(numerator &lt;&lt; <span class="number">1</span> &lt;= denominator)&#123;</span><br><span class="line">        temp = temp &lt;&lt; <span class="number">1</span>;</span><br><span class="line">        numerator = numerator &lt;&lt; <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(numerator &gt;= Math.abs((<span class="keyword">long</span>)divisor))&#123;</span><br><span class="line">        <span class="keyword">if</span>(denominator &gt;= numerator)&#123;</span><br><span class="line">            denominator -= numerator;</span><br><span class="line">            <span class="keyword">if</span>(isNegative)</span><br><span class="line">                result -= temp;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                result += temp;</span><br><span class="line">        &#125;</span><br><span class="line">        numerator = numerator &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        temp = temp &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(result &gt; Integer.MAX_VALUE)&#123;</span><br><span class="line">        result = Integer.MAX_VALUE;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">int</span>)result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="思路2：条件约束-二分查找（答案的方法看得我头疼）"><a href="#思路2：条件约束-二分查找（答案的方法看得我头疼）" class="headerlink" title="思路2：条件约束+二分查找（答案的方法看得我头疼）"></a>思路2：条件约束+二分查找（答案的方法看得我头疼）</h4>]]></content>
      <categories>
        <category>力扣刷题</category>
      </categories>
      <tags>
        <tag>位运算</tag>
      </tags>
  </entry>
  <entry>
    <title>lc798.得分最高的最小轮调</title>
    <url>/2022/03/13/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/lc798.%E5%BE%97%E5%88%86%E6%9C%80%E9%AB%98%E7%9A%84%E6%9C%80%E5%B0%8F%E8%BD%AE%E8%B0%83/</url>
    <content><![CDATA[<h2 id="798-得分最高的最小轮调"><a href="#798-得分最高的最小轮调" class="headerlink" title="798.得分最高的最小轮调"></a><a href="https://leetcode-cn.com/problems/smallest-rotation-with-highest-score/">798.得分最高的最小轮调</a></h2><p>第一次写一点思路都没有，扣了半天最终放弃，直奔题解，发现题目主要有两个难点</p>
<ol>
<li>从轮调位置角度考虑转换到每个元素位置考虑<ul>
<li>我的思路一直局限在从选k出发，如何计算出每个k位置的分数？怎么找到一种贪心或者动态规状态传递的方式</li>
<li>没有从元素的角度出发，某个元素满足小于等于index时，k一定在某个范围内，所有元素决定的k的范围交集次数最多的就是最优解（表达不出来这种思维的转变）</li>
</ul>
</li>
<li>如何记录最大交集次数？<ul>
<li>看完一半题解就想到创建一个数组，每计算出一个k的范围，就将范围内记录全部加一</li>
<li>题解提供的差分数组思路”针不错”</li>
</ul>
</li>
</ol>
<span id="more"></span>
<p>如何<strong>计算k的范围</strong>，以index=i处元素为例分情况讨论（基本思路：轮调等价于数组开头一部分元素拼接到数组末尾或者末尾一部分元素拼接到数组开头）</p>
<ol>
<li><p>$i &lt; nums[i]$ 即元素值大于索引，必须增大元素索引才能满足条件</p>
<ul>
<li>左边至少增加 $nums[i] - i$ ，也就意味着 轮调位置k必须在当前元素后面，且k<strong>后边元素个数</strong>必须大于等于当前元素左边至少要增加的元素,可得公式为 $ nums.length - k &gt;= nums[i] - i$ ，得到k的范围为<script type="math/tex; mode=display">
k \in [i+ 1, nums.length - nums[i] +i]</script></li>
</ul>
</li>
<li><p>$i &gt;= nums[i]$ 即索引值大于等于元素值，由于已经满足条件，可以如条件继续在左边增加元素（k在当前位置右边），或者从左边删掉一部分元素（k在当前位置左边）</p>
<ul>
<li><p>左边增加元素个数任意（k在当前位置右边），即k大于i时始终成立</p>
<script type="math/tex; mode=display">
k \in [i + 1, nums.length - 1]</script></li>
<li><p>左边删除一定数量元素，即最多$ i - nums[i]$，</p>
<script type="math/tex; mode=display">
k \in [0, i - nums[i]]</script></li>
</ul>
</li>
</ol>
<ul>
<li>最终第二种情况k范围为<script type="math/tex; mode=display">
k \in [0, i - nums[i]] \cup [i + 1, nums.length - 1]</script></li>
</ul>
<p>差分数组理解：</p>
<ol>
<li>差分数组元素定义为：<strong>differ[i] = nums[i] - nums[i - 1]</strong>，实际代表原数组的与前一个位置元素的差（可以理解为每个元素的参考都是前一个元素）</li>
<li>若要将原数组中<strong>某个区间内元素统一加某个值</strong>，在开始加 $differ[start] + number$，在结束后一个位置减  $differ[end + 1] - number$ </li>
</ol>
<p>如何更新差分数组：</p>
<ol>
<li><p>只需要执行两种操作，<strong>区间的左边界 + 1，区间的右边界右边-1</strong></p>
</li>
<li><p>两种情况的differ数组更新情况</p>
<ol>
<li>$i &lt; nums[i]$ <ul>
<li>differ[i + 1] + 1，differ[nums.length-nums[i] + i + 1] - 1</li>
</ul>
</li>
<li><p>$i &gt;= nums[i]$ </p>
<ul>
<li>differ[i + 1] + 1,右边界越界，不需要记录</li>
<li>differ[0] + 1, differ[i - nums[i] + 1] - 1</li>
</ul>
</li>
<li><p>$ (nums.length + i - nums[i]) % nums.length$ 可以等价与两个右边界情况</p>
</li>
</ol>
</li>
</ol>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">bestRotation</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] differ = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span> tempLow;</span><br><span class="line">        <span class="keyword">int</span> tempHigh;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; nums.length;i++)&#123;</span><br><span class="line">            tempLow = i + <span class="number">1</span>;</span><br><span class="line">            <span class="comment">// 精髓</span></span><br><span class="line">            tempHigh = (nums.length + i - nums[i]) % nums.length;</span><br><span class="line">            differ[tempLow]++;</span><br><span class="line">            differ[tempHigh + <span class="number">1</span>]--;</span><br><span class="line">            <span class="comment">//左右双开区间</span></span><br><span class="line">            <span class="keyword">if</span>(nums[i] &lt;= i)&#123;</span><br><span class="line">                differ[<span class="number">0</span>]++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i &lt; differ.length;i++)&#123;</span><br><span class="line">            differ[i] += differ[i - <span class="number">1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> score = differ[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i &lt; differ.length;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(differ[i] &gt; score)&#123;</span><br><span class="line">                result = i;</span><br><span class="line">                score = differ[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>力扣刷题</category>
      </categories>
      <tags>
        <tag>其他</tag>
      </tags>
  </entry>
  <entry>
    <title>lc869.重新排序得到2的幂</title>
    <url>/2021/11/21/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/lc869.%E9%87%8D%E6%96%B0%E6%8E%92%E5%BA%8F%E5%BE%97%E5%88%B02%E7%9A%84%E5%B9%82/</url>
    <content><![CDATA[<h3 id="869-重新排序得到-2-的幂"><a href="#869-重新排序得到-2-的幂" class="headerlink" title="869.重新排序得到 2 的幂"></a><a href="https://leetcode-cn.com/problems/reordered-power-of-2/">869.重新排序得到 2 的幂</a></h3><p>关键点是意识到2的幂次是有限</p>
<h3 id="解法1-暴力回溯法"><a href="#解法1-暴力回溯法" class="headerlink" title="解法1-暴力回溯法"></a>解法1-暴力回溯法</h3><p>看到重排序就想到回溯法中的排列树问题，按照排列树的标准模板求解即可,注意排除出现前导零的情况,与</p>
<p><strong>复杂度分析：</strong></p>
<ul>
<li>时间复杂度：由于第i层共有N- i个选择，<strong>时间复杂度为O(N!)</strong></li>
<li>空间复杂度：递归深度为数字的长度，<strong>空间复杂度为O(N)</strong></li>
</ul>
<p><strong>代码：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">backTrace</span><span class="params">(<span class="keyword">int</span> depth, <span class="keyword">int</span> nLength, Long curNumber, <span class="keyword">int</span>[] visited, <span class="keyword">int</span>[] digits)</span></span>&#123;</span><br><span class="line">        <span class="comment">//所有情况选取完毕</span></span><br><span class="line">        <span class="keyword">if</span>(depth == nLength)&#123;</span><br><span class="line">            <span class="keyword">return</span> isTwoPower(curNumber);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; nLength;i++)&#123;</span><br><span class="line">            <span class="comment">//未选用过当前位置数字,前导数字不能为0</span></span><br><span class="line">            <span class="keyword">if</span>(visited[i] == <span class="number">0</span> &amp;&amp; (depth != <span class="number">0</span> || digits[i] != <span class="number">0</span>))&#123;</span><br><span class="line">                visited[i] = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">if</span>(backTrace(depth + <span class="number">1</span>, nLength, curNumber * <span class="number">10</span> + digits[i] , visited, digits))</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                visited[i] = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h3 id="解法2-预先计算存储法"><a href="#解法2-预先计算存储法" class="headerlink" title="解法2-预先计算存储法"></a>解法2-预先计算存储法</h3><p>在数字的取值范围内，一共有$2^0, 2^1,……2^{29}$ 一共30个可能取到的2的幂次，事先计算所有2幂次数字各个位置的0~9 数字出现的次数，按照顺序生成字符串存储到Map中，对于目标数字，按照相同计算步骤生成字符串，查看Map中是否含有相同字符串即可。</p>
<p>不想写了贴个官方题解充个数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    Set&lt;String&gt; powerOf2Digits = <span class="keyword">new</span> HashSet&lt;String&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">reorderedPowerOf2</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        init();</span><br><span class="line">        <span class="keyword">return</span> powerOf2Digits.contains(countDigits(n));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> n = <span class="number">1</span>; n &lt;= <span class="number">1e9</span>; n &lt;&lt;= <span class="number">1</span>) &#123;</span><br><span class="line">            powerOf2Digits.add(countDigits(n));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">countDigits</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">char</span>[] cnt = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">10</span>];</span><br><span class="line">        <span class="keyword">while</span> (n &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            ++cnt[n % <span class="number">10</span>];</span><br><span class="line">            n /= <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> String(cnt);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">作者：LeetCode-Solution</span><br><span class="line">链接：https:<span class="comment">//leetcode-cn.com/problems/reordered-power-of-2/solution/zhong-xin-pai-xu-de-dao-2-de-mi-by-leetc-4fvs/</span></span><br><span class="line">来源：力扣（LeetCode）</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>力扣刷题</category>
      </categories>
      <tags>
        <tag>其他</tag>
      </tags>
  </entry>
  <entry>
    <title>前缀（字典）树总结</title>
    <url>/2021/11/21/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%89%8D%E7%BC%80%EF%BC%88%E5%AD%97%E5%85%B8%EF%BC%89%E6%A0%91%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="什么是前缀树（Trie）"><a href="#什么是前缀树（Trie）" class="headerlink" title="什么是前缀树（Trie）"></a>什么是前缀树（Trie）</h3><blockquote>
<p>In <a href="https://en.wikipedia.org/wiki/Computer_science">computer science</a>, a <strong>trie</strong>, also called <strong>digital tree</strong> or <strong>prefix tree</strong>, is a type of <a href="https://en.wikipedia.org/wiki/Search_tree">search tree</a>, a <a href="https://en.wikipedia.org/wiki/Tree_(data_structure">tree</a>) <a href="https://en.wikipedia.org/wiki/Data_structure">data structure</a> used for locating specific keys from within a set. These keys are most often <a href="https://en.wikipedia.org/wiki/String_(computer_science">strings</a>), with links between nodes defined not by the entire key, but by individual <a href="https://en.wikipedia.org/wiki/Character_(computing">characters</a>). - wikepidea</p>
</blockquote>
<p>前缀树（又叫做字典树）是一种特殊类型的<strong>多叉树</strong>，每条边代表一个一个字母，每个节点代表一个字符串（前缀），该字符串由从根节点到当前节点路径字母组成，由于节点间的父子关系，父节点字符串就相当于子节点字符串的前缀，因此称为<strong>前缀树</strong>。</p>
<p>需要特殊注意的是前缀树的根节点由于没有父节点，代表<strong>空字符串</strong></p>

<span id="more"></span>
<h4 id="前缀树结构"><a href="#前缀树结构" class="headerlink" title="前缀树结构"></a>前缀树结构</h4><p>假设给定字符串中只有<strong>小写字母</strong>，则每个节点可能有26种类型边指向孩子节点，数据结构定义如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrieTreeNode</span></span>&#123;</span><br><span class="line">    # 长度为<span class="number">26</span>的子节点数组</span><br><span class="line">    TrieTreeNode[] children;</span><br><span class="line">    # 当前节点是否为一个单词（还能够存储以当前前缀为前缀的单词数量）</span><br><span class="line">    <span class="keyword">boolean</span> isWord;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>重点在于孩子节点数组的定义，其他属性可以根据具体问题设计</p>
<h4 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h4><ul>
<li><p>insert  向前缀树中插入一个单词</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">inert</span><span class="params">(String word)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 临时的初始节点</span></span><br><span class="line">    TrieTreeNode curNode = root;</span><br><span class="line">    <span class="comment">//将word转换成从根节点到(非)叶子节点的一条路</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; word.length();i++)&#123;</span><br><span class="line">        <span class="comment">//如果不存在当前前缀，添加</span></span><br><span class="line">        <span class="keyword">if</span>(curNode.children[word.charAt(i) - <span class="string">&#x27;a&#x27;</span>] == <span class="keyword">null</span>)&#123;</span><br><span class="line">            curNode.children[word.charAt(i) - <span class="string">&#x27;a&#x27;</span>]； = <span class="keyword">new</span> TrieTreeNode();</span><br><span class="line">        &#125;</span><br><span class="line">        curNode = curNode.children[word.charAt(i) - <span class="string">&#x27;a&#x27;</span>]；</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//标示当前词语</span></span><br><span class="line">    curNode.isWord = <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>search 在前缀树中查找一个单词或者前缀</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">search</span><span class="params">(String word)</span></span>&#123;</span><br><span class="line">    <span class="comment">//从根节点搜索</span></span><br><span class="line">    TrieTreeNode curNode = root;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; word.length();i++)&#123;</span><br><span class="line">        <span class="comment">//不包含当前字母</span></span><br><span class="line">        <span class="keyword">if</span>(curNode.children[word.charAt(i)] == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        curNode = curNode.children[word.charAt(i)];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//如果是前缀,可以直接返回</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    <span class="comment">//如果是找单词，需要查看节点标识符</span></span><br><span class="line">    <span class="keyword">return</span> curNode.isWord;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="几个典型例题"><a href="#几个典型例题" class="headerlink" title="几个典型例题"></a>几个典型例题</h4><ol>
<li>lc:<a href="https://leetcode-cn.com/problems/design-add-and-search-words-data-structure/">211. 添加与搜索单词 - 数据结构设计</a><ul>
<li>前缀树结构练习题</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>算法整理</category>
      </categories>
      <tags>
        <tag>前缀树（Trie）</tag>
      </tags>
  </entry>
  <entry>
    <title>并查集整理</title>
    <url>/2021/07/04/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%B9%B6%E6%9F%A5%E9%9B%86%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h4 id="什么是并查集"><a href="#什么是并查集" class="headerlink" title="什么是并查集"></a>什么是并查集</h4><blockquote>
<p>并查集是一种树型的数据结构，用于处理一些不相交集合的合并及查询问题。</p>
<p>并查集的思想是用一个数组表示了整片森林（parent），树的根节点唯一标识了一个集合，我们只要找到了某个元素的的树根，就能确定它在哪个集合里。</p>
<p>百度百科</p>
</blockquote>
<p>并查集包括两种操作：</p>
<ol>
<li>find(x) 查询元素所属的集合</li>
<li>union(x, y) 合并两个不相关的集合</li>
</ol>
<p>我理解的并查集，就是对于一系列元素，不同元素组成不同的集合（使用树的形式描述集合），多个集合共同构成并查集（森林），提供集合与集合的合并（两棵的合并）和 查找元素所属的集合（在哪棵树）<br><span id="more"></span></p>
<h4 id="如何实现并查集"><a href="#如何实现并查集" class="headerlink" title="如何实现并查集"></a>如何实现并查集</h4><p>并查集中只关注元素属于哪个集合，集合之间的合并操作，以树形式表示集合，每个元素只需要知道自己所属树的根节点就能知道自己属于哪个集合(属于哪个树)，集合合并也可转化为树的合并，因此可使用类似于完全二叉树的数组存储方式实现并查集。</p>
<p>存储结构实现：</p>
<ul>
<li><p>使用father数组，存储元素的父节点(不直接存储根节点的原因在于，合并操作无法保证该性质)</p>
</li>
<li><p>每个元素初始化父节点为本身，表示并查集初始每个元素自成一个集合</p>
<p><code>int father[elements];</code></p>
</li>
</ul>
<p>操作实现：</p>
<ol>
<li><p>find(x) 查询元素所属集合</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 不断的向上查询父节点，直到找到当前集合的根</span><br><span class="line">int find(x)&#123;</span><br><span class="line">	root = x;</span><br><span class="line">	while(father[root] != root)&#123;</span><br><span class="line">		parent = father[root];</span><br><span class="line">	&#125;</span><br><span class="line">	return root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>union 合并集合操作</p>
<ul>
<li>找到两个集合的根节点，将其中一个根节点父节点设置为另一集合根节点</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int uninon(x,y)&#123;</span><br><span class="line">	int root_x = find(x)</span><br><span class="line">	int root_y = find(y)</span><br><span class="line">	// 以x插入到y为例</span><br><span class="line">	parent[root_x] = root_y</span><br><span class="line">    return root_y</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li><p>java实现</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DisjointSetUnion</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span>[] parent;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DisjointSetUnion</span><span class="params">(<span class="keyword">int</span> nums)</span></span>&#123;</span><br><span class="line">        parent = <span class="keyword">new</span> <span class="keyword">int</span>[nums];</span><br><span class="line">        <span class="comment">//初始化</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; nums;i++)&#123;</span><br><span class="line">            parent[i] = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(parent[x] != x)&#123;</span><br><span class="line">            x = parent[x];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">union</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> parent_x = find(x);</span><br><span class="line">        <span class="keyword">int</span> parent_y = find(y);</span><br><span class="line">        parent[parent_x] = parent_y;</span><br><span class="line">        <span class="keyword">return</span> parent_y;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="并查集优化"><a href="#并查集优化" class="headerlink" title="并查集优化"></a>并查集优化</h4><h5 id="1-路径压缩降低查找集合标示的复杂度"><a href="#1-路径压缩降低查找集合标示的复杂度" class="headerlink" title="1.路径压缩降低查找集合标示的复杂度"></a>1.路径压缩降低查找集合标示的复杂度</h5><p>在find的过程中，直接将当前的根节点接到所在集合的根节点</p>
<ul>
<li>该方法只有在查询的过程中才会优化，且只优化树的一条路径</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(x)</span></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(father[x] == x)&#123;</span><br><span class="line">		<span class="keyword">return</span> x;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="comment">//同样的找父根节点，增加了赋值操作</span></span><br><span class="line">		father[x] = find(father[x]);</span><br><span class="line">        <span class="keyword">return</span> father[x];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//一行简写法</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x == father[x] ? x : (father[x] = find(fa[x]));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="2-按秩合并的方式"><a href="#2-按秩合并的方式" class="headerlink" title="2.按秩合并的方式"></a>2.按秩合并的方式</h5><p>出发点是把简单的树往复杂的树上合并，以减少合并增加的平均树深度，定义节点的秩为以当前节点为根子树的深度，开辟秩数组，每个节点对应一个秩。</p>
<p>初始化方法修改为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//所有的秩均初始化为1</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">	father = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">	rank = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; n;i++)&#123;</span><br><span class="line">		father[i] = i;</span><br><span class="line">		rank[i] = i;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>合并方法修改为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">union</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> x = find(i), y = find(j);    <span class="comment">//先找到两个根节点</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (rank[x] &lt;= rank[y])</span><br><span class="line">        fa[x] = y;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        fa[y] = x;</span><br><span class="line">    <span class="comment">//深度相同时，根节点深度+1,深度不同，插入子树不影响深度</span></span><br><span class="line">    <span class="keyword">if</span> (rank[x] == rank[y] &amp;&amp; x != y)</span><br><span class="line">        rank[y]++;                   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="并查集应用"><a href="#并查集应用" class="headerlink" title="并查集应用"></a>并查集应用</h4><ol>
<li><p>洛谷p1551 亲戚问题</p>
 <img src="/2021/07/04/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%B9%B6%E6%9F%A5%E9%9B%86%E6%95%B4%E7%90%86/image-20210509112121807.png" class="">
<ul>
<li><p>简答的并查集思路，求是否具有亲戚关系，即判断两元素是否在一个集合中</p>
</li>
<li><p>java代码实现:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Main</span></span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span>[] parent;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(parent[x] != x)&#123;</span><br><span class="line">            x = parent[x];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">union</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> parent_x = find(x);</span><br><span class="line">        <span class="keyword">int</span> parent_y = find(y);</span><br><span class="line">        parent[parent_x] = parent_y;</span><br><span class="line">        <span class="keyword">return</span> parent_y;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">int</span> n,m,p;</span><br><span class="line">        n = scanner.nextInt();</span><br><span class="line">        m = scanner.nextInt();</span><br><span class="line">        p = scanner.nextInt();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//初始化并查集</span></span><br><span class="line">        parent = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; parent.length;i++)&#123;</span><br><span class="line">            parent[i] = i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//读取关系合并并查集</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; m;i++)&#123;</span><br><span class="line">            union(scanner.nextInt() - <span class="number">1</span>,scanner.nextInt() - <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//判断两个集合是否在同一个集合中</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; p;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(find(scanner.nextInt() - <span class="number">1</span>) == find(scanner.nextInt() - <span class="number">1</span>))&#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;Yes&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;No&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>岛屿数量问题(LC <a href="https://leetcode-cn.com/problems/number-of-islands/">200. 岛屿数量</a>)</p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="comment">//并查集方法</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">DisjointUnion</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> count;</span><br><span class="line">        <span class="keyword">int</span>[] parents;</span><br><span class="line">        <span class="keyword">int</span>[] rank;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">DisjointUnion</span><span class="params">(<span class="keyword">char</span>[][] grid)</span></span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.count = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">this</span>.parents = <span class="keyword">new</span> <span class="keyword">int</span>[grid.length * grid[<span class="number">0</span>].length];</span><br><span class="line">            <span class="keyword">this</span>.parents = <span class="keyword">new</span> <span class="keyword">int</span>[grid.length * grid[<span class="number">0</span>].length];</span><br><span class="line">            <span class="keyword">this</span>.rank = <span class="keyword">new</span> <span class="keyword">int</span>[grid.length * grid[<span class="number">0</span>].length];</span><br><span class="line">            <span class="comment">//初始化集合，每个元素1自成一个集合</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; grid.length;i++)&#123;</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j &lt; grid[<span class="number">0</span>].length;j++)&#123;</span><br><span class="line">                    <span class="keyword">if</span>(grid[i][j] == <span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">                        <span class="keyword">this</span>.count++;</span><br><span class="line">                        <span class="comment">//集合标识为自己</span></span><br><span class="line">                        parents[i * grid[<span class="number">0</span>].length + j] = i * grid[<span class="number">0</span>].length + j;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">this</span>.rank[i * grid[<span class="number">0</span>].length + j] = <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(x != parents[x])&#123;</span><br><span class="line">                parents[x] = find(parents[x]);</span><br><span class="line">                x = parents[x];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> x;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">union</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">            <span class="keyword">int</span> parentX = find(x);</span><br><span class="line">            <span class="keyword">int</span> parentY = find(y);</span><br><span class="line">            <span class="comment">//是否为不同的集合</span></span><br><span class="line">            <span class="keyword">if</span>(parentX != parentY)&#123;</span><br><span class="line">                <span class="keyword">this</span>.count--;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//添加秩的合并操作</span></span><br><span class="line">                <span class="keyword">if</span>(rank[parentX] &lt;= rank[parentY])&#123;</span><br><span class="line">                    parents[parentX] = parentY;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span>&#123;</span><br><span class="line">                    parents[parentY] = parentX;</span><br><span class="line">                    <span class="keyword">if</span>(rank[parentX] == parentY)&#123;</span><br><span class="line">                        rank[parentY]++;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getCount</span><span class="params">()</span></span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">numIslands</span><span class="params">(<span class="keyword">char</span>[][] grid)</span> </span>&#123;</span><br><span class="line">        DisjointUnion disjointUnion = <span class="keyword">new</span> DisjointUnion(grid);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; grid.length;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j &lt; grid[<span class="number">0</span>].length;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(grid[i][j] == <span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">                    <span class="keyword">if</span>(i - <span class="number">1</span> &gt;= <span class="number">0</span> &amp;&amp; grid[i - <span class="number">1</span>][j] == <span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">                        disjointUnion.union(i * grid[<span class="number">0</span>].length + j, (i - <span class="number">1</span>) * grid[<span class="number">0</span>].length + j);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span>(j - <span class="number">1</span> &gt;= <span class="number">0</span> &amp;&amp; grid[i][j - <span class="number">1</span>] == <span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">                        disjointUnion.union(i * grid[<span class="number">0</span>].length + j, i * grid[<span class="number">0</span>].length + j - <span class="number">1</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span>(i + <span class="number">1</span> &lt; grid.length &amp;&amp; grid[i + <span class="number">1</span>][j] == <span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">                        disjointUnion.union(i * grid[<span class="number">0</span>].length + j, (i +<span class="number">1</span>) * grid[<span class="number">0</span>].length + j);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span>(j + <span class="number">1</span> &lt; grid[<span class="number">0</span>].length &amp;&amp; grid[i][j + <span class="number">1</span>] == <span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">                        disjointUnion.union(i * grid[<span class="number">0</span>].length + j, i * grid[<span class="number">0</span>].length + j + <span class="number">1</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> disjointUnion.getCount();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法整理</category>
      </categories>
      <tags>
        <tag>并查集</tag>
      </tags>
  </entry>
  <entry>
    <title>快速幂整理</title>
    <url>/2021/07/09/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%BF%AB%E9%80%9F%E5%B9%82%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h3 id="1-什么是快速幂"><a href="#1-什么是快速幂" class="headerlink" title="1 什么是快速幂"></a>1 什么是快速幂</h3><p>在求幂次操作时，一般采用逐个相乘的方式，求多少阶幂次，就需要进行多少次乘法，乘法的时间复杂度为<strong>O(N)</strong>，通过引入”备忘录“和二分思想，将乘法次数从 <script type="math/tex">N</script> 降低到 <script type="math/tex">log_2N</script></p>
<h4 id="主要思想："><a href="#主要思想：" class="headerlink" title="主要思想："></a>主要思想：</h4><ol>
<li>求 N幂次问题 转化为 求两个 N/2幂次的乘积</li>
<li>两个相同的 N/2幂次子问题，只需要求解一次<span id="more"></span>
<h4 id="1-1-递归伪代码："><a href="#1-1-递归伪代码：" class="headerlink" title="1.1 递归伪代码："></a>1.1 递归伪代码：</h4></li>
</ol>
<ul>
<li>递归实现较为简洁，但是会存在递归栈占用，递归树深度为<script type="math/tex">logN</script></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getPow</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(e == <span class="number">0</span>)&#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//计算子问题</span></span><br><span class="line">    <span class="keyword">int</span> temp = getPow(x, e / <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">int</span> result = temp * temp;</span><br><span class="line">    <span class="keyword">if</span>(e % <span class="number">2</span> != <span class="number">0</span>)&#123;</span><br><span class="line">        result *= x;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="1-2-迭代伪代码："><a href="#1-2-迭代伪代码：" class="headerlink" title="1.2 迭代伪代码："></a>1.2 <strong>迭代伪代码</strong>：</h4><p>观察递归代码，可以得到递归过程类似于将指数e转化为二进制的过程</p>
<ul>
<li>在底数转化二进制的某位为1时，对应递归子问题为奇数时乘以底数，</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if(e % 2 != 0)&#123;</span><br><span class="line">    result *= x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>在递归返回时，每向上返回一层 该位置乘的底数，都要平方一次</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> result = temp * temp;</span><br></pre></td></tr></table></figure>
<p>根据以上分析可得递归代码为</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getPow</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> e)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> result = <span class="number">1</span>;</span><br><span class="line">	<span class="keyword">while</span>(e &gt; <span class="number">0</span>)&#123;</span><br><span class="line">		<span class="keyword">if</span>(e % <span class="number">2</span> != <span class="number">0</span>)&#123;</span><br><span class="line">			result *= x;</span><br><span class="line">		&#125;</span><br><span class="line">        <span class="comment">//阶数向右移动</span></span><br><span class="line">        x *= x;</span><br><span class="line">        <span class="comment">//等价于从底向上递归</span></span><br><span class="line">		e &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="1-3-添加了避免越界的模板"><a href="#1-3-添加了避免越界的模板" class="headerlink" title="1.3 添加了避免越界的模板"></a>1.3 添加了避免越界的模板</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getPow</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> e)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> result = <span class="number">1</span>;</span><br><span class="line">	<span class="keyword">while</span>(e &gt; <span class="number">0</span>)&#123;</span><br><span class="line">		<span class="keyword">if</span>(e % <span class="number">2</span> != <span class="number">0</span>)&#123;</span><br><span class="line">			result = result * x mod number;</span><br><span class="line">		&#125;</span><br><span class="line">        <span class="comment">//阶数向右移动</span></span><br><span class="line">        x = x * x mod number;</span><br><span class="line">        <span class="comment">//等价于从底向上递归</span></span><br><span class="line">		e &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="1-4-快速幂扩展"><a href="#1-4-快速幂扩展" class="headerlink" title="1.4 快速幂扩展"></a>1.4 快速幂扩展</h4><p>快速幂思路不止可以应用在整数乘积上，同样可扩展到类似的矩阵乘积</p>
<h3 id="2-典型例题"><a href="#2-典型例题" class="headerlink" title="2 典型例题"></a>2 典型例题</h3><h4 id="2-1-洛谷-P3390-【模板】矩阵快速幂"><a href="#2-1-洛谷-P3390-【模板】矩阵快速幂" class="headerlink" title="2.1 洛谷 P3390 【模板】矩阵快速幂"></a>2.1 洛谷 P3390 【模板】<a href="https://www.luogu.com.cn/problem/solution/P3390">矩阵快速幂</a></h4><p>难点在于避免越界和矩阵乘法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="keyword">static</span> <span class="keyword">int</span> MOD_NUMBER = <span class="number">100000007</span>;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">long</span>[][] matMul(<span class="keyword">long</span>[][] mat1, <span class="keyword">long</span>[][] mat2)&#123;</span><br><span class="line">      <span class="keyword">if</span>(mat1 == <span class="keyword">null</span>)&#123;</span><br><span class="line">          <span class="keyword">return</span> mat2;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//乘积结果存储在result中</span></span><br><span class="line">      <span class="keyword">long</span>[][] result = <span class="keyword">new</span> <span class="keyword">long</span>[mat1.length][mat1.length];</span><br><span class="line">      <span class="keyword">long</span> temp;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; mat1.length;i++)&#123;</span><br><span class="line">          <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j &lt; mat1.length;j++)&#123;</span><br><span class="line">              temp = <span class="number">0</span>;</span><br><span class="line">              <span class="comment">//第i行与第j列相乘</span></span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> m = <span class="number">0</span>; m &lt; mat1.length;m++)&#123;</span><br><span class="line">                  <span class="comment">//避免越界的关键点</span></span><br><span class="line">                  temp  = (temp + mat1[i][m] * mat2[m][j] % MOD_NUMBER) % MOD_NUMBER;</span><br><span class="line">              &#125;</span><br><span class="line">              result[i][j] = temp;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">      Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">      <span class="keyword">int</span> n = scanner.nextInt();</span><br><span class="line">      Long k = scanner.nextLong();</span><br><span class="line">      <span class="keyword">long</span>[][] aimMatrix = <span class="keyword">new</span> <span class="keyword">long</span>[n][n];</span><br><span class="line">      <span class="comment">//读取matrix数组</span></span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; n;i++)&#123;</span><br><span class="line">          <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j &lt; n;j++)&#123;</span><br><span class="line">              aimMatrix[i][j] = scanner.nextLong();</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">long</span>[][] result = <span class="keyword">null</span>;</span><br><span class="line">      <span class="keyword">while</span>(k &gt; <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="keyword">if</span> (k % <span class="number">2</span> != <span class="number">0</span>) &#123;</span><br><span class="line">              result = matMul(result, aimMatrix);</span><br><span class="line">          &#125;</span><br><span class="line">          k &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">          aimMatrix = matMul(aimMatrix, aimMatrix);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//输出结果集合,省略</span></span><br><span class="line">System.....</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<img src="/2021/07/09/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%BF%AB%E9%80%9F%E5%B9%82%E6%95%B4%E7%90%86/image-20210706154401051.png" class="" title="image-20210706154401051">
<h4 id="2-2-力扣-1922-统计好数字的数目"><a href="#2-2-力扣-1922-统计好数字的数目" class="headerlink" title="2.2 力扣 1922. 统计好数字的数目"></a>2.2 力扣 <a href="https://leetcode-cn.com/problems/count-good-numbers/">1922. 统计好数字的数目</a></h4><p>难点在于将题目转化为求幂形式，<strong>通过归纳得到数组位数每增加两位，好数字数目*20</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">countGoodNumbers</span><span class="params">(<span class="keyword">long</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//设置初始化值</span></span><br><span class="line">        <span class="keyword">long</span> result = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(n % <span class="number">2</span> == <span class="number">1</span>)&#123;</span><br><span class="line">            result = <span class="number">5</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        n = n / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">long</span> a = <span class="number">20</span>;</span><br><span class="line">        <span class="comment">//快速幂模板</span></span><br><span class="line">        <span class="keyword">while</span>(n &gt; <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(n % <span class="number">2</span> == <span class="number">1</span>)&#123;</span><br><span class="line">                result = result * a % <span class="number">1000000007</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//右移一位</span></span><br><span class="line">            n &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">            a = a * a % <span class="number">1000000007</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> (<span class="keyword">int</span>)result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<img src="/2021/07/09/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%BF%AB%E9%80%9F%E5%B9%82%E6%95%B4%E7%90%86/image-20210706155441634.png" class="" title="image-20210706155441634">]]></content>
      <categories>
        <category>算法整理</category>
      </categories>
      <tags>
        <tag>快速幂</tag>
      </tags>
  </entry>
  <entry>
    <title>树状DP整理</title>
    <url>/2021/07/21/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%A0%91%E7%8A%B6DP%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h3 id="什么是树状DP"><a href="#什么是树状DP" class="headerlink" title="什么是树状DP"></a>什么是树状DP</h3><p>建立在 ”树“ 这一数据结构上的DP问题，难度介于线性dp和图dp之间，当前节点的状态可能取决于父亲节点或者孩子节点，即存在两种状态转移方向，树可能包括二叉树和多叉树。</p>
<p>难点还是如何找到状态转移方程</p>
<h4 id="主要题目类型"><a href="#主要题目类型" class="headerlink" title="主要题目类型"></a>主要题目类型</h4><ol>
<li><p>最大独立子集合问题</p>
<p>给一无向图，找出一个点集，使得任意两点之间都没有连边，这个点集就是独立集。而点最多的独立集，就是最大独立集，针对不问题，选取点的条件可能发生变化，但总体还是在限定条件下，选择最优的点集</p>
</li>
<li><p>最小点覆盖</p>
</li>
<li><p>最小支配集</p>
<span id="more"></span>
</li>
</ol>
<h3 id="典型例题"><a href="#典型例题" class="headerlink" title="典型例题"></a>典型例题</h3><h4 id="1-最大独立子集合问题-洛谷P1352-没有上司的舞会"><a href="#1-最大独立子集合问题-洛谷P1352-没有上司的舞会" class="headerlink" title="1. 最大独立子集合问题-洛谷P1352 没有上司的舞会"></a>1. 最大独立子集合问题-洛谷P1352 <a href="https://www.luogu.com.cn/problem/P1352">没有上司的舞会</a></h4><p>每个节点均有两个状态，当前节点参加和当前节点不参加的最优解，状态转移公式如下</p>
<script type="math/tex; mode=display">
dp_{in}[parent] = dp_{out}[child_1] + dp_{out}[child_1]+...+dp_{out}[child_n]</script><ul>
<li>如果选取父节点，孩子节点不能选取</li>
</ul>
<script type="math/tex; mode=display">
dp_{out}[parent] = max(dp_{out}[child_1],dp_{in}[child_1]) + ...+max(dp_{out}[child_n],dp_{in}[child_n])</script><ul>
<li>如果不选取父节点，子节点可选可不不选，本题选择两者较大</li>
</ul>
<h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span></span>&#123;</span><br><span class="line">        List&lt;TreeNode&gt; children;</span><br><span class="line">        <span class="keyword">int</span> happiness;</span><br><span class="line">        TreeNode(<span class="keyword">int</span> happiness)&#123;</span><br><span class="line">            <span class="keyword">this</span>.children = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="keyword">this</span>.happiness = happiness;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] dfs(TreeNode cur)&#123;</span><br><span class="line">        <span class="comment">//选择当前节点与不选择当前节点两种解</span></span><br><span class="line">        <span class="keyword">int</span>[] totalHappiness = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">2</span>];</span><br><span class="line">        totalHappiness[<span class="number">0</span>] = cur.happiness;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span>[] childHappiness;</span><br><span class="line">        <span class="keyword">for</span>(TreeNode child:cur.children)&#123;</span><br><span class="line">            childHappiness = dfs(child);</span><br><span class="line">            <span class="comment">//父亲选了，孩子不能选</span></span><br><span class="line">            totalHappiness[<span class="number">0</span>] += childHappiness[<span class="number">1</span>];</span><br><span class="line">            <span class="comment">//父亲没选，孩子可选可不选</span></span><br><span class="line">            totalHappiness[<span class="number">1</span>] += Math.max(childHappiness[<span class="number">0</span>],childHappiness[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> totalHappiness;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		<span class="comment">//建立二叉树的过程省略</span></span><br><span class="line">        <span class="comment">//....</span></span><br><span class="line">        <span class="comment">//dfs遍历</span></span><br><span class="line">        <span class="keyword">int</span>[] result = dfs(root);</span><br><span class="line">        System.out.println(Math.max(result[<span class="number">0</span>], result[<span class="number">1</span>]));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<img src="/2021/07/21/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%A0%91%E7%8A%B6DP%E6%95%B4%E7%90%86/image-20210721144347304.png" class="" title="image-20210721144347304">
<h4 id="2-洛谷P2015-二叉苹果树"><a href="#2-洛谷P2015-二叉苹果树" class="headerlink" title="2. 洛谷P2015 二叉苹果树"></a>2. 洛谷P2015 <a href="https://www.luogu.com.cn/problem/P2015">二叉苹果树</a></h4><p>假设求节点 <script type="math/tex">i</script> 保留 <script type="math/tex">j</script> 条边的最优情况，其状态转移公式如下</p>
<script type="math/tex; mode=display">
dp[i][j] = max(dp[i][j], dp[u][i - k - 1] + dp[v][k] + weight[i][v])</script><p>其中 <script type="math/tex">v</script> 为当前节点的某个子节点，<script type="math/tex">weight[i][v]</script> 为连接子节点的遍历的权重，需要遍历所有子节点以及所有保留边情况，计算得到当前节点的所有候选状态。</p>
<p><strong>难理解的几个点</strong></p>
<ol>
<li><p>为什么状态转移是 <script type="math/tex">dp[u][i - k - 1] + dp[v][k] + weight[i][v]</script></p>
<ul>
<li>我们通常理解的二叉树或者多叉树的子问题划分，一定是将保留的边分配给所有的子节点，看每个子节点最最优情况，最后求和为当前节点最优情况，以二叉树为例子，即 <script type="math/tex">dp[left][i - k - 1] + dp[right][k] + weight[left][v] + + weight[right][v]</script></li>
<li>该状态转移公式，并不从所有的子节点出发，而是将保留的边分配给当前已遍历的子节点，即默认未遍历子树中的边全部删除情况，每遍历到一个新的孩子节点，重新考虑当前子节点分配边的情况</li>
<li>形象的例子就是，分苹果给张三、李四、王五，第一种思路是直接把三个人叫过来，看如何分成三份；另一种是先把张三叫过来，记录下来 把苹果从 0-全部 给他的情况，再叫李四，看给张三后，剩下不同苹果的基础上，给李四 0-全部 的最优情况。最后把王五拉过来，看给张三李四分完剩下，给他分怎么最优。</li>
</ul>
<p>即一种是直接考虑所有的节点之间分配，另一种是分成 已遍历 + 未遍历，未遍历节点不断地加入已遍历集合。<strong>第二种思路适用范围更广</strong>，如果问题变为多叉或者不定叉树，第一种思路代码无法实现</p>
</li>
<li><p>遍历部分的实现，<strong>为什么要倒序DP</strong>，即保留边数从大到小</p>
<ul>
<li>第一层遍历，遍历当前节点保留不同边的情况，最大边数 <script type="math/tex">j</script> 取<strong>已遍历子节点子树中边的个数和</strong>与<strong>要求保留边个数</strong>的较小值，<strong>倒序DP</strong></li>
<li>第二层遍历，遍历当前新节点的所有保留边数情况，最大边数 <script type="math/tex">k</script> 取 <strong><script type="math/tex">j-1</script></strong> 与 <strong>当前子节点子树边数</strong>的较小值，<strong>顺序无所谓</strong></li>
</ul>
<p>划分为子问题后，需要与原数组的前部元素求和，所以求大时会用到小，如果正序，前部修改导致后部计算使用的新计算的状态</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = Math.min(childEdges[curNode], m);j &gt;= <span class="number">1</span>;j--)&#123;</span><br><span class="line">    <span class="comment">//k为当前子节点中保留边的个数</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> k = Math.min(j - <span class="number">1</span>, childEdges[i]);k &gt;= <span class="number">0</span>;k--)&#123;</span><br><span class="line">        dpArray[curNode][j] = Math.max(dpArray[curNode][j], dpArray[curNode][j - k - <span class="number">1</span>] + dpArray[i][k] + adjustMatrix[curNode][i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h5 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h5><p>基于dfs+邻接矩阵实现</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> curNode, <span class="keyword">int</span>[][] adjustMatrix,<span class="keyword">int</span>[][] dpArray,<span class="keyword">int</span>[] childEdges, <span class="keyword">int</span>[] visited, <span class="keyword">int</span> m)</span></span>&#123;</span><br><span class="line">        visited[curNode] = <span class="number">1</span>;</span><br><span class="line">        <span class="comment">//首先dfs子树，求子问题解并获得子结点个数</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; adjustMatrix.length;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(adjustMatrix[curNode][i] != -<span class="number">1</span> &amp;&amp; visited[i] != <span class="number">1</span>)&#123;</span><br><span class="line">                visited[i] = <span class="number">1</span>;</span><br><span class="line">                dfs(i, adjustMatrix, dpArray, childEdges, visited, m);</span><br><span class="line">                <span class="comment">//已遍历子树边数和</span></span><br><span class="line">                childEdges[curNode] += childEdges[i] + <span class="number">1</span>;</span><br><span class="line">                <span class="comment">//遍历当前节点保留不同数量树枝的解</span></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> j = Math.min(childEdges[curNode], m);j &gt;= <span class="number">1</span>;j--)&#123;</span><br><span class="line">                    <span class="comment">//k为当前子节点中保留边的个数</span></span><br><span class="line">                    <span class="keyword">for</span>(<span class="keyword">int</span> k = Math.min(j - <span class="number">1</span>, childEdges[i]);k &gt;= <span class="number">0</span>;k--)&#123;</span><br><span class="line">                        dpArray[curNode][j] = Math.max(dpArray[curNode][j], dpArray[curNode][j - k - <span class="number">1</span>] + dpArray[i][k] + adjustMatrix[curNode][i]);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="comment">// 节点和边的数量</span></span><br><span class="line">        <span class="keyword">int</span> m,n;</span><br><span class="line">        <span class="keyword">int</span>[][] adjustMatrix;</span><br><span class="line">        m = scanner.nextInt();</span><br><span class="line">        n = scanner.nextInt();</span><br><span class="line">        adjustMatrix = <span class="keyword">new</span> <span class="keyword">int</span>[m][m];</span><br><span class="line">        <span class="comment">//初始化矩阵</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; m;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j &lt; m;j++)&#123;</span><br><span class="line">                adjustMatrix[i][j] = -<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//读取边</span></span><br><span class="line">        <span class="keyword">int</span> tempNode1;</span><br><span class="line">        <span class="keyword">int</span> tempNode2;</span><br><span class="line">        <span class="keyword">int</span> tempWeight;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; m - <span class="number">1</span>;i++)&#123;</span><br><span class="line">            tempNode1 = scanner.nextInt() - <span class="number">1</span>;</span><br><span class="line">            tempNode2 = scanner.nextInt() - <span class="number">1</span>;</span><br><span class="line">            tempWeight = scanner.nextInt();</span><br><span class="line">            adjustMatrix[tempNode1][tempNode2] = tempWeight;</span><br><span class="line">            adjustMatrix[tempNode2][tempNode1] = tempWeight;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//dfs动态规划</span></span><br><span class="line">        <span class="keyword">int</span>[] visited = <span class="keyword">new</span> <span class="keyword">int</span>[m];</span><br><span class="line">        <span class="keyword">int</span>[] childEdges = <span class="keyword">new</span> <span class="keyword">int</span>[m];</span><br><span class="line">        <span class="keyword">int</span>[][] dpArray = <span class="keyword">new</span> <span class="keyword">int</span>[m][n + <span class="number">1</span>];</span><br><span class="line">        dfs(<span class="number">0</span>, adjustMatrix, dpArray, childEdges, visited, n);</span><br><span class="line">        System.out.println(dpArray[<span class="number">0</span>][n]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<img src="/2021/07/21/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%A0%91%E7%8A%B6DP%E6%95%B4%E7%90%86/image-20210719171204593.png" class="" title="image-20210719171204593">
]]></content>
      <categories>
        <category>算法整理</category>
      </categories>
      <tags>
        <tag>树状DP</tag>
      </tags>
  </entry>
  <entry>
    <title>树状数组整理</title>
    <url>/2022/04/10/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h2 id="树状数组（Binary-indexed-tree）"><a href="#树状数组（Binary-indexed-tree）" class="headerlink" title="树状数组（Binary indexed tree）"></a>树状数组（Binary indexed tree）</h2><blockquote>
<p>A <strong>Fenwick tree</strong> or <strong>binary indexed tree</strong> is a data structure that can efficiently update elements and calculate <a href="https://en.wikipedia.org/wiki/Prefix_sum">prefix sums</a> in a table of numbers.</p>
<p>来自Wikipedia</p>
</blockquote>
<p>树状数组是一种求解前缀和问题的简单数据结构，能够在<strong>O(logn)</strong>的时间复杂度内解决区间求和问题，主要支持两种操作</p>
<ol>
<li><strong>单点修改</strong></li>
<li><strong>区间操作</strong></li>
</ol>
<h3 id="主要思路"><a href="#主要思路" class="headerlink" title="主要思路"></a>主要思路</h3><p>类比<strong>任何数</strong>均可以拆分为<strong>有限个不同2的幂次</strong>的求和，原论文作者认为<strong>一串序列</strong>的求和同样可以拆分为<strong>有限个不同长度为2的幂次的序列</strong>的和。在数字的拆分中，其二进制表示中1的个数即为不同2的幂次，例如 “10”，“12”</p>
<script type="math/tex; mode=display">
10 -> 1010 = 1000 + 0010\\
14 -> 1110 = 1000 + 0100 + 0010</script><span id="more"></span>
<p>自然想到一个问题：如何快速的找到<strong>任意数的不同二次幂的组合</strong>？引出了<strong>lowbit思路</strong>:</p>
<ol>
<li><p>通过<strong>原码与补码求逻辑与运算</strong>，求得目标数字的<strong>最右边的一个1</strong></p>
<script type="math/tex; mode=display">
lowbit(x) = (x)\&(-x)</script></li>
<li><p>原数字减去lowbit，获得少了一位1的数字</p>
<script type="math/tex; mode=display">
x_i = x_{i-1} - lowbit(x_{i-1})</script></li>
<li><p>重复操作，直到 $x_i = 0$ </p>
</li>
</ol>
<p>重复此过程每一步计算得到的 $lowbit(x_i)$ 即为原数字的不同2的幂次拆分，以14为例:</p>
<script type="math/tex; mode=display">
lowbit(14) = 0010 \\
\downarrow \\
lowbit(14 - 0010 = 12) = 0100\\
\downarrow \\
lowbit(12 - 0100 = 8) = 1000 \\
\downarrow \\
lowbit(0 - 1000 = 0) = 0000</script><h4 id="如何将对应拆分思路迁移到前缀和？"><a href="#如何将对应拆分思路迁移到前缀和？" class="headerlink" title="如何将对应拆分思路迁移到前缀和？"></a>如何将对应拆分思路迁移到前缀和？</h4><p>不妨用 $C[i]$ 表示 目标数组$f[1]…f[i]$的前缀和，通过$lowerbit$操作，我们可以快速找到<strong>任意区间长度</strong> 对应的<strong>二次幂区间长度的组合</strong>。</p>
<p>问题是我们如何设计存储方式，提前存储这些二次幂区间长度的区间和？观察14转化为组合的过程，对应到区间表达形式为:</p>
<script type="math/tex; mode=display">
(0,14] = (12, 14] + (8, 12] + (0, 8] \\
\updownarrow \\
(0,14] = (14 - lowbit(14), 14] + (12 - lowbit(12), 12] + (8 - lowbit(8), 8] \\
\updownarrow \\
(0,1110] = (1100, 1110] + (1000, 1100] + (0000, 1000]\\</script><p>可以得到每个区间右边界可以由上个区间右边界减去lobit得到，而区间的长度等于当前区间右边界的lowbit（<strong>拆分区间的递推关系</strong>），自然可以想到，将区间$(i - lowbit(i),i]$的和存储到 <strong>index = i</strong>的位置</p>
<ul>
<li>因此定义$tree$数组，对于<strong>任意$tree[i]$，其存储范围为 $(i-lowbit(i),i]$区间的和</strong>，如下：</li>
</ul>
<script type="math/tex; mode=display">
tree[i] = \sum^i_{j=i-lowbit(i) + 1}f[j]</script><p>任意前缀和 $C[i]$计算，按照上述拆分过程，可以转化为不断的减去最右边一的过程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for(int index = i;index &gt; 0;index -= lowbit(index)):</span><br><span class="line">	//index -= lowbit(index) 就是类似于14拆分过程中的找所有2次幂过程</span><br><span class="line">	C[i] += tree[index]</span><br></pre></td></tr></table></figure>
<h4 id="如何支持单点修改"><a href="#如何支持单点修改" class="headerlink" title="如何支持单点修改"></a>如何支持单点修改</h4><p>若修改 $f[i]$，需要修改所有包含$f[i]$ 的 $tree[index]$, 我们要找到所有包含当前元素的区间，假设 $tree[idx]$ 包含 $f[i]$，则 $i$一定满足:</p>
<script type="math/tex; mode=display">
idx - lowbit(idx) < i <= idx</script><p>来自博客<a href="http://duanple.blog.163.com/blog/static/7097176720081131113145832/"><strong>Binary indexed tree-树状数组</strong> </a>的形象解释</p>
<img src="/2022/04/10/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84%E6%95%B4%E7%90%86/image-20220405190706777.png" class="" title="image-20220405190706777">
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="lowbit-函数"><a href="#lowbit-函数" class="headerlink" title="lowbit()函数"></a>lowbit()函数</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">lowbit</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">	<span class="keyword">return</span> x &amp; -x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="求前缀和"><a href="#求前缀和" class="headerlink" title="求前缀和"></a>求前缀和</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//预先处理好的子区间和数组</span></span><br><span class="line"><span class="keyword">int</span> tree[maxIndex]</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">query</span><span class="params">(<span class="keyword">int</span> index)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span>(;index &gt; <span class="number">0</span>;index++)&#123;</span><br><span class="line">		result += tree[index];</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="更新方法"><a href="#更新方法" class="headerlink" title="更新方法"></a>更新方法</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//预先处理好的子区间和数组</span></span><br><span class="line"><span class="keyword">int</span> tree[maxIndex]</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">update</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">int</span> val)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> pos = index; pos &lt; maxIndex; pos += lowbit(pos))</span><br><span class="line">        <span class="comment">//以加法为例</span></span><br><span class="line">        tree[pos] += x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="初始化树状数组"><a href="#初始化树状数组" class="headerlink" title="初始化树状数组"></a>初始化树状数组</h4><p>wekipedia上的一个从数组[1,2,3,4,5]构建树状数组的过程：</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/dc/BITDemo.gif/220px-BITDemo.gif" alt="BITDemo.gif"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> tree[maxIndex]</span><br><span class="line"><span class="comment">//待求和数组</span></span><br><span class="line"><span class="keyword">int</span> aim[maxIndex]</span><br><span class="line"><span class="comment">//1.调用update方法（复杂度O(nlogn)）</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i &lt; maxIndex;i++)&#123;</span><br><span class="line">    update(i, aim[i]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.前缀和法(tree[i] = preSum[i] - preSum[i - lowbit(i)])</span></span><br><span class="line"><span class="comment">// 	复杂度O(n)</span></span><br><span class="line"><span class="keyword">int</span> preSum[maxIndex];</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i &lt; maxIndex;i++)&#123;</span><br><span class="line">    preSum[i] = preSum[i - <span class="number">1</span>] + aim[i];</span><br><span class="line">    tree[i] = preSum[i] - preSum[i - lowbit(i)];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="模板题目"><a href="#模板题目" class="headerlink" title="模板题目"></a>模板题目</h3><ol>
<li><a href="https://www.luogu.com.cn/problem/P3374">luogu P3374 :树状数组1</a><ul>
<li>使用scanner做输入会出现三个测试用例TLE<strong>（坑爹）</strong></li>
</ul>
</li>
<li><a href="https://www.luogu.com.cn/problem/P3368">luogu P3368 :树状数组2</a><ul>
<li>通过<strong>差分</strong>的思路，将树状数组支持的操作变为<ul>
<li><strong>单点修改-&gt;区间修改</strong></li>
<li><strong>区间查询-&gt;单点查询</strong></li>
</ul>
</li>
</ul>
</li>
<li><a href="https://www.luogu.com.cn/problem/P2880">luogu P2880 ：牛飞盘比赛</a><ul>
<li>树状数组维护值从 <strong>区间和</strong> 变为 <strong>区间内最值</strong></li>
<li>update操作变为最大值更新，查询操作递归完成</li>
</ul>
</li>
</ol>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ol>
<li><a href="https://zh.wikipedia.org/wiki/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84">wikipidia：树状数组</a></li>
<li><a href="http://duanple.blog.163.com/blog/static/7097176720081131113145832/">博客：Binary indexed tree-树状数组</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/93795692">博客：算法学习笔记(2) : 树状数组</a></li>
<li><a href="https://www.luogu.com.cn/training/3079">树状数组模板题</a></li>
</ol>
]]></content>
      <categories>
        <category>算法整理</category>
      </categories>
      <tags>
        <tag>树状数组</tag>
      </tags>
  </entry>
  <entry>
    <title>Dockerfile理解使用</title>
    <url>/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Dockerfile%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>docker file 包含一系列命令行，docker通过该文件组织生成镜像，一个docker file文件主要包括四部分：</p>
<ol>
<li>基础镜像信息</li>
<li>维护者信息</li>
<li>镜像操作指令</li>
<li>容器启动时执行指令</li>
</ol>
<p>以一个dockerfile为例：<br><img src="/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Dockerfile%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/image-20210618162710697.png" class="" title="image-20210618162710697.png"></p>
<blockquote>
<p>Dockerfile 的指令每执行一次都会在 docker 上新建一层。所以过多无意义的层，会造成镜像膨胀过大</p>
</blockquote>
<span id="more"></span>
<h3 id="基本的命令内容"><a href="#基本的命令内容" class="headerlink" title="基本的命令内容"></a>基本的命令内容</h3><ol>
<li><p>FORM：指定基础镜像，必须为第一个命令</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line">   　　<span class="comment"># 格式：</span></span><br><span class="line"><span class="keyword">FROM</span> &lt;image&gt;</span><br><span class="line"><span class="keyword">FROM</span> &lt;image&gt;:&lt;tag&gt;</span><br><span class="line"><span class="keyword">FROM</span> &lt;image&gt;@&lt;digest&gt;</span><br><span class="line">   　　<span class="comment"># 例如：</span></span><br><span class="line">   　　<span class="keyword">FROM</span> node:alpine</span><br></pre></td></tr></table></figure>
<p>同一个镜像可能有不同大小版本，不同的tag表示基于不同的base image（以python3.9.6 镜像为例）</p>
<ol>
<li><p><strong>完整版</strong> </p>
<img src="/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Dockerfile%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/image-20210727160609885.png" class="" title="image-20210727160609885">
</li>
<li><p><strong>alpine</strong>  基于<strong>alpine linux</strong>的docker镜像，特点就是容量小，适合作为基础镜像</p>
<ul>
<li>缺少<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/GNU_C_Library">GNU C Library</a> (glibc)</li>
<li>缺少的软件过多，<a href="https://pythonspeed.com/articles/alpine-docker-python/">构建时间长</a></li>
</ul>
<blockquote>
<p>Alpine 是众多 Linux 发行版中的一员，和 CentOS、Ubuntu、Archlinux之类一样，只是一个发行版的名字，号称小巧安全，有自己的包管理工具 apk</p>
</blockquote>
<img src="/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Dockerfile%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/image-20210727160550212.png" class="" title="image-20210727160550212">
</li>
<li><p><strong>Debian Buster</strong> 基于<strong>Debian linux</strong>的docker镜像</p>
<img src="/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Dockerfile%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/image-20210727160529738.png" class="" title="image-20210727160529738">
</li>
<li><p><strong>slim</strong> 即瘦身版本，删去了部分通用包支持</p>
<ul>
<li>python:slim-buster是大多数Python用例的良好基础镜像</li>
</ul>
<img src="/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Dockerfile%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/image-20210727160805454.png" class="" title="image-20210727160805454">
<img src="/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Dockerfile%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/image-20210727160814190.png" class="" title="image-20210727160814190">
</li>
</ol>
</li>
<li><p>RUN: 构建镜像是执行的命令(支持两种执行方式)</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="comment"># shell执行</span></span><br><span class="line"><span class="comment"># 格式：</span></span><br><span class="line">    <span class="keyword">RUN</span><span class="bash"> &lt;<span class="built_in">command</span>&gt;</span></span><br><span class="line"><span class="comment"># exec执行</span></span><br><span class="line"><span class="comment"># 格式：</span></span><br><span class="line">    <span class="keyword">RUN</span><span class="bash"> [<span class="string">&quot;command&quot;</span>, <span class="string">&quot;param1&quot;</span>, <span class="string">&quot;param2&quot;</span>]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>ADD/COPY:将本地文件添加到docker镜像中</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ADD</span><span class="bash"> &lt;src&gt; &lt;dest&gt;</span></span><br><span class="line"><span class="comment"># 例如：</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> . /data</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>WORKDIR:切换工作目录,即容器启动后的pwd，当前目录</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 格式：</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> path</span></span><br><span class="line"><span class="comment"># 例如：</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> . /data</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>CMD： 在容器启动后执行相关命令</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 格式：</span></span><br><span class="line">    <span class="keyword">CMD</span><span class="bash"> [<span class="string">&quot;executable&quot;</span>,<span class="string">&quot;param1&quot;</span>,<span class="string">&quot;param2&quot;</span>] </span></span><br><span class="line">    <span class="keyword">CMD</span><span class="bash"> <span class="built_in">command</span> param1 param2 </span></span><br></pre></td></tr></table></figure>
<ul>
<li>如果 Dockerfile 中如果存在多个 CMD 指令，仅最后一个生效。</li>
</ul>
<p>参数名和参数值相连，分别为列表中的两个不同项，以gunicorn为例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gunicorn MainWebApp:app -c gunicorn.conf.py -t 100</span><br></pre></td></tr></table></figure>
<p>等价于</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">&quot;gunicorn&quot;</span>, <span class="string">&quot;MainWebApp:app&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;gunicorn.conf.py&quot;</span>, <span class="string">&quot;&quot;</span>-t<span class="string">&quot;, &quot;</span>100<span class="string">&quot;]</span></span></span><br></pre></td></tr></table></figure>
<p>列表中的参数和命令一定要使用双引号</p>
</li>
</ol>
]]></content>
      <categories>
        <category>框架/技术学习</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker基础概念</title>
    <url>/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<p>主要包括三个基本核心概念：</p>
<ul>
<li><strong>镜像（image)</strong>：相当于一个静态的文件系统，类似于未安装的windows ios文件，相当于是一个未挂载的root文件系统</li>
<li><strong>容器（container)</strong>：容器是镜像运行时的实体，可以启动、创建、停止以及删除。容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间</li>
<li><strong>仓库（Repository）</strong>：镜像中心，类似于maven的依赖包中心，用来保存镜像，最常使用的公共仓库是官方的Docker Hub</li>
</ul>
<p>基本架构（cs架构）:</p>
<blockquote>
<p>守护进程（daemon）是生存期长的一种进程，没有控制终端。它们常常在系统引导装入时启动，仅在系统关闭时才终止</p>
</blockquote>
<ul>
<li><strong>Docker 客户端(Client)</strong>:与docker host中的守护进程进行通信，通过命令执行实际的操作（cs架构中的c）</li>
<li><strong>Docker 主机(Host)</strong>:运行容器，存储镜像的机器</li>
<li><strong>Docker Registry</strong>：一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。</li>
</ul>
<img src="/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210727162745322.png" class="" title="image-20210727162745322">
<span id="more"></span>
<p>从hello-word可以得到docker执行的基本流程</p>
<ol>
<li>client 连接 hosts 的daemon进程</li>
<li>daemon进程查看本地镜像，如果不存在，向远程Registry获取镜像</li>
<li>daemon进程为镜像启动容器</li>
<li>daemon进程将信息发送给client</li>
</ol>
<img src="/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210727162755137.png" class="" title="image-20210727162755137">
<h3 id="几个基本概念"><a href="#几个基本概念" class="headerlink" title="几个基本概念"></a>几个基本概念</h3><h4 id="1-容器与虚拟机的区别"><a href="#1-容器与虚拟机的区别" class="headerlink" title="1. 容器与虚拟机的区别"></a>1. 容器与虚拟机的区别</h4><p><strong>虚拟机</strong>，相当于一套真的操作系统，在os层上增加了一层hypervisor，用来虚拟化硬件，每个虚拟机以层虚拟化的硬件为基础，建立自己的OS层（GuestOa），应用层.</p>
<p><strong>容器</strong>，更偏向于一个进程隔离空间，空间内包含特定进程执行所需要的环境（镜像），容器之间互不干扰，但是共同调用底层的os接口，由daemon进程统一管理</p>
<img src="/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210727162804622.png" class="" title="image-20210727162804622">
<h4 id="2-Dockerfile相关总结"><a href="#2-Dockerfile相关总结" class="headerlink" title="2. Dockerfile相关总结"></a>2. Dockerfile相关总结</h4><h2 id="Docker基本使用（以某个项目部署为例）"><a href="#Docker基本使用（以某个项目部署为例）" class="headerlink" title="Docker基本使用（以某个项目部署为例）"></a>Docker基本使用（以某个项目部署为例）</h2><ol>
<li><p>在项目中添加Dockerfile(该文件用来提供构建镜像文件的必要信息)</p>
<img src="/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210727162814042.png" class="" title="image-20210727162814042">
</li>
<li><p>docker builder 构建镜像文件</p>
<ul>
<li>首先拉取基础docker镜像，之后将应用程序复制到docker容器中，根据应用程序依赖信息加载相关依赖，加载完成后执行初始run命令</li>
<li>-t 指明生成镜像文件的tag</li>
<li>. 指明dockerfile位置</li>
</ul>
</li>
<li><p>docker run 指定并绑定端口后运行项目</p>
<img src="/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210727162829498.png" class="" title="image-20210727162829498">
</li>
</ol>
<h3 id="docker基础镜像构建（搭建基础机器学习-深度学习环境）"><a href="#docker基础镜像构建（搭建基础机器学习-深度学习环境）" class="headerlink" title="docker基础镜像构建（搭建基础机器学习+深度学习环境）"></a>docker基础镜像构建（搭建基础机器学习+深度学习环境）</h3><blockquote>
<p>To stop a container, use <code>CTRL-c</code>. This key sequence sends <code>SIGKILL</code> to the container. If <code>--sig-proxy</code> is true (the default),<code>CTRL-c</code> sends a <code>SIGINT</code> to the container. If the container was run with <code>-i</code> and <code>-t</code>, you can detach from a container and leave it running using the <code>CTRL-p CTRL-q</code> key sequence.</p>
</blockquote>
<ol>
<li><p>使用ubuntu作为基础镜像,安装anaconda</p>
<ul>
<li><p>docker cp 复制文件到镜像中</p>
<img src="/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210726103848754.png" class="" title="image-20210726103848754">
</li>
<li><p>docker attach 切换到前台</p>
<img src="/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210726104207802.png" class="" title="image-20210726104207802">
</li>
<li><p><code>sh Anaconda3-2021.05-Linux-x86_64.sh</code> 执行安装脚本</p>
</li>
</ul>
</li>
<li><p>退出容器，将当前容器导出为镜像 docker commit</p>
<img src="/2021/07/27/%E6%9D%82%E9%A1%B9%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210726105455898.png" class="" title="image-20210726105455898">
</li>
<li><p>推送远程仓库（镜像名称中的用户名一定要与远程仓库用户名一致，否则报错）</p>
</li>
</ol>
]]></content>
      <categories>
        <category>框架/技术学习</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>读论文1-resNet</title>
    <url>/2022/03/13/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%871-resNet/</url>
    <content><![CDATA[<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><blockquote>
<p>论文地址 ：<a href="http://cn.arxiv.org/pdf/1512.03385.pdf">Deep Residual Learning for Image Recognition</a></p>
</blockquote>
<p>针对深度神经网络难以训练的问题，ResNet提出了一种特殊的网络结构-残差，有效的解决了深度网络的退化问题，降低了深度网络学习难度</p>
<h3 id="论文结构"><a href="#论文结构" class="headerlink" title="论文结构"></a>论文结构</h3><ol>
<li><p>摘要 abstract</p>
<p>按照<strong>问题-&gt;解决方案-&gt;实验效果</strong>的逻辑，首先提出问题：“深度神经网络难以训练”，引出解决方案-residual net，最后列举在不同数据上的卓越效果（ImageNet，COCO，CIFAR），证明解决方案的有效性</p>
</li>
<li><p>介绍 intro</p>
<p>与摘要的逻辑相同，逻辑非常严密（ps:太丝滑了）</p>
<ol>
<li>为什么要用深度网络？ 因为深度网络有助于捕捉特征，提升任务效果</li>
<li>增加网络深度又会出现<strong>两个主要问题</strong>：一是梯度爆炸/消失，二是深度网络的退化问题</li>
<li>梯度爆炸可以通过 <strong>normalized initialization and intermediate normalization layers</strong> 解决</li>
<li>网络退化如何解决？引出了本文的残差机制-residual</li>
<li>最后又展示了一轮不同数据上的实验效果</li>
</ol>
</li>
</ol>
<span id="more"></span>
<ol>
<li><p>相关工作 related work</p>
<p>没细看。。</p>
</li>
<li><p>算法描述 Deep Residual Learning</p>
<p><strong>残差机制-&gt;网络架构设计-&gt;网络实现</strong></p>
<ol>
<li><p>提出了深度网络之所以会出现退化,是因为 <strong>难以学习直接映射</strong>（只是形式上的理解，没有给出公式证明），很自然引出了残差机制的设计</p>
<blockquote>
<p>might have difficulties in approximating identity mappings by multiple nonlinear layers.</p>
</blockquote>
</li>
<li><p>为了方便对比残差机制是否真的有效，设计了<strong>Plain Networks</strong>和<strong>Residual Networks</strong>两种架构</p>
</li>
</ol>
</li>
<li><p>实验 Experiments</p>
<p>列举了从 ImageNet 到CIFAR10再到PASCAL and MS COCO三个数据集上不同任务的实验效果</p>
<ol>
<li><p>首先对比在ImageNet数据集上34层Plain network和18层效果，证明深层网络确实出现了退化现象，并且排除了是梯度消失造成的可能</p>
<blockquote>
<p> We conjecture that the deep plain nets may have <strong>exponentially low convergence rates</strong>, which impact the reducing of the training error</p>
</blockquote>
</li>
<li><p>然后对比34层和18层Residual Networks效果,证明残差确实能够解决深层网络退化问题,并且能够在训练初期加速网络收敛速度</p>
<blockquote>
<p>This indicates that the <strong>degradation problem is well addressed</strong> in this setting and we manage to obtain accuracy gains from increased depth</p>
</blockquote>
</li>
<li><p>继续对比了 <strong>直接映射 和 投影映射</strong> 两种不同残差计算方式，是否影响残差发挥作用，得到结论：单纯的直接映射残差机制即可解决退化问题，投影映射效果优于直接映射，但是计算量增加较大</p>
</li>
<li><p>之后提出了一种<strong>Deeper Bottleneck Architectures</strong>，在ImageNet上探索残差机制在更深层网络上的效果</p>
<img src="/2022/03/13/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%871-resNet/image-20220309093015669.png" class="" title="image-20220309093015669">
</li>
<li><p>在小数据集CIFAR-10上验证：<strong>当学习任务足够简单，并不需要过深网络时，深度网络中的残差映射会近似于直接映射</strong>（加了残差块的深层网络 <strong>等价于</strong> 浅层网络+多个直接映射层）</p>
<blockquote>
<p>the residual functions might be generally closer to zero than the non-residual functions.</p>
</blockquote>
</li>
<li><p>最后秀了一波在目标检测任务上的优秀成果</p>
</li>
</ol>
</li>
<li><p>总结 summarize</p>
<p>因为CVPR的页数限制没有总结（？）</p>
</li>
</ol>
<h3 id="残差机制-residual"><a href="#残差机制-residual" class="headerlink" title="残差机制-residual"></a>残差机制-residual</h3><img src="/2022/03/13/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%871-resNet/image-20220309093814713.png" class="" title="image-20220309093814713">
<p>文章中分别提出了两种残差映射公式，其中第一种为 “Identity Mapping”，直接映射公式如下</p>
<script type="math/tex; mode=display">
y=F(x,\{W_i\}) +x  \tag{1}</script><p>其中 <script type="math/tex">F(x,\{W_i\})</script> 代表残差块内从X到残差的映射（如图2为中间带ReLU激活函数的两个权重层）,直接映射要求输入输出的唯独相同，可以直接叠加</p>
<p>第二种为 “linear projection”，将输入投影到与输出相同维度，方便叠加</p>
<script type="math/tex; mode=display">
y=F(x,\{W_i\}) +W_s x  \tag{2}</script><ul>
<li>论文中已通过实验证明， “Identity Mapping”即可解决深度网络的退化问题</li>
<li>虽然 “linear projection”效果略优于 “Identity Mapping”，但投影操作增加了计算量</li>
</ul>
<h3 id="网络结构设计"><a href="#网络结构设计" class="headerlink" title="网络结构设计"></a>网络结构设计</h3><p>两个基本设计原则</p>
<ol>
<li>当特征图大小缩小一半（<script type="math/tex">224*224->112*112</script>），通道数翻一倍（<script type="math/tex">64->128</script>）</li>
<li>当特征图大小不变时，通道数保持不变</li>
</ol>
<p>网络组成结构如图</p>
<ul>
<li>每个中括号内为一个残差块，每一个例如$conv2_x，conv3_x$ 的卷积层包含多个残差块</li>
<li>跨越不同卷积层时特征图缩小一般，通道数扩大一倍</li>
<li>例如18层网络的结构为<ol>
<li>首先通过 <script type="math/tex">7*7</script> 输出通道为64的卷积层+ <script type="math/tex">3*3</script>的最大池化层</li>
<li>进入第<script type="math/tex">conv2\_x</script>卷积层，包括两个残差块，每个残差块内包含两个<script type="math/tex">3*3*64</script> 的卷积层</li>
<li>进入第<script type="math/tex">conv2\_x</script>卷积层，同样包括两个残差块，由于第一个残差块输入为<script type="math/tex">56*56</script> 输出为<script type="math/tex">28*28</script> 需要使用投影残差计算，其余无差异，其中投影残差作者对比了两种不同的选择<ul>
<li>通过padding升维度，避免投影计算（A)</li>
<li>1*1卷积核，类似于全连接层(B）</li>
</ul>
</li>
</ol>
</li>
</ul>
<img src="/2022/03/13/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%871-resNet/image-20220309150217483.png" class="" title="image-20220309150217483">
<p><strong>Deeper Bottleneck Architectures</strong></p>
<p>针对ImageNet设计深层网络时，为了降低计算复杂度，设计了一种Bottleneck block</p>
<ul>
<li>与普通残差块不同在于 包含三个卷积层: <script type="math/tex">1*1 + 3*3 + 1*1</script>,其中 <script type="math/tex">1*1</script> 负责升维和降维</li>
<li>上图中34层和50层，虽然增加了16层，但是使用Bottleneck block的50层网络计算量增加不大</li>
</ul>
<img src="/2022/03/13/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%871-resNet/image-20220309152918557.png" class="" title="image-20220309152918557">
<p><strong>训练超参数</strong></p>
<ol>
<li>使用SGD，256 mini-batch，迭代训练<script type="math/tex">6*10^4</script> 次</li>
<li>初始学习率为0.1，每当错误率达到稳定，学习率缩小10倍</li>
<li>weight decay:0.0001,momentum:0.9,不使用dropout</li>
<li>每个卷积层之后，激活层之前，添加BN层</li>
</ol>
<h3 id="Pytorch-代码实现"><a href="#Pytorch-代码实现" class="headerlink" title="Pytorch 代码实现"></a>Pytorch 代码实现</h3><p>基础残差块：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    expansion: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        inplanes: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        planes: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        stride: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        downsample: <span class="type">Optional</span>[nn.Module] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        base_width: <span class="built_in">int</span> = <span class="number">64</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        dilation: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        norm_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        <span class="keyword">if</span> groups != <span class="number">1</span> <span class="keyword">or</span> base_width != <span class="number">64</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;BasicBlock only supports groups=1 and base_width=64&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> dilation &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;Dilation &gt; 1 not supported in BasicBlock&quot;</span>)</span><br><span class="line">        <span class="comment"># Both self.conv1 and self.downsample layers downsample the input when stride != 1</span></span><br><span class="line">        <span class="comment"># 第一个卷积层传入 输入通道，输出通道，步长</span></span><br><span class="line">        self.conv1 = conv3x3(inplanes, planes, stride)</span><br><span class="line">        <span class="comment"># 卷积层后+batchnorm层</span></span><br><span class="line">        self.bn1 = norm_layer(planes)</span><br><span class="line">        <span class="comment"># relu激活函数</span></span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.conv2 = conv3x3(planes, planes)</span><br><span class="line">        self.bn2 = norm_layer(planes)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        identity = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">		<span class="comment"># 若输入与输出不相同，使用投影映射，也就是残差公式2</span></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line">		<span class="comment"># 残差计算+ReLU激活函数</span></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>投影残差函数初始化(就是简单的$1*1$卷积核加batchnorm）:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.inplanes != planes * block.expansion:</span><br><span class="line">    downsample = nn.Sequential(</span><br><span class="line">        conv1x1(self.inplanes, planes * block.expansion, stride),</span><br><span class="line">        norm_layer(planes * block.expansion),</span><br><span class="line">    )</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Bottleneck块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">       identity = x</span><br><span class="line">       <span class="comment"># 1*1</span></span><br><span class="line">       out = self.conv1(x)</span><br><span class="line">       out = self.bn1(out)</span><br><span class="line">       out = self.relu(out)</span><br><span class="line">	<span class="comment"># 3*3</span></span><br><span class="line">       out = self.conv2(out)</span><br><span class="line">       out = self.bn2(out)</span><br><span class="line">       out = self.relu(out)</span><br><span class="line">	<span class="comment"># 1*1</span></span><br><span class="line">       out = self.conv3(out)</span><br><span class="line">       out = self.bn3(out)</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">           identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">       out += identity</span><br><span class="line">       out = self.relu(out)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>resnet网络结构</p>
<img src="/2022/03/13/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%871-resNet/image-20220309154506166.png" class="" title="image-20220309154506166">]]></content>
      <categories>
        <category>读论文</category>
      </categories>
      <tags>
        <tag>resNet</tag>
      </tags>
  </entry>
  <entry>
    <title>读论文2-transformer</title>
    <url>/2022/03/20/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%872-transformer/</url>
    <content><![CDATA[<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><blockquote>
<p>论文地址：<a href="https://arxiv.org/pdf/1706.03762v5.pdf">Attention Is All You Need</a></p>
</blockquote>
<p>本文提出一个区别于传统RNN、CNN的基于attention机制的网络架构-transformer，在具备RNN捕捉序列特征能力的基础上，实现了并行计算，降低了计算成本（看论文名字就知道作者对论文内容非常自信）</p>
<h3 id="论文结构"><a href="#论文结构" class="headerlink" title="论文结构"></a>论文结构</h3><p>总结下来发现这篇论文没讲故事，就是单纯的讲自己的工作，非常的简洁</p>
<ol>
<li><p>摘要 abstract</p>
<p>从问题提出-&gt;解决方案-&gt;效果，三句话介绍NMT-&gt;encoder-decoder-&gt;transformer模型，剩余的句子全在列举是实验效果，<strong>非常简洁</strong></p>
</li>
<li><p>介绍 intro</p>
<p>还是围绕着 rnn、encoder-decoder、attention这三个主要内容</p>
<ul>
<li>首先介绍rnn在LM和NMT中广泛应用，取得了SOTA成果，但是其还是存在无法并行计算的问题（介绍了为什么）</li>
<li>然后介绍了attention机制能够跨距离建模依赖，但是目前研究大部分还是与rnn结合在一起</li>
<li>最后引出了本文完全基于attention机制的transformer模型</li>
</ul>
</li>
<li><p>背景 background</p>
<ul>
<li>列举了其他为了降低计算量的研究（convS2S, ByteNet）,但是降低效果不如transformer（证明自己工作的价值）</li>
<li>通过列举参考文献，证明 self-attention，decoder-encoder两种设计的有效性（证明自己解决方案的科学性）</li>
</ul>
</li>
</ol>
<span id="more"></span>
<ol>
<li><p>模型架构</p>
<ul>
<li>先是一段话+一张模型总图，整体介绍模型，然后每个层进行拆解，讲解内部机制</li>
<li><strong>总分结构</strong></li>
</ul>
</li>
<li><p>why attention</p>
<p>对比attention，rnn，cnn三种机制的计算的时间复杂度，证明attention机制真的有助于降低计算量</p>
</li>
<li><p>训练 traning</p>
<p>介绍了训练参数等</p>
</li>
<li><p>实验 Results</p>
<p>列举了在NMT以及English Constituency Parsing两个任务上的实验效果</p>
<ul>
<li>WMT 2014 English-to-German，WMT 2014 English-to-French实现了SOTA</li>
<li>为了证明transformer的在其它任务上的可泛化性，做了该实验，除了RNNG这个模型，transformer效果优于之前的所有模型</li>
</ul>
</li>
<li><p>结论 conclusion</p>
<p>总说自己提出了一个完全基于attention的模型，并再提了实验结果，最后说自己要把transformer推广到其他任务上（efficiently handle large inputs and outputssuch as images, audio and video）</p>
</li>
</ol>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>整体架构图如下，标准的encoder-decoder架构，encoder和decoder均为多个相同层的堆叠，其中encoder6层、decoder6层，共12层。</p>
<img src="/2022/03/20/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%872-transformer/image-20220314104109242.png" class="" title="image-20220314104109242">
<p>其他机制不再赘述，之前的<a href="https://fuhaifei.github.io/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/">博客</a>总结过，简单总结一下几个新理解</p>
<h4 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h4><img src="/2022/03/20/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%872-transformer/image-20220314104956244.png" class="" title="image-20220314104956244">
<p><strong>为什么要Scale？</strong></p>
<blockquote>
<p>We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients[4]. To counteract this effect, we scale the dot products by $\frac{1}{\sqrt d_k}$</p>
</blockquote>
<p>在计算注意力分数时，QK向量相乘后，要除以输入向量维度的平方根，此论文中给出的解释，当输入维度 $d_k$ 较大时，QK向量会变得相对较大，导致softmax函数落在图像两边（如下图），导致梯度过小（梯度消失），通过除以$\sqrt d_k$ 避免此问题的出现</p>
<img src="/2022/03/20/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%872-transformer/image-20220314105426864.png" class="" title="image-20220314105426864">
<p><strong>encoder-decoder attention 的输入是什么</strong></p>
<blockquote>
<p>the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.</p>
</blockquote>
<p>query来自于decoder上一层的多头注意力机制输出，value和key来自于encoder的输出</p>
<p><strong>多头注意力机制为什么有用？</strong></p>
<p>类似于卷积神经网络中的通道的作用，不同的通道代表不同特征空间</p>
<h4 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h4><p><strong>为什么embedding层在+pos encoding前要乘以 $\sqrt d_k$?</strong></p>
<p>随着输入维度的增加，嵌入向量的每个位置的数字会变小（总和接近于一），而pos encoding的大小不变，为了保持嵌入向量和pos encoding的相对大小关系保持不变，乘以 $\sqrt d_k$。</p>
<h3 id="why-self-attention"><a href="#why-self-attention" class="headerlink" title="why self-attention?"></a>why self-attention?</h3><p>原文中通过对比自注意力机制、卷积层、循环神经网络的计算复杂度、顺序计算量(是否可并行计算)、最大序列特征捕捉计算次数三个指标，证明自注意力机制在计算量上的优势</p>
<img src="/2022/03/20/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%872-transformer/image-20220314145009609.png" class="" title="image-20220314145009609">
<p>Complexity per Layer</p>
<ol>
<li>self-Attenion的主要计算量在 Q和V相乘，两个均为 $n<em>d$维度矩阵，相乘复杂度为 $O(n^2</em>d)$</li>
<li>rnn每个时间步是一个输入为d,输出为d的全连接网络，整个seq的计算量为$O(n*d^2)$</li>
<li>卷积层与rnn类似，假设一维卷积核大小为k，每个元素近似在卷积核中卷积k次，整个seq的计算量为$O(k<em>n</em>d^2)$</li>
<li>可见当n &lt; d时，self-Attention的复杂度小于RNN</li>
</ol>
<p>其他理解比较简单</p>
<h3 id="实验参数"><a href="#实验参数" class="headerlink" title="实验参数"></a>实验参数</h3><p><strong>词嵌入</strong></p>
<ol>
<li>WMT 2014 English-German 使用 <strong>byte pair encoding</strong> 做词典，共37000 个token</li>
<li>WMT 2014 English-French 使用 32000 <strong>word-piece vocabulary</strong></li>
</ol>
<p><strong>optimizer</strong></p>
<p>使用Adam作为模型优化器</p>
<ol>
<li><p>三个参数 $\beta_1=0.9,\beta_2=0.98,\epsilon=10^{-9}$</p>
</li>
<li><p>学习率如下，当训练步数小于warmup步数时，取 $step_num*warmup_steps^{-1.5}$,其中warmup取4000步</p>
<img src="/2022/03/20/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%872-transformer/image-20220314150054771.png" class="" title="image-20220314150054771">
</li>
</ol>
<p><strong>dropout</strong></p>
<ol>
<li>每个子层在残差相加之前，增加dropout</li>
<li>pos encoding+embedding后增加dropout</li>
<li>base模型的dropout概率为 $P_{drop} = 0.1$</li>
</ol>
<p><strong>label smoothing</strong></p>
<p>训练时增加了标签平滑，参数值为 $\epsilon_{ls} = 0.1$ </p>
<img src="/2022/03/20/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%872-transformer/image-20220314150919745.png" class="" title="image-20220314150919745">]]></content>
      <categories>
        <category>读论文</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>读论文3-BERT</title>
    <url>/2022/03/20/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%873-BERT/</url>
    <content><![CDATA[<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><blockquote>
<p>论文地址:<a href="https://arxiv.org/abs/1810.04805v2">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
</blockquote>
<p>提出了一个基于transformer的预训练模型，引领了在大规模数据上预训练深度模型，在下游任务上微调的风潮，其主要贡献在两点</p>
<ol>
<li>引领了预训练模型的风潮</li>
<li>特殊的训练机制，实现了”真”双向语言模型</li>
</ol>
<h3 id="论文结构"><a href="#论文结构" class="headerlink" title="论文结构"></a>论文结构</h3><p>明显该论文的工作是基于ELMo和GPT改进，内容组织也是围绕着 前人工作+自己改进+实验效果展开的</p>
<ol>
<li><p>摘要 Abstract</p>
<p>提了一下 GPT和ELMo,强调BERT是区别于两者的基于transformer的“真”双向模型，并列举实验效果</p>
</li>
<li><p>介绍 Intro</p>
<p>主要是围绕着直接解决的问题以及相对于前人解决方案的优越性进行阐述</p>
<ol>
<li>首先强调 sentence-level 和 token-level 两种不同类型任务，需要聚焦于不同level的模型</li>
<li>又介绍了目前两种主要的预训练应用手段，一是特征提取，用预训练模型做特征提取，输入到下游任务模型；二是微调，预训练模型在下游任务数据上继续训练，微调参数</li>
<li>然后列举GPT和ELMo主流预训练模型存在的问题： 局限于语言模型的特性，无法实现真”双向”</li>
<li>引出了自己通过MLM+“next sentence prediction”,实现真双向，同时解决sentence level和token level两个问题</li>
</ol>
<p><strong>自己提出问题，自己解决问题，自圆其说</strong></p>
</li>
</ol>
<span id="more"></span>
<ol>
<li><p>相关工作 Related Word</p>
<p>从三个方面介绍了预训练相关工作情况，主要还是为了增加自己工作的可信度</p>
</li>
<li><p>bert</p>
<ul>
<li>模型架构</li>
<li>输入</li>
<li>预训练的两个target</li>
</ul>
<p>没细讲模型结构，主要强调思路，把有些难懂的细节放在了附录</p>
</li>
<li><p>实验 Experiments</p>
<p>列举了包括GLUE、SQuAD、SWAG四个任务上的微调的效果，验证了bert模型在token-level、sentence-level不同类型任务上的有效性</p>
</li>
<li><p>对比试验 Ablation Studies</p>
<ol>
<li><p>通过控制变量实验验证MLM和NSP对于提升模型特征抽取能力是有效的</p>
<img src="/2022/03/20/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%873-BERT/image-20220319143433975.png" class="" title="image-20220319143433975">
</li>
<li><p>不同模型深度对于模型效果的影响</p>
<img src="/2022/03/20/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%873-BERT/image-20220319143540227.png" class="" title="image-20220319143540227">
</li>
<li><p>bert用于特征抽取时，下游任务的效果（CoNLL-2003 命名实体识别任务的实验）</p>
</li>
</ol>
<img src="/2022/03/20/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%873-BERT/image-20220319143621944.png" class="" title="image-20220319143621944">
</li>
<li><p>结论</p>
<blockquote>
<p>Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks</p>
</blockquote>
</li>
</ol>
<p>​        简单强调了一下自己的主要贡献，即探索了深度双向架构预训练模型在解决一系列NLP任务中的作用。</p>
<h3 id="MLM-amp-NSP"><a href="#MLM-amp-NSP" class="headerlink" title="MLM &amp; NSP"></a>MLM &amp; NSP</h3><p>bert通过设计MLM和NSP这两个预训练目标，使得bert既能解决token-level，又能解决sentence-level的一系列NLP任务。</p>
<h4 id="Masked-LM"><a href="#Masked-LM" class="headerlink" title="Masked LM"></a>Masked LM</h4><p>由于语言模型本身约束，为了避免当前词看到未来词，只能训练单向语言模型，或者将两个方向的单向模型拼接在一起近似双向，bert在与训练过程中，将一部分词替换掉，训练模型预测被替换的词</p>
<ol>
<li><p>替换掉序列中 15%的 WordPiece token(还得保证替换之后，预训练预料和finetuning语料分布差距不会过大)</p>
<ul>
<li>其中 80% 替换为 [mask]</li>
</ul>
</li>
</ol>
<ul>
<li><p>10% 替换为词表中的任意一个词</p>
<ul>
<li>10% 保持原词不变</li>
</ul>
<p>在附录中的对比实验中，通过语言模型和NER两个任务效果对比，确定了8:1:1的比率设置</p>
</li>
</ul>
<ol>
<li><p>使用替换位置的最后一层隐藏状态，预测原词</p>
<blockquote>
<p>will be used to predict the original token with cross entropy loss.</p>
</blockquote>
</li>
</ol>
<p>通过Mask LM任务，bert具备了双向特征提取能力（bidirectional）</p>
<h4 id="NSP"><a href="#NSP" class="headerlink" title="NSP"></a>NSP</h4><p>为了使得BERT能够具备建模sentence level的任务（例如 QA,自然语言推断），预训练过程中增加 Next Sentence Prediction任务</p>
<img src="/2022/03/20/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%873-BERT/image-20220320190738243.png" class="" title="image-20220320190738243">
<ol>
<li>输入两个句子：A和B，其中 50% 训练数据 B为A的next（labeled as IsNext），50%训练数据 A和B随机抽取组合(labeled as NotNext），两句子拼接，中间使用特殊字符 [SEP]分隔</li>
<li>两个句子的<strong>token长度和小于512</strong></li>
<li>使用 <strong>[CLS]</strong> token最后一层输出做概率预测</li>
</ol>
<blockquote>
<p>The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.</p>
</blockquote>
<p>在预训练过程中，两个任务并行训练，训练损失为 <strong>平均mask概率 + 平均NSP概率</strong> 损失</p>
<h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>BERT预训练输入词向量包括三个不同Embeddings求和，三种Embeddings层均在训练中得到</p>
<ol>
<li>Token Embeddings 词嵌入，首先经过WordPieces模型分词后，在预训练模型中训练词嵌入</li>
<li>Segment Embeddings 段嵌入，与NSP任务相关，区分两个句子中的token，在预训练模型中训练得到</li>
<li>Position Embeddings 位置嵌入，也采用了 训练得到，<strong>未使用transformer中的正弦周期函数</strong></li>
</ol>
<img src="/2022/03/20/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%873-BERT/image-20220320192229816.png" class="" title="image-20220320192229816">
<h3 id="模型训练参数"><a href="#模型训练参数" class="headerlink" title="模型训练参数"></a>模型训练参数</h3><ol>
<li><strong>batch size=256</strong>, 每个batch的<strong>seq_length最长为512</strong>，训练了 <strong>40 epochs</strong>,大约1000000步</li>
<li>Adam优化器，lr= $1e^{-4}$ ,$\beta_1=0.9$, $\beta_2 = 0.999$ , weight decay $0.01$ ,dropout = 0.1</li>
<li>学习率随着训练epoch线性下降</li>
<li>使用 gelu实现替代了transformer中的relu函数</li>
</ol>
<p><strong>特殊的训练trick</strong></p>
<p>由于训练数据token长度分布大部分长度为128，为了加快收敛速度，在与训练过程中分为两阶段</p>
<ol>
<li>首先使用 <strong>128作为每个batch的seq_length</strong> 训练90%的step</li>
<li>再使用 <strong>seq_length=256</strong> 训练10%的step，用来学习position embedding（没太理解这么做的原因）</li>
</ol>
]]></content>
      <categories>
        <category>读论文</category>
      </categories>
      <tags>
        <tag>bert</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML-1.学什么</title>
    <url>/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-1.%E5%AD%A6%E4%BB%80%E4%B9%88/</url>
    <content><![CDATA[<h3 id="学什么"><a href="#学什么" class="headerlink" title="学什么"></a>学什么</h3><iframe src="//player.bilibili.com/player.html?aid=675507034&bvid=BV13U4y1N7Uo&cid=409501228&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<p>主要是从机器学习的工作流出发，每个阶段主要的工作和工业界流行的技术和概念</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-1.%E5%AD%A6%E4%BB%80%E4%B9%88/image-20220224095954652.png" class="" title="image-20220224095954652">
<p>主要包括四个学习模块：</p>
<ol>
<li><strong>数据</strong> 数据获取、存储、清洗</li>
<li><strong>模型</strong> 训练、迁移、多模态</li>
<li><strong>部署</strong></li>
<li><strong>监控</strong> 可视化</li>
</ol>
<h3 id="学习计划"><a href="#学习计划" class="headerlink" title="学习计划"></a>学习计划</h3><p>基本学习计划：</p>
<ol>
<li>数据模块<ul>
<li>一天一个视频，慢慢学</li>
</ul>
</li>
<li>模型模块<ul>
<li>机器学习、深度学习模块部分，快速过一遍</li>
<li>模型验证，调优部分慢慢学，基础较差</li>
</ul>
</li>
<li>部署 + 监控部分<ul>
<li>慢慢学</li>
</ul>
</li>
</ol>
<p>大概一共30多个视频，三周内学完（无意外的情况下），<strong>deadline 3-15</strong></p>
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML-5.偏差与方差</title>
    <url>/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-5.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/</url>
    <content><![CDATA[<h2 id="Bias-amp-Variance"><a href="#Bias-amp-Variance" class="headerlink" title="Bias &amp; Variance"></a>Bias &amp; Variance</h2><p><strong>bias(偏差)</strong>：模型对于样本的拟合程度，模型输出结果与样本真实结果之间的差距，通过增加模型复杂程度，增加训练轮数，可以实现bias的降低，但可能会出现过拟合问题（high variance）</p>
<p><strong>variance(方差)</strong>：模型预测结果的稳定性，通过简化模型，可以实现variance的降低 计算公式为 $E[(\hat y - E(\hat y)) ^ 2]$</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-5.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/image-20220301152126220.png" class="" title="image-20220301152126220">
<span id="more"></span>
<h3 id="公式角度分析Bias-与-Variance"><a href="#公式角度分析Bias-与-Variance" class="headerlink" title="公式角度分析Bias 与 Variance"></a>公式角度分析Bias 与 Variance</h3><ul>
<li><p>以MSE误差为例，计算bias和variance</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-5.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/image-20220301153351002.png" class="" title="image-20220301153351002">
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-5.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/image-20220301153447334.png" class="" title="image-20220301153447334">
<ul>
<li><p>在样本上求误差均值，首先将f展开为    <script type="math/tex">y = f +\epsilon</script> </p>
</li>
<li><p>再在平方项内添加一个 y_hat期望 <script type="math/tex">E[\hat y]</script></p>
</li>
<li><p>由于 随机误差 $\epsilon$ 均值为0，方差为 $\sigma ^ 2$</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-5.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/image-20220301154214532.png" class="" title="image-20220301154214532">
</li>
<li><p>最终得到 Bias + Variance +  $\sigma ^ 2$ 的形式</p>
</li>
</ul>
</li>
</ul>
<h3 id="偏差与方差之间的关系"><a href="#偏差与方差之间的关系" class="headerlink" title="偏差与方差之间的关系"></a>偏差与方差之间的关系</h3><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-5.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/image-20220301154635423.png" class="" title="image-20220301154635423">
<h3 id="如何降低偏差和方差"><a href="#如何降低偏差和方差" class="headerlink" title="如何降低偏差和方差"></a>如何降低偏差和方差</h3><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-5.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/image-20220301155342109.png" class="" title="image-20220301155342109">
<ul>
<li><strong>集成学习</strong>可以同时降低两个误差</li>
</ul>
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML_2.数据获取</title>
    <url>/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_2.%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96/</url>
    <content><![CDATA[<h2 id="机器学习-数据"><a href="#机器学习-数据" class="headerlink" title="机器学习-数据"></a>机器学习-数据</h2><p>当使用机器学习技术解决实际问题时，最先要考虑模型输入数据问题，如何获取数据，对数据进行标注、清理以及变换等，以满足模型的输入要求。</p>
<h3 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h3><p>主要有两个获取手段</p>
<ol>
<li>寻找已有的数据集（MNIST ImageNet等等）<ul>
<li><a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research">维基百科数据集总结</a></li>
<li><a href="https://paperswithcode.com/datasets">paperwithcode datasets</a> 论文中常见的数据集</li>
<li><a href="https://www.kaggle.com/datasets">Kaggle Datasets</a> 用户上传数据集</li>
<li><a href="https://datasetsearch.research.google.com/">Google Dataset search</a> 数据集搜索引擎，聚合数据集网站内容</li>
<li><a href="https://huggingface.co/datasets">Hugging Face 数据集</a> 聚焦于文本数据</li>
</ul>
</li>
<li>根据任务，收集，形成自己的数据集</li>
<li>生成数据</li>
</ol>
<span id="more"></span>
<p>三种不同的数据集类型</p>
<ol>
<li>学术数据集<ul>
<li>定义清晰，但是局限于某个小问题</li>
</ul>
</li>
<li>竞赛数据集<ul>
<li>接近于真实的机器学习应用</li>
</ul>
</li>
<li>原始数据<ul>
<li>需要消耗大量的精力预处理</li>
</ul>
</li>
</ol>
<p>当收集到的数据来源不同，我们需要对数据进行集成（integrate）</p>
<ul>
<li><p>类似于数据库中不同表的join，寻找合并key，处理重复项、冲突以及确实</p>

</li>
</ul>
<p>当找不到可用的数据，或者收集到的数据量过小时，可通过<strong>人造(synthetic)数据</strong>增加数据量</p>
<ol>
<li>使用GANs  生成数据</li>
<li>数据增强（augmentation） <ul>
<li>图像反转、拉伸等</li>
<li>文本的反复翻译，改变语序</li>
</ul>
</li>
</ol>
<p><strong>总结</strong></p>

<h3 id="网页数据抓取（Scraping）"><a href="#网页数据抓取（Scraping）" class="headerlink" title="网页数据抓取（Scraping）"></a>网页数据抓取（Scraping）</h3><p> 网页数据抓取是获取数据的有效手段，包括ImageNet在内的很多数据集通过这种方式获取生成，其优缺点包括</p>
<ol>
<li>数据较为原始，包含的无关信息较多</li>
<li>数据量大</li>
</ol>

<p>常用的网页抓取工具</p>
<ol>
<li><p>linux的curl命令（通常会被反爬虫屏蔽）</p>
</li>
<li><p>使用模拟浏览器</p>
</li>
<li>不断更换ip，防止短时间大量相同ip访问导致的屏蔽</li>
</ol>
<h3 id="数据标注"><a href="#数据标注" class="headerlink" title="数据标注"></a>数据标注</h3><p>在完成数据收集后，需要考虑数据的标注问题，根据标签的多少，选取不同的策略</p>
<ol>
<li>标签充足-监督/半监督学习</li>
<li>标签不足-人工标注</li>
<li>标签+经费均不足-弱监督学习</li>
</ol>

<h4 id="半监督学习（Semi-supervised-learning-SSL）-简单理解"><a href="#半监督学习（Semi-supervised-learning-SSL）-简单理解" class="headerlink" title="半监督学习（Semi-supervised learning SSL）- 简单理解"></a>半监督学习（Semi-supervised learning SSL）- 简单理解</h4><blockquote>
<p>Semi-supervised learning is a class of machine learning tasks and techniques that also make use of unlabeled data for training – typically a small amount of labeled data with a large amount of unlabeled data.</p>
<p>李宏毅机器学习视频中有<a href="https://www.bilibili.com/video/BV1JE411g7XF?p=64">详细讲解</a></p>
</blockquote>
<p>半监督训练主要针对训练数据只有少部分已标注，大部分均未标注的情况，假设训练数据满足以下条件</p>
<ol>
<li>一致性假设(Continuity assumption) 具有相同特征的样本标签相同</li>
<li>聚类/簇假设(Cluster assumption) 若数据存在簇结构（即可以聚类），一个簇具有一个标签</li>
<li>流形假设(manifold assumption)  高维数据大致会分布在一个低维的流形上,邻近的样本拥有相似的输出,邻近的程度常用“相似”程度来刻画</li>
</ol>
<p><strong>半监督算法之一：自学习</strong></p>
<p>首先用带标签的数据训练一个模型，在未标注数据上进行预测，获得预测标签，也就是所谓的伪标签（Pseudo-labeled），根据置信度选择部分伪标签数据与标签数据合并训练一个新模型（自学习），不断循环训练，直到模型满足任务要求。</p>

<p><strong>半监督学习算法之二：主动学习（active learning）</strong></p>
<p>与自学训练过程类似，但是每次预测结果<strong>最典型的</strong>未标记样本，由人工标记</p>
<ul>
<li>uncertainty sampling 选取最不确定的样本作为最典型（概率为1/N）</li>
</ul>

<h4 id="Label-through-Crowdsourcing-众包"><a href="#Label-through-Crowdsourcing-众包" class="headerlink" title="Label through Crowdsourcing(众包)"></a>Label through Crowdsourcing(众包)</h4><p>花钱找人标注</p>

<h4 id="弱监督学习（Weak-Supervision）"><a href="#弱监督学习（Weak-Supervision）" class="headerlink" title="弱监督学习（Weak Supervision）"></a>弱监督学习（Weak Supervision）</h4><p>半自动生成标号，虽然比人工标注差，但是足够训练使用，常用手段 </p>
<p><strong>Data programming</strong></p>
<p>人工总结一些规律，设计一些规则（规则匹配），通过程序半自动生成标签</p>


<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4>]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML_3.提升数据质量</title>
    <url>/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/</url>
    <content><![CDATA[<h2 id="提升数据质量"><a href="#提升数据质量" class="headerlink" title="提升数据质量"></a>提升数据质量</h2><p>在数据收集完成后，数据可能还存在大量的噪声、或者模型难以使用，需要我们进一步处理提高数据质量</p>
<ol>
<li>数据噪声较多（脏数据过多） - 数据清洗</li>
<li>数据格式与模型要求输入不符 - 数据变换</li>
<li>数据难学习                                - 特征工程</li>
</ol>
<h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><p>针对数据噪声较多的问题，通过数据清洗改善数据质量，常见的Data Error由</p>
<ol>
<li>Outliers(离群值) 某些数据值远远偏离数据整体分布</li>
<li>Rule violations(违反约束) 例如某些非空字段为空</li>
<li>Pattern violation（语义语法冲突）单位是美元，数据给rmb；数据项目标类型是float，实际类型是string</li>
</ol>
<span id="more"></span>
<h4 id="离群检测-Outlier-Detection"><a href="#离群检测-Outlier-Detection" class="headerlink" title="离群检测(Outlier Detection)"></a>离群检测(Outlier Detection)</h4><h4 id="基于规则检测（Rule-based-Detection）"><a href="#基于规则检测（Rule-based-Detection）" class="headerlink" title="基于规则检测（Rule-based Detection）"></a>基于规则检测（Rule-based Detection）</h4><h4 id="基于模式检测（Pattern-based-Detection）"><a href="#基于模式检测（Pattern-based-Detection）" class="headerlink" title="基于模式检测（Pattern-based Detection）"></a>基于模式检测（Pattern-based Detection）</h4><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/image-20220228150552932.png" class="" title="image-20220228150552932">
<h3 id="数据变换（Data-Transformation）"><a href="#数据变换（Data-Transformation）" class="headerlink" title="数据变换（Data Transformation）"></a>数据变换（Data Transformation）</h3><p>实值规范化（Normalization）</p>
<ol>
<li>归一化</li>
<li>Z-score</li>
<li>十进制放缩</li>
<li>log放缩</li>
</ol>
<p>图片转换（减少图片的空间占用）</p>
<ol>
<li>下采样、裁切</li>
<li>图像白化处理（Whitening） 减少像素量</li>
</ol>
<p>视频转换 （预处理以平衡 存储，质量和加载速度 三者之间的关系）</p>
<ul>
<li>通常使用 短视频裁切（&lt;10sec），每个切片内包含单个时间</li>
</ul>
<p>文本转换</p>
<ol>
<li>Stemming and lemmatization 还原词形</li>
<li>Tokenization 切词</li>
</ol>
<h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/image-20220228155016025.png" class="" title="image-20220228155016025">
<p>如何表示特征值？</p>
<ol>
<li><p>直接使用数值表示 或者 桶划分</p>
</li>
<li><p>类别特征可采用独热向量（one-hot）</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/image-20220228155854227.png" class="" title="image-20220228155854227">
</li>
<li><p>日期 按照复合特征处理，划分为子特征</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/image-20220228155800660.png" class="" title="image-20220228155800660">
</li>
<li><p>将几个单独特征组合形成新特征</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/image-20220228160046548.png" class="" title="image-20220228160046548">
</li>
</ol>
<p><strong>文本数据</strong> 特征表示</p>
<ol>
<li><p>one hot 或者 word embedding</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/image-20220228160535728.png" class="" title="image-20220228160535728">
</li>
<li><p>预训练模型做特征提取（上下文语义嵌入 context embedding）</p>
</li>
</ol>
<p><strong>图片/视频</strong> 特征表示</p>
<ol>
<li>传统的手工特征抽取，例如SIFT</li>
<li>深度神经网络特征提取（ResNet）,类似于提取文本特征的预训练网络</li>
</ol>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/image-20220228161500068.png" class="" title="image-20220228161500068">]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML_4.模型验证</title>
    <url>/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_4.%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/</url>
    <content><![CDATA[<h2 id="模型验证（Validation）"><a href="#模型验证（Validation）" class="headerlink" title="模型验证（Validation）"></a>模型验证（Validation）</h2><h3 id="验证（validation）集与测试（test）集"><a href="#验证（validation）集与测试（test）集" class="headerlink" title="验证（validation）集与测试（test）集"></a>验证（validation）集与测试（test）集</h3><p>验证集往往是从训练集中划分出的一部分数据，用来验证模型泛化能力，可以使用多次；测试集是单独的一系列数据，在模型训练完成后，衡量模型效果，一般只使用一次</p>
<h3 id="如何生成验证集"><a href="#如何生成验证集" class="headerlink" title="如何生成验证集"></a>如何生成验证集</h3><h4 id="随机划分"><a href="#随机划分" class="headerlink" title="随机划分"></a>随机划分</h4><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_4.%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/image-20220301142501284.png" class="" title="image-20220301142501284">
<h4 id="特殊情况"><a href="#特殊情况" class="headerlink" title="特殊情况"></a>特殊情况</h4><p>某些情况下，训练数据可能不适合采用随机划分的方式验证集合，如</p>
<ol>
<li>具有序列关系的数据-股价、房子销售<ul>
<li>验证集数据应在训练集后，避免模型训练使用到了验证集的未来信息，导致模型在验证集上的表现较好</li>
</ul>
</li>
<li>训练数据由不同组，每个组有多个样本-同一个人的多个照片<ul>
<li>以组为单位进行随即划分</li>
<li>一百组照片，选70个人训练，30个人验证</li>
</ul>
</li>
<li>类别不均衡数据<ul>
<li>对于较小类更多的采样</li>
</ul>
</li>
</ol>
<span id="more"></span>
<h4 id="K-fold-Cross-Validation-k折交叉验证"><a href="#K-fold-Cross-Validation-k折交叉验证" class="headerlink" title="K-fold Cross Validation k折交叉验证"></a>K-fold Cross Validation k折交叉验证</h4><p>将数据集划分为k个子集，每次选取一个子集作为有验证集，其余k个集合合并为训练集，重复k次，用k次平均验证误差作为验证集误差。</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_4.%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/image-20220301144205630.png" class="" title="image-20220301144205630">
<h3 id="有关验证集的常见错误（common-mistakes）"><a href="#有关验证集的常见错误（common-mistakes）" class="headerlink" title="有关验证集的常见错误（common mistakes）"></a>有关验证集的常见错误（common mistakes）</h3><ol>
<li>验证集中有来自训练集的样本<ul>
<li>数据集中有重复样本</li>
</ul>
</li>
<li>数据泄露（leaking）<ul>
<li>训练时用到了验证集未来的数据（时间序列分析）</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML_6.集成学习</title>
    <url>/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="集成学习（Ensemble-Learning）"><a href="#集成学习（Ensemble-Learning）" class="headerlink" title="集成学习（Ensemble Learning）"></a>集成学习（Ensemble Learning）</h2><blockquote>
<p>In <a href="https://en.wikipedia.org/wiki/Statistics">statistics</a> and <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>, <strong>ensemble methods</strong> use multiple learning algorithms to obtain better <a href="https://en.wikipedia.org/wiki/Predictive_inference">predictive performance</a> than could be obtained from any of the constituent learning algorithms alone.</p>
</blockquote>
<h3 id="Bagging-Bootstrap-Aggregating"><a href="#Bagging-Bootstrap-Aggregating" class="headerlink" title="Bagging-Bootstrap Aggregating"></a>Bagging-Bootstrap Aggregating</h3><p>Bagging主要思路是通过结合几个模型(<strong>多个相同模型</strong>)降低总体的泛化误差（bias+viariance），实际上降低的是<strong>方差</strong></p>
<ol>
<li>同时训练多个模型（parallel）</li>
<li>输出取模型平均值（回归模型）或投票（分类模型）</li>
</ol>
<p>其中每个模型训练数据通过bootstrap sampling方式采样</p>
<ol>
<li>有放回采样，m个训练数据，进行m次又放回采样</li>
<li>大概有 $1- \frac{1}e$ 63%的数据在一次bootstrap采样中未被采样到（out of bag），可以作为模型的验证集</li>
</ol>
<span id="more"></span>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220302112353956.png" class="" title="image-20220302112353956">
<h4 id="Random-Forest-特殊的bagging"><a href="#Random-Forest-特殊的bagging" class="headerlink" title="Random Forest-特殊的bagging"></a>Random Forest-特殊的bagging</h4><ul>
<li>基学习器为决策树</li>
<li>bootstrap采样过程中，不仅对样本进行随机采样，同样对属性列进行采样，增加决策树的多样性</li>
</ul>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220302112534348.png" class="" title="image-20220302112534348">
<h4 id="Unstable-Learners"><a href="#Unstable-Learners" class="headerlink" title="Unstable Learners"></a>Unstable Learners</h4><p>对于不稳定（方差较大）模型效果提升较好</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303083634112.png" class="" title="image-20220303083634112">
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303083728465.png" class="" title="image-20220303083728465">
<ul>
<li>当 $h(x)$ 即模型预测值，方差较小，上述不等式趋向于等号</li>
<li>极端情况，当 $h(x)$ 恒相等时， $E(h(x))^2 = E(h(x)^2)$ , 使用bagging对模型效果没有提升</li>
<li>由上公式易得，bagging能够通过较少viarance，降低模型泛化误差，模型方差越大，提升效果越好</li>
</ul>
<p>决策树就是一种 unstable learner,而线性回归是稳定的</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303084813664.png" class="" title="image-20220303084813664">
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303084859931.png" class="" title="image-20220303084859931">
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>将一系列的弱模型(<strong>多个相同模型</strong>)结合组成一个强模型，以降低模型的偏差（bias）</p>
<ul>
<li>按照顺序学习n个弱模型，每次模型训练完成后，根据该模型预测错误部分对数据重新采样，训练下一个模型</li>
<li>不断的迭代，“在bias上不断地boosting”</li>
</ul>
<h4 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h4><p>每次模型训练目标为拟合残差（有点类似于梯度拟合）</p>
<ol>
<li>训练新模型 $h_t$ 时，其<strong>训练目标</strong>为 <strong>预测真实值$y_i$</strong> 与 <strong>已训练模型预测值和 $H_{t}(x_i)$</strong>的差</li>
<li>累积得到新的预测值和 $H_{t+1}(x) = H{t}(x) + \eta h_t(x)$, 其中 $\eta$ 为正则项系数，为了避免模型过度拟合</li>
</ol>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303090038631.png" class="" title="image-20220303090038631">
<p>当损失函数为MSE，即均方误差损失时，实际上残差即为负梯度，Gradient 名字的由来</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303091042627.png" class="" title="image-20220303091042627">
<h4 id="Gradient-Boosting-Decision-Tree-GBDT"><a href="#Gradient-Boosting-Decision-Tree-GBDT" class="headerlink" title="Gradient Boosting  Decision Tree(GBDT)"></a>Gradient Boosting  Decision Tree(GBDT)</h4><p>使用决策树作为弱学习模型，问题就是构建、训练时间较长</p>
<ul>
<li>使用强模型，Gradient Boosting容易出现过拟合现象</li>
<li>通过限制决策树的层数（2-3）层，控制过拟合</li>
</ul>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303091517929.png" class="" title="image-20220303091517929">
<p>Boosting VS 随机森林</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303091554319.png" class="" title="image-20220303091554319">
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p>降低 <strong>偏差 偏差 偏差 ！！！</strong></p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303091702515.png" class="" title="image-20220303091702515">
<h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><p>类似于bagging,将不同的基模型连接起来，共同预测结果以降低方差（variance）</p>
<ul>
<li><p>基模型可以是不同的类别</p>
</li>
<li><p>最后由一个线性全连接层，输入各个模型的输出，输出目标结果</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303092746542.png" class="" title="image-20220303092746542">
</li>
</ul>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303092311096.png" class="" title="image-20220303092311096">
<h4 id="Multi-layer-Stacking"><a href="#Multi-layer-Stacking" class="headerlink" title="Multi-layer Stacking"></a>Multi-layer Stacking</h4><p>多层堆叠模型，降低模型偏差（bias）</p>
<ul>
<li>容易过拟合</li>
</ul>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303094206264.png" class="" title="image-20220303094206264">
<p>如何降低过拟合问题</p>
<ol>
<li>将数据划分A和B两部分，其中A部分用来训练第一层，并用第一层预测B，预测结果作为第二层输入训练第二层（避免了原来多层堆叠出现的一个在不同层重复训练的情况）</li>
<li>重复 k-fold bagging <ul>
<li>使用k-fold训练k个模型（在多层堆叠中特指<strong>第一层的所有模型</strong>）</li>
<li>将每个模型在k-fold训练中作为验证集部分数据的输出拼接成一个份完整的第一层输出，输入到第二层训练</li>
<li><strong>进一步降低方差</strong>：重复n次k折交叉验证,每个数据均有n个输出，对n个输出做平均，获得一个完整的第一层输出，输入到第二层训练</li>
</ul>
</li>
</ol>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303095451723.png" class="" title="image-20220303095451723">
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303095533009.png" class="" title="image-20220303095533009">
<h4 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h4><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303095827469.png" class="" title="image-20220303095827469">
<h3 id="集成学习总结"><a href="#集成学习总结" class="headerlink" title="集成学习总结"></a>集成学习总结</h3><p>简单理解就是</p>
<ul>
<li>增加模型数量，学习相同内容,可以降低 Variance</li>
<li>增加模型，不断学习误差，可以降低Bias</li>
<li>K-fold multi-level stacking 通过横向增加模型数量，纵向增加层数，可以既降低Bias，又降低 Variance</li>
<li>由于多个模型容易出现过拟合现象<ul>
<li>多模型降低Variance，通过 特殊采样方法（bagging的bootstrap，staking的k-fold）避免过拟合</li>
<li>降低Bias,通过 正则项 避免过拟合</li>
</ul>
</li>
</ul>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303100132120.png" class="" title="image-20220303100132120">
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML_7.模型调参</title>
    <url>/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/</url>
    <content><![CDATA[<h2 id="模型调参（Tuning）"><a href="#模型调参（Tuning）" class="headerlink" title="模型调参（Tuning）"></a>模型调参（Tuning）</h2><p>模型参数对模型效果有影响较大， 在不同的问题中，我们不仅需要选择合适的模型，也要选择合适的参数</p>
<ol>
<li><p>手动调参（Manual）</p>
<ul>
<li><p>从默认参数（工具包默认参数、论文推荐参数）开始</p>
</li>
<li><p>不断调整参数，记录调整后的结果（tensorboard, weight &amp; bias）</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220303145559540.png" class="" title="image-20220303145559540">
</li>
</ul>
</li>
<li><p>自动调参（Automated）</p>
<ul>
<li><p>计算成本在下降，人力成本在上升</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220303145731538.png" class="" title="image-20220303145731538">
</li>
</ul>
</li>
<li><p>AutoML(Automated Machine Learning)</p>
<ul>
<li>从数据清洗、特征提取到模型选择每一步均由AutoML自动完成</li>
<li>目前模型选择之前的步骤，AutoML完成效果并不好，主要能够解决的两个问题<ol>
<li>HPO（Hyperparameter optimization）选择合适的超参数</li>
<li>NAS（Neural architecture search）网络架构搜索</li>
</ol>
</li>
</ul>
</li>
</ol>
<span id="more"></span>
<h3 id="HOP-Algorithm-超参数选择"><a href="#HOP-Algorithm-超参数选择" class="headerlink" title="HOP Algorithm 超参数选择"></a>HOP Algorithm 超参数选择</h3><p>首先确定参数的搜索空间，例如</p>
<ul>
<li>避免超参数空间过大，导致搜索成本过高</li>
</ul>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304082552518.png" class="" title="image-20220304082552518">
<p>在搜索空间上采取特定的搜索算法进行搜索，主要分为两大类别</p>
<ol>
<li>Black-box 每个参数集选择进行一次完整的训练，训练完成后比较不同参数集模型效果</li>
<li>Multi-fidelity 修改训练过程，降低搜索代价<ul>
<li>在训练数据子集上训练</li>
<li>减小模型大小（减少层数，减少channel等）</li>
<li>提前终止某些效果明显较差的参数实验</li>
</ul>
</li>
</ol>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304083338601.png" class="" title="image-20220304083338601">
<h4 id="两类常见的HPO算法"><a href="#两类常见的HPO算法" class="headerlink" title="两类常见的HPO算法"></a>两类常见的HPO算法</h4><ol>
<li><p>Grid Search 网格搜索</p>
<ul>
<li>传入不同参数的多个值，搜索所有参数不同值的所有组合</li>
<li>获得 最优的参数值组合</li>
<li>随着参数数量的增加，组合数量呈指数级上升，训练成本较高</li>
</ul>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304083750975.png" class="" title="image-20220304083750975">
</li>
<li><p>Random Search</p>
<ul>
<li>与Grid Search类似，但是只随机选择n次参数组合</li>
<li>只搜索一部分参数组合空间，一定程度减少了训练成本</li>
</ul>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304083937025.png" class="" title="image-20220304083937025">
</li>
</ol>
<h4 id="Bayesian-Optimization-BO-贝叶斯优化-不深入理解"><a href="#Bayesian-Optimization-BO-贝叶斯优化-不深入理解" class="headerlink" title="Bayesian Optimization(BO) 贝叶斯优化-不深入理解"></a>Bayesian Optimization(BO) 贝叶斯优化-不深入理解</h4><p>通过不断地对数据进行采样（超参数与模型评价指标的对应关系）学习一个从<strong>超参数</strong>到<strong>模型评价指标</strong>（用该超参数训练出来的模型的评价指标）之间的函数，每次采样会根据以往的采样结果，选取采样点(online learning的感觉)。</p>
<p><strong>Surrogate model</strong></p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304090035219.png" class="" title="image-20220304090035219">
<p><strong>Acquisition function</strong></p>
<ul>
<li>每次新采样点选取 <strong>Acquisition max</strong>，即时Acquisition function最大的点</li>
<li><strong>Acquisition max</strong>的样本点 <strong>约等于</strong> <strong>置信区间较大 + 可能取得较高评价指标</strong>的点（<strong>简单理解</strong>：置信区间小我就没必要再采样了，直接使用模型预测超参数；较高指标的超参数是模型追求的目标，总结就是 <strong>既要增加可信度又要找到最优解</strong>）</li>
<li>Trade off exploration and exploitation 在 <strong>探索未知解</strong> 和 <strong>最优解附近深挖</strong> 的权衡</li>
</ul>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304090743169.png" class="" title="image-20220304090743169">
<p><strong>BO算法的局限性</strong></p>
<ul>
<li>算法的最初始阶段，近似于随机搜索</li>
</ul>
<h4 id="Successive-Halving"><a href="#Successive-Halving" class="headerlink" title="Successive Halving"></a>Successive Halving</h4><p>算法如其名，比较简单</p>
<ol>
<li>随机选择n个超参数组合，训练m轮</li>
<li>每m轮训练完毕，删除 n/2 个指标相对较差的超参数组合</li>
<li>不断重复第二步，直到只剩下一个超参数组合</li>
</ol>
<p>根据具体的预算，选取n和m</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304092244412.png" class="" title="image-20220304092244412">
<h4 id="Hyperband-实际使用比较多的算法"><a href="#Hyperband-实际使用比较多的算法" class="headerlink" title="Hyperband-实际使用比较多的算法"></a>Hyperband-实际使用比较多的算法</h4><p>在Successive Halving算法中，n和m的选取很大程度上会影响最后获得最优超参数组合</p>
<ul>
<li>资源固定的情况下，<strong>m*n = budget</strong> </li>
<li>当m选取不够大时，有些超参数组合可能在前m个epoch指标表现不好（训练不充分），导致提前被淘汰</li>
<li>当n选取不够大时，随机采样的超参数组合可能过少，无法找到最优解</li>
</ul>
<p>Hyperband算法针对Successive Halving算法的以上问题，进行了针对性改进</p>
<ol>
<li>运行多次 Successive Halving算法，每次运行降低n，增加m</li>
<li>先 exploration  再 exploitation,先 BFS 再 DFS</li>
<li>单次成本不变，多次运行，最后还是增加计算成本</li>
</ol>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304093734300.png" class="" title="image-20220304093734300">
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304093845224.png" class="" title="image-20220304093845224">
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304094115462.png" class="" title="image-20220304094115462">
<h3 id="Neural-Architecture-Search-NAS-Algorithm"><a href="#Neural-Architecture-Search-NAS-Algorithm" class="headerlink" title="Neural Architecture Search(NAS) Algorithm"></a>Neural Architecture Search(NAS) Algorithm</h3><p>超参数选择中，除了学习率、epoch、batchsize等的训练超参数，还包括模型层数、隐藏层维度等的涉及到模型结构的参数</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304110012929.png" class="" title="image-20220304110012929">
<p>NAS算法用来解决模型结构相关参数的选择</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304110121568.png" class="" title="image-20220304110121568">
<h4 id="NAS-with-Reinforcement-Learning"><a href="#NAS-with-Reinforcement-Learning" class="headerlink" title="NAS with  Reinforcement Learning"></a>NAS with  Reinforcement Learning</h4><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304110328974.png" class="" title="image-20220304110328974">
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304110853192.png" class="" title="image-20220304110853192">
<h4 id="The-One-shot-Approach（思想）"><a href="#The-One-shot-Approach（思想）" class="headerlink" title="The One-shot Approach（思想）"></a>The One-shot Approach（思想）</h4><ul>
<li><p>将网络架构与模型参数的学习连接在一起（既学网络架构，又学网络参数）</p>
</li>
<li><p>训练一个大模型，其子模型为各种各样的备选架构（先搞大杂烩，再从大杂烩里挑我要的）</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304111150887.png" class="" title="image-20220304111150887">
</li>
<li><p>问题在于 大模型资源耗费过大，如何解决？</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304111204080.png" class="" title="image-20220304111204080">
</li>
</ul>
<p><strong>Differentiable Architecture Search</strong></p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304144329266.png" class="" title="image-20220304144329266">
<ul>
<li><p>每一层有多个候选网络/结构/模型</p>
</li>
<li><p>第l层的第i个候选网络的输出为 $0_i^l$</p>
</li>
<li><p>每一层后 select模块，权重为 $a^l$ ，对该层所有候选模型输出加权求和输入到 $l + 1 $ 层</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304144536920.png" class="" title="image-20220304144536920">
</li>
<li><p>最后选择每层中 参数最大的 候选模型</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304144709664.png" class="" title="image-20220304144709664">
</li>
</ul>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304143711492.png" class="" title="image-20220304143711492">
<h4 id="Scaling-CNNs"><a href="#Scaling-CNNs" class="headerlink" title="Scaling CNNs"></a>Scaling CNNs</h4><p>对于卷积神经网络，有三种调整架构的方式</p>
<ul>
<li>更深：增加层数</li>
<li>更宽：增加channel</li>
<li>更大的输入：提高输入图像分辨率</li>
</ul>
<p>EfficientNet 提出了一种Scaling方式（调整一个，其他两个同时一起调节）</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304145127431.png" class="" title="image-20220304145127431">
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304145828105.png" class="" title="image-20220304145828105">]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML_8.深度神经网络</title>
    <url>/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h2 id="深度学习模型设计模式"><a href="#深度学习模型设计模式" class="headerlink" title="深度学习模型设计模式"></a>深度学习模型设计模式</h2><p>随着深度学习的不断发展，各式各样包括bert、transformer在内的深度神经网络架构层出不穷，在这些不同的网络架构中往往包含一些通用的深度网络设计模式，主要理解三种：</p>
<ol>
<li>batch/layer normalization</li>
<li>残差</li>
<li>注意力机制</li>
</ol>
<h3 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h3><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>对输入的一个batch进行标准化处理（减均值，除以方差），能够有效的降低深度网络的学习难度</p>
<ul>
<li>l从梯度角度理解，当从x-&gt;y的梯度变化 $\beta$ 变化较大时，梯度下降时沿着x点梯度移动，学习会出现偏差(斜率替代每个点的导数，如果导数变化过大，沿着斜率走就会偏离函数图像)</li>
<li>对于分布变化较大的数据要采用较小的学习率，否则就需要通过Batch Normalization对输入进行平滑化（smooth）</li>
<li>只对线性方法起作用，深度神经不起作用</li>
</ul>
<span id="more"></span>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307084121951.png" class="" title="image-20220307084121951">
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307084716730.png" class="" title="image-20220307084716730">
<p>批量归一化的主要步骤：</p>
<ol>
<li><p>首先将输入转化为2D形式数据</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307084939777.png" class="" title="image-20220307084939777">
</li>
<li><p>标准化</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307084955220.png" class="" title="image-20220307084955220">
</li>
<li><p>还原</p>
<ul>
<li>$\beta_j $ $\gamma_j$两个参数神经网络学习决定</li>
<li>形式上理解为 由模型通过学习决定是否真的需要normalization</li>
</ul>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307085101509.png" class="" title="image-20220307085101509">
</li>
<li><p>转换为输入形状，输出</p>
</li>
</ol>
<p>实现代码如下</p>
<ol>
<li><p>训练过程中 并不保留所有batch的均值和方差平均值 供预测使用</p>
</li>
<li><p>每次使用动量更新一个新的moving_var,moving_mean在预测中使用</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307090421825.png" class="" title="image-20220307090421825">
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307090430766.png" class="" title="image-20220307090430766">
</li>
</ol>
<p>完整代码：</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307090218611.png" class="" title="image-20220307090218611">
<h4 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h4><blockquote>
<p>BN在batch上做归一化，LN在样本上做归一化</p>
</blockquote>
<p>由于RNN循环计算特性，不同时间步类似于不同的batch，均值和方差变化较大，无法共享使用（若输入长度为10，需要维护10个不同时间步的均值和方差，当预测长度为20时，后10个时间步在训练过程中没有均值和方差）</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307091707388.png" class="" title="image-20220307091707388">
<p>相较于Batch Normalization，将时间步看作为batch维度，reshape维度改变，其他操作不同</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307091911670.png" class="" title="image-20220307091911670">
<h4 id="More-Normalization"><a href="#More-Normalization" class="headerlink" title="More Normalization"></a>More Normalization</h4><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307092724701.png" class="" title="image-20220307092724701">
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>Normalization的主要目的是为了 使目标函数平滑化，降低模型学习难度（即可以使用更大的学习率）</li>
<li>一般步骤包括<ul>
<li>reshape</li>
<li>normalization</li>
<li>recovery</li>
</ul>
</li>
</ul>
<h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p>略</p>
<h3 id="残差"><a href="#残差" class="headerlink" title="残差"></a>残差</h3><p>略</p>
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML_9.迁移学习</title>
    <url>/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><p>由于深度网络的训练对数据要求较大，且训练成本较高，对在其他任务训练好的深度模型使用到相关任务的需求较大，主要方式有</p>
<ol>
<li>使用深度模型做特征抽取（bert，word2vec等），输入到不同模型，解决不同任务</li>
<li>在方便训练的相近的任务（可能是 训练数据充足等）上训练模型，在目标任务上重用训练好的模型</li>
<li>微调预训练模型</li>
</ol>
<h3 id="Fine-tuning（从CV方向进行讲解）"><a href="#Fine-tuning（从CV方向进行讲解）" class="headerlink" title="Fine-tuning（从CV方向进行讲解）"></a>Fine-tuning（从CV方向进行讲解）</h3><p>CV领域存在很多良好的数据集（imagenet），如何将在这些大数据集上训练好的模型（学到的知识）迁移到自己的任务上来？这就是Fine-tuning要做的任务。</p>
<h4 id="Pre-trained-Model"><a href="#Pre-trained-Model" class="headerlink" title="Pre-trained Model"></a>Pre-trained Model</h4><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220307144746729.png" class="" title="image-20220307144746729">
<span id="more"></span>
<h4 id="如何进行Fine-Tuning"><a href="#如何进行Fine-Tuning" class="headerlink" title="如何进行Fine-Tuning"></a>如何进行Fine-Tuning</h4><p>模型构建</p>
<ul>
<li>直接使用预训练模型的所有参数和架构</li>
<li>只随机初始化最后一个输出层的参数</li>
</ul>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220307145106552.png" class="" title="image-20220307145106552">
<p>模型学习</p>
<ul>
<li>使用小学习率寻找解（预训练模型已在解附近，小步探索）</li>
</ul>
<h4 id="Freeze-Bottom-Layers"><a href="#Freeze-Bottom-Layers" class="headerlink" title="Freeze Bottom Layers"></a>Freeze Bottom Layers</h4><p>深度神经网络不同层在具体任务中捕捉不同层级的特征信息，更接近输出层的网络层更加倾向于捕捉<strong>任务相关特征</strong>（task specific），底层网络更加倾向于<strong>捕捉通用的、一般的特征</strong>（图片中的曲线、边等）</p>
<p>针对预训练模型该特性以及要Fine-tuning目标任务，对底层和高层网络采用不同训练手段</p>
<ul>
<li>底层冻结或者低学习率</li>
<li>高层正常训练</li>
</ul>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220307150714650.png" class="" title="image-20220307150714650">
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220307150826148.png" class="" title="image-20220307150826148">
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220307151536617.png" class="" title="image-20220307151536617">
<h3 id="Fine-tuning-in-NLP"><a href="#Fine-tuning-in-NLP" class="headerlink" title="Fine-tuning in NLP"></a>Fine-tuning in NLP</h3><p>NLP领域面临的问题与CV不同，NLP没有大规模良好标记的数据集，只有大量未标记的文本数据（维基百科、电子书、爬取的数据等）</p>
<p>只能使用自监督学习学习的方式进行模型预训练</p>
<ul>
<li>生成伪标签，然后在伪标签上进行监督学习</li>
<li>主要的两种类型<ol>
<li>语言模型：给定序列预测下一个词</li>
<li>Masked language model：给定序列，预测序列中的某一个词</li>
</ol>
</li>
</ul>
<h4 id="常见预训练模型"><a href="#常见预训练模型" class="headerlink" title="常见预训练模型"></a>常见预训练模型</h4><ol>
<li>词嵌入模型：获得包含语义信息的词向量</li>
<li>基于transformer的预训练模型<ul>
<li>BERT：encoder  适用于文本分类等的NLU任务</li>
<li>GPT: decoder 适用于摘要、总结等NLG任务</li>
<li>T5: encoder-decoder 适用于摘要、总结等NLG任务</li>
</ul>
</li>
</ol>
<p><strong>bert微调小技巧</strong></p>
<ul>
<li><p>在将bert应用到自己任务时，可以将bert接近输出层的几层权重默认初始化</p>
</li>
<li><p>Hugging Face 获取预训练模型</p>
<img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220308085742505.png" class="" title="image-20220308085742505">
</li>
</ul>
<h4 id="应用-1"><a href="#应用-1" class="headerlink" title="应用"></a>应用</h4><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220308085949407.png" class="" title="image-20220308085949407">
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><img src="/2022/03/08/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220308090255661.png" class="" title="image-20220308090255661">]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-1.如何在计算机中表示词语</title>
    <url>/2021/08/11/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/</url>
    <content><![CDATA[<img src="/2021/08/11/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210809091658017.png" class="" title="image-20210809091658017">
<p>想要通过计算机解决NLP问题，首先要解决的就是词语表示问题，由于一个词语在不同语境以及不同文化背景下含义的多样性，如何在计算机中有效的存储，表示不同词语的不同含义，是需要解决的重要问题。</p>
<h3 id="传统表示方式"><a href="#传统表示方式" class="headerlink" title="传统表示方式"></a>传统表示方式</h3><h4 id="WordNet-（discrete-representation）"><a href="#WordNet-（discrete-representation）" class="headerlink" title="WordNet （discrete representation）"></a>WordNet （discrete representation）</h4><blockquote>
<p>上义词是对事物的概括性、抽象性说明；下义词是事物的具体表现形式或更为具体的说明</p>
</blockquote>
<p>采用同义词（synonym）和上义词（hypernym）两个相关词语集合来描述当前词语的含义，当前方法一定程度上能够正确表示词语含义，但是存在一定问题</p>
<ol>
<li>忽略了词语在不同语境中的细微语义差异（比如 “中” 只有在河南话中和 “好” 是同义词）</li>
<li>词语的新的词义的添加较为困难</li>
<li>同义词和上义词定义较为主观，需要人工来整理两个词语集合</li>
</ol>
<img src="/2021/08/11/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210809091554618.png" class="" title="image-20210809091554618">
<span id="more"></span>
<h4 id="one-hot-词向量"><a href="#one-hot-词向量" class="headerlink" title="one-hot 词向量"></a>one-hot 词向量</h4><p>引入神经网络的向量思想，使用词的位置独热编码表示作为词在计算机中的表示形式，其中词向量的维度就是词典中词语的个数，这就导致了如果词典中词语数量增加，会导致词向量维度不断增加。</p>
<img src="/2021/08/11/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210809093045800.png" class="" title="image-20210809093045800">
<p>另一问题是所有独热编码表示的词语，均正交，无法表示词语之间的关联关系，由此引出 要将词语的相似度表示在词向量中的想法。</p>
<h4 id="Word-Vector"><a href="#Word-Vector" class="headerlink" title="Word Vector"></a>Word Vector</h4><blockquote>
<p> <strong>Distributional semantics</strong>:一个词语的含义由其上下文的词语所决定</p>
</blockquote>
<img src="/2021/08/11/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210809093839703.png" class="" title="image-20210809093839703">
<p>为了解决独热编码词向量的稀疏问题，引入分布式语义，根据上下文确定当前词语的dense vector，即Word Vector（Word Embedding）</p>
<h3 id="Word2Vec（跳词模型为例）"><a href="#Word2Vec（跳词模型为例）" class="headerlink" title="Word2Vec（跳词模型为例）"></a>Word2Vec（跳词模型为例）</h3><p>通过上下文相关性，确定当前词语的含义，即向量表示，这种相关性定义为某一单词上下文出现另一个单词的概率，如下图表示的为以C为中心词，周围出现单词o的概率。</p>
<img src="/2021/08/11/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210809095950546.png" class="" title="image-20210809095950546">
<p>为了方便计算为每个词典中的词语定义了 中心词向量$v_i$​ 和背景词$u_i$​​两个特征向量。根据由中心词确定上下文的思想，因此整个上下文句子的似然函数为</p>
<script type="math/tex; mode=display">
L(\theta) = \prod^T_{t=1}\prod_{-m<j<m 且j\neq0}P(w_{t+j}|w_t,\theta)\tag{1}</script><p>即以上下文中所有词语作为中心词，在窗口大小为2m的条件下，出现背景词概率乘积的连，对应的损失函数形式为</p>
<script type="math/tex; mode=display">
J(\theta) = -\frac{1}{T}log(L(\theta))= -\frac{1}{T}\sum^T_{t=1}\sum_{-m<j<m 且j\neq0}log(P(w_{t+j}|w_t,\theta))\tag{2}</script><p>带入概率公式求损失函数（2）关于特定中心词向量$v_c$​​​​​的偏导数，可得</p>
<script type="math/tex; mode=display">
\frac{\partial J(\theta)}{\partial v_c} = \sum_{-m<j<m 且j\neq0} [u_{c+j} - \sum_{w \in v} P(w|c)u_w]\tag{3}</script><p>即损失函数以中心词$v_c$​ 背景词$u_{c+j}$​​​​的部分偏导数为：</p>
<ul>
<li><strong>当前背景词的背景向量 - 当前词库所有背景词以$v_c$为中心词概率分布的背景向量$u$的加权平均和</strong></li>
</ul>
<h4 id="为什么设置两个向量（背景向量-u-和-中心向量-v-）"><a href="#为什么设置两个向量（背景向量-u-和-中心向量-v-）" class="headerlink" title="为什么设置两个向量（背景向量$u$ 和 中心向量 $v$）"></a>为什么设置两个向量（背景向量$u$ 和 中心向量 $v$）</h4><p>为了便于损失函数求偏导计算，在真正使用时，使用两个向量的平均值作为词语的表征向量</p>
<h4 id="如何解决计算量过大的问题"><a href="#如何解决计算量过大的问题" class="headerlink" title="如何解决计算量过大的问题"></a>如何解决计算量过大的问题</h4><p>由条件概率公式可得，分母的计算需要遍历整个语料库，计算量过大</p>
<h5 id="随机梯度下降（Stochastic-Gradient-Descent）"><a href="#随机梯度下降（Stochastic-Gradient-Descent）" class="headerlink" title="随机梯度下降（Stochastic Gradient Descent）"></a>随机梯度下降（Stochastic Gradient Descent）</h5><p>按照特定抽样方法，每个训练epoch从语料库中抽取一部分的作为当前训练语料库，抽样能否代表整体，取决于抽样方法的设计</p>
<img src="/2021/08/11/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811084901573.png" class="" title="image-20210811084901573">
<h5 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h5><img src="/2021/08/11/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811093414764.png" class="" title="image-20210811093414764">
<blockquote>
<p>样本量大时，k一般选择2-5,样本量小时，k一般选择5-20</p>
</blockquote>
<p>负采样通过两个思路解决计算量较大的问题：</p>
<ol>
<li>将概率公式从softmax替换为sigmoid</li>
<li>概率公式的计算范围为负采样空间内</li>
</ol>
<p>以词袋模型为例，当 $context(w)$ 预测 $w$ 时，以{ $context(w)$ , $w$ }作为正样本，并从语料库库中选取k个非 $w$ 词语，与 $w$ 上下文构成{ $context(w)$ , $w_i$​​​​ }负样本，以正负样本集对模型进行训练。</p>
<p>替换后的概率公式（D=1或0 1表示该情况出现，0表示该情况不出现）</p>
<script type="math/tex; mode=display">
P(D=1|w_c,w_o) = \sigma(u_o^Tv_c)\tag{4}</script><p>其中 $\sigma$ 为sigmoid函数</p>
<script type="math/tex; mode=display">
\sigma(u_o^Tv_c) = sigmoid(u_o^Tv_c) = \frac{1}{1 + exp(-u_o^Tv_c)}\tag{5}</script><p>新的概率计算公式为,P(w)为随机负采样的概率分布</p>
<script type="math/tex; mode=display">
p(o|c) = p(D=1|w_c,w_o)\prod_{k=1,k\in P(w)}p(D=0|w_c,w_k)</script><p>推导可得</p>
<script type="math/tex; mode=display">
-log(p(o|c)) = -log[\sigma(u_o^Tv_c)]-\sum_{k=1,k\in P(w)}log(1- \sigma(u_k^Tv_c))</script><p>随之而来的问题是<strong>如何进行负采样</strong>才能保证训练是有效的？（trick）</p>
<blockquote>
<p> word2vec论文作者通过观察测试发现最佳采样概率分布是采用特殊的独立分布，概率公式为$p(w_i) = \frac{count(wi)^{\frac{3}{4}}}{\sum_{j= 0}^{total}w_j}$</p>
</blockquote>
<h5 id="层序softmax"><a href="#层序softmax" class="headerlink" title="层序softmax"></a>层序softmax</h5><p>首先按照单词在语料库中出现的频率构建haffman树，每个叶子节点为语料库中的词语，将softmax概率函数转化为每个二叉树节点的二分类逻辑回归（sigmoid）函数，我们的目标就是最大化从根节点到目标词语叶子节点路径的似然函数。（类似于负采样的概率函数）</p>
<img src="/2021/08/11/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811091807339.png" class="" title="image-20210811091807339">
<h4 id="词袋模型的概率公式"><a href="#词袋模型的概率公式" class="headerlink" title="词袋模型的概率公式"></a>词袋模型的概率公式</h4><p>已经知上下文，推导中心词，概率公式如下(窗口大小为2m)</p>
<script type="math/tex; mode=display">
p(w_c|w_{o1},......,w_{o2m}) = \frac{exp(\frac{1}{2m} u_c^T(v_{o1},......v_{om}))}{\sum_{i\in window}exp(\frac{1}{2m} u_i^T(v_{o1},......v_{om}))}</script><p>即背景词出现情况下中心词出现的条件概率（背景词向量取了平均值）</p>
<h3 id="词共现矩阵（co-occurence-matrix）"><a href="#词共现矩阵（co-occurence-matrix）" class="headerlink" title="词共现矩阵（co-occurence matrix）"></a>词共现矩阵（co-occurence matrix）</h3><p>首先限定句子窗口长度后，统计每个词在窗口长度内其他词出现的次数，记录在共现矩阵中，记录完成后以行作为每个词的特征向量</p>
<img src="/2021/08/11/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811094558672.png" class="" title="image-20210811094558672">
<p>虽然一定程度上解决了独热编码的过于系数，无法表示词语之间关系的问题，仍存在向量维度爆炸，且过于稀疏的问题，课程中提发了集中解决方案：</p>
<ol>
<li>使用SVD分解后的矩阵作为特征向量，以降低向量维度</li>
</ol>
<h3 id="Glove"><a href="#Glove" class="headerlink" title="Glove"></a>Glove</h3><p>Glove算法通过结合传统计数思路对word2vec模型进行了改进，词与词之间的关系不再通过神经网络式的预测模拟，而是通过拟合词与词之间共同出现的条件概率，实现词向量的构建。</p>
<img src="/2021/08/11/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811100512861.png" class="" title="image-20210811100512861">
<p>Glove为了实现计算出来向量满足线性计算，将 两个词语的词向量乘积 与  共现条件概率 对应起来</p>
<img src="/2021/08/11/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811104406982.png" class="" title="image-20210811104406982">
<p>通过拟合共现条件概率，实现词向量的学习</p>
<img src="/2021/08/11/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811104602858.png" class="" title="image-20210811104602858">
<p>动手学习机器学习中的讲解（另一个角度理解）</p>
<img src="/2021/08/11/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811104020990.png" class="" title="image-20210811104020990">
<h3 id="如何评估一组词向量"><a href="#如何评估一组词向量" class="headerlink" title="如何评估一组词向量"></a>如何评估一组词向量</h3><ol>
<li>通过后续任务（文本分类，问答系统）等的效果评估</li>
<li>词向量是否易于构建（维度低，效果好）</li>
<li>通过余弦相似度，衡量词语之间的相似度</li>
</ol>
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-10.上下文词嵌入</title>
    <url>/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/</url>
    <content><![CDATA[<h2 id="课程大纲"><a href="#课程大纲" class="headerlink" title="课程大纲"></a>课程大纲</h2><img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211013201952293.png" class="" title="image-20211013201952293">
<h2 id="预训练到来之前"><a href="#预训练到来之前" class="headerlink" title="预训练到来之前"></a>预训练到来之前</h2><h3 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h3><p>迄今为止，我们主要使用包括Word2Vec，Glove等词嵌入向量，通过大量的数据预训练，提供给下游任务作为单词输入</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211013202450741.png" class="" title="image-20211013202450741">
<p>之后我们遇到了 OOV(out of dic)和各种英语后缀不同导致单词含义不同得问题，传统预训练向量难以解决，引入了character-level的向量嵌入，如fastText。</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211013203324779.png" class="" title="image-20211013203324779">
<p>然而以往的词向量存在<strong>两个问题</strong></p>
<ul>
<li>以往的词向量嵌入<strong>不考虑上下文语境信息（或者说只考虑一种固定的语境）</strong>，仅仅是固定的一个词向量，应用于不同的下游任务</li>
<li>一个词不止有字面意思，还有包括词性，语法以及适用的语境等其他隐含信息，以往词向量只考虑了词的字面含义</li>
</ul>
<span id="more"></span>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211013204802812.png" class="" title="image-20211013204802812">
<h3 id="TagLM-“pre-EMLO”"><a href="#TagLM-“pre-EMLO”" class="headerlink" title="TagLM-“pre-EMLO”"></a>TagLM-“pre-EMLO”</h3><p>出发点：想要获得拥有上下文信息的词嵌入向量，但是又不想使用大量的标记数据</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211013211127036.png" class="" title="image-20211013211127036">
<p>TagLM模型使用大量无标签数据，训练一个嵌入模型和一个基于RNN的语言模型，用两个模型的输出作为当前词语的嵌入向量，输入到词性标注模型中进行监督训练。</p>
<ul>
<li><strong>嵌入模型</strong>由字符集嵌入和词嵌入两个模型共同生成两个嵌入向量拼接，输入第一层双向RNN</li>
<li><strong>语言模型</strong>，由大量的无标签数据预训练后，输入上下文嵌入后与RNN第一层隐藏层输出拼接输入第二层</li>
<li>预训练<strong>语言模型</strong>，在预训练完成后就<strong>冻结</strong></li>
</ul>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211013212332139.png" class="" title="image-20211013212332139">
<h3 id="EMLO"><a href="#EMLO" class="headerlink" title="EMLO"></a>EMLO</h3><p>ELMo 采用语言模型的思路，不输出固定的词向量表述，而是按照语言模型的方式根据上下文和当前词语，输入当前此与的向量表示。</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211013213914902.png" class="" title="image-20211013213914902">
<p>ELMO使用了多层堆叠的双向LM（LSTM），与传统只选取最后一层隐藏状态输出不同，ELMO认为不同层抽出不同的特征信息（隐藏状态），使用各个层的隐藏状态加权平均，作为输出。</p>
<ul>
<li>加权平均的不同层权重 $s^{task}_j$ 以及学习率 $\gamma^{task}$ 取值，由下游任务(task)决定</li>
</ul>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211014162907326.png" class="" title="image-20211014162907326">
<p>ELMo可以有不同的深度，提取不同深度的特征信息，适用于对应的问题</p>
<ul>
<li>浅层网络：Part-of-speech(词性标注)，syntactic dependencies（句法依存），NER（命名实体识别）</li>
<li>深层网络：sentiment（情感），Semantic role labeling(SRL 语义角色标注)，question andwering（阅读理解），SNLI（自然语言推理）</li>
</ul>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211014164651518.png" class="" title="image-20211014164651518">
<h3 id="ULMfit-基于迁移学习思路"><a href="#ULMfit-基于迁移学习思路" class="headerlink" title="ULMfit-基于迁移学习思路"></a>ULMfit-基于迁移学习思路</h3><img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211014182546795.png" class="" title="image-20211014182546795">
<p>ULMfit模型同样使用语言模型做词嵌入，共分为三步（文本情感分类为例）</p>
<ol>
<li>使用大量的领域数据对语言模型进行预训练</li>
<li>使用目标任务输入文本对训练好的语言模型进行微调（fine-tuning）</li>
<li>在模型的最后一层添加两个情感分类神经元</li>
</ol>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211014183233285.png" class="" title="image-20211014183233285">
<h2 id="预训练迁移模型-爆发"><a href="#预训练迁移模型-爆发" class="headerlink" title="预训练迁移模型 爆发"></a>预训练迁移模型 爆发</h2><img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211014184204582.png" class="" title="image-20211014184204582">
<h3 id="Transformers简介"><a href="#Transformers简介" class="headerlink" title="Transformers简介"></a>Transformers简介</h3><p>为了解决RNN无法并行计算的问题，Transformer结合之前模型的经验，实现了高度的并行性</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211014185419211.png" class="" title="image-20211014185419211">
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-11.Transformers</title>
    <url>/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/</url>
    <content><![CDATA[<h2 id="主要概念理解"><a href="#主要概念理解" class="headerlink" title="主要概念理解"></a>主要概念理解</h2><h3 id="self-Attention-自注意力机制"><a href="#self-Attention-自注意力机制" class="headerlink" title="self-Attention 自注意力机制"></a>self-Attention 自注意力机制</h3><blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/265108616">注意力机制介绍</a></p>
</blockquote>
<p>自注意力机制与注意力不同点在于，自注意力机制在序列内部进行注意力的计算，每个词与相邻词进行F(Q,K)计算，当前词作为Q，临近词作为K，计算主力注意力分数后，进行嵌入向量的加权平均。</p>
<ul>
<li>一个词语的含义取决于上下文临近词语的含义（语境），自注意力机制解决了RNN<strong>无法抽取长距离信息</strong>的问题</li>
<li>自注意力机制的计算可以通过矩阵运算实现并行化，解决了RNN<strong>无法并行优化</strong>的问题</li>
</ul>
<span id="more"></span>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211016162648608.png" class="" title="image-20211016162648608">
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017152513315.png" class="" title="image-20211017152513315">
<p><strong>self-attention过程简单总结</strong></p>
<ol>
<li>为每个输入词语随机初始化<strong>Query、Key、Value</strong>三个<strong>向量</strong>，其中，Value与注意力分数相乘做加权平均，输出结果</li>
<li>当需要计算某个词语表示时候，以当前词语的Query和相邻词Key（包括自己）用来计算注意力分数</li>
<li>使用上一步计算的注意力分数，输入softmax归一化，对相邻词的Value向量进行加权平均后输出当前词的向量表示</li>
</ol>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211016170802649.png" class="" title="image-20211016170802649">
<h3 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h3><p>降维思想，将原来的Q,K,V映射到多个不同得子空间(即一系列得$Q_i K_i V_i$ ,transformers中为八个)，这种映射通过矩阵乘法实现，每个一head维护一组$W_{iQ} W_{iK} W_{iV}$，通过单头中的$Q_i K_i V_i$ 与对应权重矩阵相乘得到对应得$Q_i K_i V_i$</p>
<ul>
<li>$QKV \in R^{1*512}$ </li>
<li>$W_{iQ}W_{iK}W_{iV} \in R^{512*64}$ </li>
<li>$Q*W_{iQ} = Q_i$ </li>
<li>可得 $Q_i K_i V_i \in R^{1*64}$，共计8个不同head，实现了从高维到低维得转换，但是没有增加参数数量。</li>
</ul>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017103444937.png" class="" title="image-20211017103444937">
<p>在不同得子空间上进行注意力机制得计算后，进行拼接乘以权重矩阵获得多头注意力机制的输出</p>
<p>​                        <script type="math/tex">MultiAttention(Q, K, V) = concat(head_1,......,head_2) * W_{out}</script>  其中 $W_{out} \in R^{512*512}$</p>
<p>相当于 输出结果得每个位置都是由所有参数加权平均得到</p>
<p><strong>为什么要使用多头注意力机制</strong></p>
<ul>
<li>通过将QKV映射到不同的子空间，实现了从不同角度对于当前词语含义的捕捉，而在不同的翻译任务中，我们对不同角度的重视程度不同（例如 <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> 举得it的例子），这种映射有助于模型自适应的捕捉特征，提升NMT的效果。</li>
<li>我觉得这就是个trick，到底有什么用也没人说清楚了（对黑盒算法强行解释），能用好用就完事了</li>
</ul>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017105229086.png" class="" title="image-20211017105229086">
<h2 id="网络结构理解"><a href="#网络结构理解" class="headerlink" title="网络结构理解"></a>网络结构理解</h2><img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017151555753.png" class="" title="image-20211017151555753">
<h3 id="Postion-Encoding-位置编码"><a href="#Postion-Encoding-位置编码" class="headerlink" title="Postion-Encoding 位置编码"></a>Postion-Encoding 位置编码</h3><p>transformer在对输入词语进行embedding后，由于transformer舍弃了RNN的时间序列输入形式，整个序列中不同位置的单词在输入时的地位是相同的，<strong>通过位置编码重赋予输入位置特征</strong>。</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017110556688.png" class="" title="image-20211017110556688">
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017110713718.png" class="" title="image-20211017110713718">
<p>位置编码存在两种计算方式</p>
<ol>
<li><p>通过固定的公式，根据字符在序列中的位置，计算得到 (<a href="https://www.zhihu.com/question/347678607/answer/864217252">公式理解</a>),总结就是又要体现不同位置的差异性，又不能影响embedding的词语含义。</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017111718315.png" class="" title="image-20211017111718315">
</li>
<li><p>随机初始化pos编码，通过模型学习生成对应编码</p>
</li>
</ol>
<h3 id="LayerNormalization层"><a href="#LayerNormalization层" class="headerlink" title="LayerNormalization层"></a>LayerNormalization层</h3><p>LN方法与BN方法类似，在一条数据不同特征之间进行归一化处理（BN对不同数据的同一特征归一化处理），由于RNN类似于一个不固定batchsize(时间步)的CNN网络，无法使用BN方法，采用LN方法一定程度上也可以实现BN方法的效果。</p>
<h3 id="Feed-Forward（FFN）层"><a href="#Feed-Forward（FFN）层" class="headerlink" title="Feed Forward（FFN）层"></a>Feed Forward（FFN）层</h3><p>类似于两个卷积核大小为1的卷积操作，目的是为了应用非线性激活函数RELU，增加<strong>非线性</strong>性质。</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017150618854.png" class="" title="image-20211017150618854">
<h3 id="Decoder端"><a href="#Decoder端" class="headerlink" title="Decoder端"></a>Decoder端</h3><p>与encoder段基本一致，每个decoder块每部增加了一个 “encoder-decoder attention” 注意力机制，以encoder输出的K，V矩阵为输入，和Encoder当前词的Q进行注意力机制运算。</p>
<p><img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif" alt="img"></p>
<p>基本流程如图</p>
<p><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif" alt="img"></p>
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-12.自然语言生成</title>
    <url>/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/</url>
    <content><![CDATA[<h3 id="内容回顾"><a href="#内容回顾" class="headerlink" title="内容回顾"></a>内容回顾</h3><h4 id="Beam-Search：不同K的区别"><a href="#Beam-Search：不同K的区别" class="headerlink" title="Beam Search：不同K的区别"></a>Beam Search：不同K的区别</h4><ol>
<li>小K值可能导致生成序列效果较差（语法上、语义上、流畅度上）</li>
<li>大K值虽然能够解决以上问题，但是会带来更多的计算成本，一定程度上减低NMT任务中的BLUE分数<ul>
<li>开放式问答任务中，更大的K可能导致 回答更加的宽泛（与原问题的关联度更低）</li>
</ul>
</li>
</ol>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019145744076.png" class="" title="image-20211019145744076">
<p><strong>其他的解决方案</strong></p>
<ol>
<li><strong>Sampling-based decoding</strong>（与Beam search不同在于decoder每一步只需要追踪一个词）<ul>
<li>Pure Sampling：每次随机选择概率分布中的某一个词，作为decoder输出词</li>
<li>Top-n Sampling：每次从前N个概率大小词语中选择某一个词，作为decoder输出词</li>
</ul>
</li>
</ol>
<h4 id="Softmax-temperature-带温度系数的Softmax方法"><a href="#Softmax-temperature-带温度系数的Softmax方法" class="headerlink" title="Softmax temperature: 带温度系数的Softmax方法"></a>Softmax temperature: 带温度系数的Softmax方法</h4><blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/132785733">$\tau$解释</a></p>
</blockquote>
<p>在原始的Softmax函数中添加 <strong>temperature hyperparameter: $\tau$</strong> .</p>
<ol>
<li>$\tau$ 起到一种平滑作用， $\tau$ 越大softmax计算得到的概率分布越平滑，t越小分布越不均匀</li>
<li>在训练开始将 $\tau$ 值设置较大，概率分布较为平滑，loss较大可以避免模型落入局部最优解，随着训练的进行，不断增大 $\tau$ 值，从而提升模型的效果。（某一个 $\tau$ 值并不影响模型的结果）</li>
</ol>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019151158168.png" class="" title="image-20211019151158168">
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019151618394.png" class="" title="image-20211019151618394">
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019151751505.png" class="" title="image-20211019151751505">
<span id="more"></span>
<h3 id="NLG"><a href="#NLG" class="headerlink" title="NLG"></a>NLG</h3><p>NLG主要解决的问题就是，给定输入文本，对文本进行分析和抽取，输出我们需要的指定文本信息（summary或者再创作）</p>
<ol>
<li>输入可以是单个文档，也可以是多个文档（多个文档通常内容是相关的）</li>
</ol>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019152628788.png" class="" title="image-20211019152628788">
<p>目前NLG任务领域的主要数据集有</p>
<ol>
<li><p>single-document任务</p>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019153334719.png" class="" title="image-20211019153334719">
</li>
<li><p>句子简化（sentence simplification）</p>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019153513309.png" class="" title="image-20211019153513309">
</li>
</ol>
<p>NLG任务的<strong>两种</strong>主要实现方式</p>
<ol>
<li>抽取性（extractive）摘要</li>
<li>生成式（abstractive）摘要</li>
</ol>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019153742775.png" class="" title="image-20211019153742775">
<h4 id="神经网络之前的NLG"><a href="#神经网络之前的NLG" class="headerlink" title="神经网络之前的NLG"></a>神经网络之前的NLG</h4><p>一般的单文本摘要流程如下</p>
<ol>
<li>首先从原文中选取特定的句子，作为摘要的句子组成内容（content selecting）</li>
<li>对选取的句子进行排序（Information Ordering）</li>
<li>对句子进行调整和修改（Sentence realization）</li>
</ol>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019154611084.png" class="" title="image-20211019154611084">
<p>重点在于<strong>如何选择句子</strong></p>
<ol>
<li>基于算法对句子评分，选取评分高的句子作为候选摘要内容<ul>
<li>根据关键词出现频率，如tf-idf</li>
<li>句子在文章中出现的位置</li>
</ul>
</li>
<li>基于图算法（句子视为节点，句子之间的相似度视为边）</li>
</ol>
<h4 id="摘要评价-ROUGE"><a href="#摘要评价-ROUGE" class="headerlink" title="摘要评价-ROUGE"></a>摘要评价-ROUGE</h4><img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019164507272.png" class="" title="image-20211019164507272">
<p>ROUGE（Recall-Oriented Understudy for Gisting Evaluation）与BLUE类似，均基于n-gram共现评估生成文本与目标摘要文本的差距</p>
<ol>
<li>ROUGE在计算的过程中引入了召回率的概念，相较于NMT任务，摘要任务对召回率的重视程度更高</li>
</ol>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019164852685.png" class="" title="image-20211019164852685">
<p>一种ROUGE评分只针对一种n-gram语法</p>
<ol>
<li>ROUGE-1: unigram overlap</li>
<li>ROUGE-2: bigram overlap</li>
<li>ROUGE-L: LCS overlap</li>
</ol>
<h4 id="Neural-Summarization-（2015-）发展历史"><a href="#Neural-Summarization-（2015-）发展历史" class="headerlink" title="Neural Summarization （2015-）发展历史"></a>Neural Summarization （2015-）发展历史</h4><ol>
<li><p>第一次使用seq2seq + attention实现单文档摘要</p>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019170212035.png" class="" title="image-20211019170212035">
</li>
<li><p>2015后出现的一系列解决方案</p>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211020112720437.png" class="" title="image-20211020112720437">
</li>
</ol>
<h4 id="Copy-Mechanisms：解决seq2seq局限于特征"><a href="#Copy-Mechanisms：解决seq2seq局限于特征" class="headerlink" title="Copy Mechanisms：解决seq2seq局限于特征"></a>Copy Mechanisms：解决seq2seq局限于特征</h4><p>由于seq2seq机制中decoder基于encoder输出特征进行生成，无法满足<strong>保留源语句中某些关键词</strong>的要求（虽然fluent，但不够imformative），通过Copy Mechanisms解决这个问题。</p>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211020113937063.png" class="" title="image-20211020113937063">
<p>Copy Mechanisms 通过attention机制，将input中的某些词或者某些句子，<strong>直接作为output中的内容输出</strong>，通过与seq2seq结合，更好的适应文档摘要问题</p>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211020114155828.png" class="" title="image-20211020114155828">
<p>几个复制机制的实现</p>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211020114245710.png" class="" title="image-20211020114245710">
<p>存在的问题（单词级别的复制机制）</p>
<ol>
<li>复制的度难以把握，容易退化成抽取算法</li>
<li>复制时难以考虑整篇文章的信息，当文章过长时，复制效果不理想</li>
<li>如何选择，没有一个确定的策略</li>
</ol>
<p><strong>Bottom up summarization</strong></p>
<p>解决单词级别复制问题，将摘要过程分为两个阶段</p>
<ol>
<li>内容选择阶段: 使用标注模型，对原文章中的词进行标记是否需要</li>
<li>摘要阶段：同样的seq2seq+attention摘要模型，只是对上一步标记的不需要词语进行掩码覆盖，不参与预测</li>
</ol>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211023091146567.png" class="" title="image-20211023091146567">
<h3 id="Dialogue-对话生成任务"><a href="#Dialogue-对话生成任务" class="headerlink" title="Dialogue 对话生成任务"></a>Dialogue 对话生成任务</h3><p>对话生成任务的范围较为广泛，从日常对话、智能客服到辩论等，主要包括任务型对话和社会对话两种</p>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211023094731197.png" class="" title="image-20211023094731197">
<h4 id="pre-neural"><a href="#pre-neural" class="headerlink" title="pre-neural"></a>pre-neural</h4><p>由于开放式文档问题难度较高，神经网络应用之前的系统往往使用模板式回答或者从一系列语料中寻找回答的方式，实现对话。</p>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211023095226942.png" class="" title="image-20211023095226942">
<h4 id="seq2seq-based-dialogue"><a href="#seq2seq-based-dialogue" class="headerlink" title="seq2seq-based dialogue"></a>seq2seq-based dialogue</h4><p>虽然很会人们就发现seq2seq模型可以应用于解决对话生成任务，但是目前随着应用的开展，发现存在一系列的问题</p>
<ol>
<li>回答宽泛（虽然给了答案，但是答案没什么用） Genericness</li>
<li>回答与上下文不相关 Irrelevant</li>
<li>不断地重复 Repetition</li>
<li>缺乏上下文信息 （对话的上下文，对话的人等）</li>
</ol>
<p><strong>解决irrelevant问题</strong></p>
<ol>
<li>生成的一般回答，而不是想要的回答 （i don’t know）</li>
<li>回答的主题与问题不相关</li>
</ol>
<p>传统NN模型的优化目标往往是最大化  $p(Target|Source)$ ，即给定目前问题下得到对应答案的条件概率；当前解决方案采用互信息MMI作为优化目标\</p>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211023101005268.png" class="" title="image-20211023101005268">
<p>在公式上体现为原条件概率减去了当前回答出现的概率（形式上理解为减去回答概率，对在预料中出现频率较高的通用回答进行了惩罚）</p>
<p><strong>解决genericness问题</strong></p>
<img src="/2021/11/21/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211023101625189.png" class="" title="image-20211023101625189">
<p><strong>解决repetition问题</strong></p>
<p>简单解决方案: 对于decoder输出的n-gram,禁止n-gram句子中存在相同词语。（简单而粗暴）</p>
<p>复杂方案</p>
<ol>
<li>避免注意力机制计算时重复的计算相同背景词（从源头铲除相同词语）</li>
<li>设置训练目标，减少重复出现</li>
</ol>
<h3 id="NLG-Evaluation"><a href="#NLG-Evaluation" class="headerlink" title="NLG Evaluation"></a>NLG Evaluation</h3><p>传统基于词覆盖的评价方式，例如BLUE、ROGUE等，并不适用于NLG任务（或者说无法真正反映一个NLG解决方案的好坏），既然无法找到一种衡量NLG任务好坏的方式，我们不如聚焦于生成结果的某一个方面</p>
<ul>
<li>流利性</li>
<li>正确的风格</li>
<li>多样性</li>
<li>相关输入</li>
<li>简单的长度和重复</li>
<li>特定于任务的指标，如摘要的压缩率</li>
</ul>
<p>人类评估（省略）</p>
<h3 id="NLG发展方向"><a href="#NLG发展方向" class="headerlink" title="NLG发展方向"></a>NLG发展方向</h3><ul>
<li>将离散潜在变量纳入NLG（不同任务可能有不同的潜在特征）<ul>
<li>可以帮助在真正需要它的任务中建模结构，例如讲故事，任务导向对话等</li>
</ul>
</li>
<li>严格的从左到右生成的替代方案<ul>
<li>并行生成，迭代细化，自上而下生成较长的文本</li>
</ul>
</li>
<li>替代teacher forcing的模型训练方法<ul>
<li>更全面的句子级别的目标函数（而不是单词级别）</li>
</ul>
</li>
</ul>
<p>目前NLG发展</p>
<ul>
<li>在NLP+深度学习的早期，社区主要将成功的机器翻译方法转移到NLG任务中。</li>
<li>现在，越来越多的创新NLG技术出现，针对非NMT生成环境。</li>
<li>越来越多（神经）NLG研讨会和竞赛，特别关注开放式NLG<ul>
<li>NeuralGen workshop</li>
<li>Storytelling workshop</li>
<li>Alexa challenge</li>
<li>ConvAI2 NeurIPS challenge</li>
</ul>
</li>
<li>这些对于组织社区提高再现性、标准化评估特别有用</li>
<li>最大障碍是评估</li>
</ul>
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-2.使用神经网络解决nlp问题</title>
    <url>/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h3 id="神经网络介绍"><a href="#神经网络介绍" class="headerlink" title="神经网络介绍"></a>神经网络介绍</h3><p>神经网络介绍部分省略</p>
<h3 id="命名实体识别（NER）"><a href="#命名实体识别（NER）" class="headerlink" title="命名实体识别（NER）"></a>命名实体识别（NER）</h3><p>标注中句子中的目标词性词语-实体，其中实体是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/image-20210831093150777.png" class="" title="image-20210831093150777">
<p>命名实体存在的困难：</p>
<ol>
<li>难以确定实体的上下文边界（王小明 还是 小明）</li>
<li>难以确定词语是否为 实体</li>
<li>实体的具体含义依赖于上下文，难以确定</li>
<li>难以辨别不知道的实体（特定语境 特定环境下的某些词作为实体）</li>
</ol>
<span id="more"></span>
<h4 id="1-Binary-word-window-classification"><a href="#1-Binary-word-window-classification" class="headerlink" title="1.Binary word window classification"></a>1.Binary word window classification</h4><p>由于词语根据语境等具有不同的含义和词性，单独识别句子中的某个单词是不现实的，使用一个上下文窗口对词语进行分类</p>
<p><strong>思路：</strong> 使用上下文窗口中的背景词对当前词进行分类</p>
<ul>
<li><p>对窗口内词向量进行平均，作为分类输入。<strong>问题：</strong>忽略了位置信息和词语之间的关系</p>
</li>
<li><p>使用窗口内词向量拼接向量作为输入</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/image-20210831095343422.png" class="" title="image-20210831095343422">
</li>
</ul>
<p>训练分类函数</p>
<ol>
<li><p>softmax 或者 交叉熵函数，通过反向传播更新词向量和权重</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/image-20210831095949882.png" class="" title="image-20210831095949882">
</li>
<li><p>unnormarlized score</p>
<ul>
<li><p>选取一个正例窗口和一个反例窗口</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/image-20210831100250180.png" class="" title="image-20210831100250180">
</li>
<li><p>将拼接的窗口向量输入到具有一个隐藏层和一个全连接输出层的神经网络，输出一个评价分数（score)</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/image-20210831100708874.png" class="" title="image-20210831100708874">
<ul>
<li><strong>添加隐藏层的目的</strong>在于通过隐藏层的非线性激活函数，学习输入与输出之间的非线性关系（non-linear interaction）</li>
</ul>
</li>
<li><p>训练目标为：正例窗口分数最大，负例窗口分数最小</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/image-20210901090231216.png" class="" title="image-20210901090231216">
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/image-20210901090217098.png" class="" title="image-20210901090217098">
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-3.依存分析问题</title>
    <url>/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h4 id="句法分析"><a href="#句法分析" class="headerlink" title="句法分析"></a>句法分析</h4><p>句法分析与上下文无关文法相对立，强调通过对于句子语法结构的分析，实现对于句子的理解。最常见的三种句法分析任务如下</p>
<ol>
<li>句法结构分析 识别句子中的短语结构和层次关系</li>
<li>依存关系分析 识别句子中词与词之间的依存关系，确定词语的含义</li>
<li>深层文法句法分析 利用深层文法对句子进行分析</li>
</ol>
<h4 id="依存句法分析（Dependency-Parsing）"><a href="#依存句法分析（Dependency-Parsing）" class="headerlink" title="依存句法分析（Dependency Parsing）"></a>依存句法分析（Dependency Parsing）</h4><p>依存结构展示了句子中依赖于其他词语的单词，这种依赖体现为被修饰或者被限定</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909082635347.png" class="" title="image-20210909082635347">
<span id="more"></span>
<p>同一个句子，在不同的依存关系下，理解的含义可能有所不同</p>
<ol>
<li>介词修饰歧义（preposition attachment ambiguity）(刀杀死了男人，还是杀死了带刀的男人）</li>
</ol>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909083302896.png" class="" title="image-20210909083302896">
<ol>
<li><p>修饰范围歧义（Coordination scope ambiguity）</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909084806696.png" class="" title="image-20210909084806696">
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909084821340.png" class="" title="image-20210909084821340">
</li>
<li><p>形容词修饰歧义</p>
</li>
</ol>
<h4 id="依存句法（Dependency-Grammar）"><a href="#依存句法（Dependency-Grammar）" class="headerlink" title="依存句法（Dependency Grammar）"></a>依存句法（Dependency Grammar）</h4><p>依存句法假设句子中的词语存在语法结构上的关联，这种通常为非对称的二元关联关系成为依赖（Dependency）</p>
<ol>
<li>依存语法树</li>
</ol>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909090638171.png" class="" title="image-20210909090638171">
<ol>
<li><p>直接在句子上标注</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909092703216.png" class="" title="image-20210909092703216">
</li>
</ol>
<p><strong>依赖关系的一般约束</strong></p>
<ol>
<li>依赖不循环</li>
<li>相依赖的词语一般距离较近</li>
<li>依赖项一般能够构成树形结构</li>
</ol>
<p><strong>依存句法分析的基本方法</strong></p>
<ol>
<li>Dynamic programming</li>
<li>Graph algorithms</li>
<li>Constraint Satisfaction</li>
<li>“Transition-based parsing” or “deterministic dependency parsing”</li>
</ol>
<h4 id="Greedy-transition-based-parsing-基于贪婪思想转换的依存分析"><a href="#Greedy-transition-based-parsing-基于贪婪思想转换的依存分析" class="headerlink" title="Greedy transition-based parsing 基于贪婪思想转换的依存分析"></a>Greedy transition-based parsing 基于贪婪思想转换的依存分析</h4><img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909103541425.png" class="" title="image-20210909103541425">
<p>简单理解就是 从句子的开头逐个单词进行压栈，判断即将入栈元素和栈顶元素的依存关系，根据依存关系在边集合<script type="math/tex">arcs A</script> 添加对应方向的边，弹出栈顶元素（reduce）(如果不存在关系，不弹)，并将当前元素压栈（shift）</p>
<p><strong>Arc-Standard transition-based parser</strong></p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909104413757.png" class="" title="image-20210909104413757">
<p><strong>MaltParser</strong></p>
<p>引入机器学习分类器，通过分类器判断添加的依赖，以及依赖的方向。避免了搜索，提供了一种线性的解析方式</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909104740587.png" class="" title="image-20210909104740587">
<h4 id="如何衡量依存分析的效果？"><a href="#如何衡量依存分析的效果？" class="headerlink" title="如何衡量依存分析的效果？"></a>如何衡量依存分析的效果？</h4><p><strong>Dependency Accurracy</strong></p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909105300077.png" class="" title="image-20210909105300077">
<h4 id="基于神经网络的依存分析器构建"><a href="#基于神经网络的依存分析器构建" class="headerlink" title="基于神经网络的依存分析器构建"></a>基于神经网络的依存分析器构建</h4><img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909110120011.png" class="" title="image-20210909110120011">
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909111052510.png" class="" title="image-20210909111052510">
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-4.语言模型与RNN引入</title>
    <url>/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-4.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8ERNN%E5%BC%95%E5%85%A5/</url>
    <content><![CDATA[<h4 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h4><p>通过语言模型的构建，实现能够根据已知序列推断序列中的下一个单词，如搜索引擎中的搜索推断等。</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-4.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8ERNN%E5%BC%95%E5%85%A5/image-20210911095229424.png" class="" title="image-20210911095229424">
<p>模型的形式化定义如下，给定一系列单词，预测下一个单词的概率分布，该概率分布为当前模型词典库上词语的概率分布</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-4.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8ERNN%E5%BC%95%E5%85%A5/image-20210911095436865.png" class="" title="image-20210911095436865">
<span id="more"></span>
<p>条件概率角度理解语言模型形式如下，即一系列条件概率的连乘</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-4.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8ERNN%E5%BC%95%E5%85%A5/image-20210911095643154.png" class="" title="image-20210911095643154">
<h4 id="如何构建一个语言模型（N-grams模型）"><a href="#如何构建一个语言模型（N-grams模型）" class="headerlink" title="如何构建一个语言模型（N-grams模型）"></a>如何构建一个语言模型（N-grams模型）</h4><blockquote>
<p><strong><a href="https://baike.baidu.com/item/马尔可夫/2774684">马尔可夫</a></strong>（Markov）假设：下一个词的出现仅仅依赖于前面的几个词，而不是整个前部分句子（简化概率计算，减少模型参数）</p>
<p><strong>大数定理</strong>：在试验不变的条件下，重复试验多次，随机事件的频率近似于它的概率。偶然中包含着某种必然</p>
</blockquote>
<p>N-grams模型基于马尔可夫假设，指给定的一段文本或语音中N个项目（item）的序列，项目可以是音节、词语或者碱基对等，N-gram模型中的N，即对应概率依赖的词语个数，例如：</p>
<ul>
<li><p>当N = 1时，1-gram模型（unigram）：$p(T) = p(W1)p(W2)…..p(Wn)$ 即词语的出现相互独立</p>
</li>
<li><p>当N = 2 时，2-gram模型（bigram）：$p(T) = p(W1)p(W2|W1)…..p(Wn|Wn-1)$ 即依赖前一个词语</p>
</li>
</ul>
<p>以N-gram模型为例子，根据大数定理可得，在语料库足够大的条件下，$W_n$关于$W_1…….W_{n-1}$的条件概率为</p>
<script type="math/tex; mode=display">
p(Wi|Wi-1) = \frac{count(W_1,W_2,.......W_{N-1},W_n)}{count(W_1,W_2,.......W_{N-1})}</script><p>即语料种$W_1^n$语句的出现次数除以$W_i^{n-1}$语句的次数</p>
<h5 id="单词稀疏问题（Sparsity-Problem）"><a href="#单词稀疏问题（Sparsity-Problem）" class="headerlink" title="单词稀疏问题（Sparsity Problem）"></a>单词稀疏问题（Sparsity Problem）</h5><p>根据N-grams的计算公式，如果语料库中不存在（或几乎没有）当前上文与目标推断单词共同出现的情况，则概率公式的分母为0，即<strong>稀疏问题</strong>，</p>
<p>解决方式为通过在概率公式中增加一个增量，使词库中的所有单词在给定上文预测的情况下，都具有一定的概率值（即使是不可能的单词），称之为<strong>平滑法</strong>。随着n元语法中的n不断增大，单词稀疏问题会更加严重</p>
<ul>
<li>Add‐One 平滑</li>
<li>Add‐K 平滑</li>
<li>等</li>
</ul>
<h5 id="固定窗口神经网络语言模型"><a href="#固定窗口神经网络语言模型" class="headerlink" title="固定窗口神经网络语言模型"></a>固定窗口神经网络语言模型</h5><p>固定输入窗口大小，将输入窗口内的词语向量拼接，作为模型输入，预测下一个单词的概率，解决了非神经网络语言模型存在的单词稀疏和存储稀疏问题，但是仍存在</p>
<ol>
<li>固定窗口有些时候可能太小</li>
<li>扩大窗口会导致模型参数量爆炸</li>
<li>窗口内不同位置单词对应不同的权重值（？）</li>
</ol>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-4.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8ERNN%E5%BC%95%E5%85%A5/image-20210911103426907.png" class="" title="image-20210911103426907">
<h5 id="基于RNN的语言模型"><a href="#基于RNN的语言模型" class="headerlink" title="基于RNN的语言模型"></a>基于RNN的语言模型</h5><img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-4.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8ERNN%E5%BC%95%E5%85%A5/image-20210911103854491.png" class="" title="image-20210911103854491">
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-5.RNN主要问题与改进</title>
    <url>/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/</url>
    <content><![CDATA[<h3 id="RNN梯度消失-爆炸问题（vanishing-exploding-gradient-problem）"><a href="#RNN梯度消失-爆炸问题（vanishing-exploding-gradient-problem）" class="headerlink" title="RNN梯度消失/爆炸问题（vanishing/exploding gradient problem）"></a>RNN梯度消失/爆炸问题（vanishing/exploding gradient problem）</h3><p>由于RNN对于不同时间步的输入使用同一个神经元，在反向传播计算梯度时，随着时间步的增加，会出现梯度爆炸和消失问题</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914151209756.png" class="" title="image-20210914151209756">
<span id="more"></span>
<p>RNN时间步t计算公式为</p>
<script type="math/tex; mode=display">
S_t = \sigma(S_{t-1}W_s + X_tW_x+b_t) \\
O_t = W_oS_t+b_o</script><p>反向传播的损失函数求导结果为</p>
<script type="math/tex; mode=display">
\frac{\partial L_t}{\partial W_x} = \sum_{k=0}^t \frac{\partial L_t}{\partial O_x}\frac{\partial O_t}{\partial S_t}(\prod_j^t \frac{\partial S_j}{\partial S_{j-1}})\frac{\partial O_k}{\partial W_x}</script><p>其中 $\frac{\partial s_j}{\partial S_{j-1}} = \sigma’ W_s$ (不考虑激活函数的情况下),由于累乘的性质，随着神经元系数W的累乘次数增加，如果$w$大于1，产生梯度爆炸问题；如果$w$小于，出现梯度消失问题。</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914152544786.png" class="" title="image-20210914152544786">
<p>梯度消失问题导致 <strong>RNN难以捕捉长时间步距离中的有效信息</strong></p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914153650599.png" class="" title="image-20210914153650599">
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914154543524.png" class="" title="image-20210914154543524">
<h4 id="解决梯度爆炸问题"><a href="#解决梯度爆炸问题" class="headerlink" title="解决梯度爆炸问题"></a>解决梯度爆炸问题</h4><p>梯度剪切（clipping）</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914154902059.png" class="" title="image-20210914154902059">
<h3 id="Long-Short-term-Memory-LSTM"><a href="#Long-Short-term-Memory-LSTM" class="headerlink" title="Long Short-term Memory(LSTM)"></a>Long Short-term Memory(LSTM)</h3><p>在原RNN每个时间步拥有一个隐藏状态 $h^{(t)}$ 的基础上，增加一个存储单元 $c^{(t)}$（cell state）存储RNN无法捕捉的长距离时间步信息</p>
<ul>
<li>LSTM能够从存储单元中删除、写入以及读取相关信息，是否执行这些操作由对应的控制门（gate）决定</li>
<li>每个时间步gate值由当前时间步信息动态计算</li>
</ul>
<p>三种类型门的值均采用神经网络方式预测得到</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914161611434.png" class="" title="image-20210914161611434">
<p><strong>基本计算流程：</strong></p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914162031478.png" class="" title="image-20210914162031478">
<ol>
<li>首先根据上一时间步隐藏状态和当前输入，计算当前存储单元候选值</li>
<li>根据<strong>遗忘门和输入门</strong>有选择的组合历史/当前存储单元计算值，获得当前存储单元值（忘记多少从前，留下多少现在）</li>
<li>根据<strong>输出门</strong>，决定隐藏状态值</li>
</ol>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914162308396.png" class="" title="image-20210914162308396">
<h4 id="为何LSTM能够解决梯度消失问题"><a href="#为何LSTM能够解决梯度消失问题" class="headerlink" title="为何LSTM能够解决梯度消失问题"></a>为何LSTM能够解决梯度消失问题</h4><p>关注点不在于解决梯度消失或者梯度爆炸，在于解决由梯度消失带来的 <strong>长距离信息无法捕捉的问题</strong>（不解决因，而改善果），类似于resnet中的残差思路</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914163439128.png" class="" title="image-20210914163439128">
<h3 id="Gated-Recurrent-Units-GRU-降低了LSTM的复杂度"><a href="#Gated-Recurrent-Units-GRU-降低了LSTM的复杂度" class="headerlink" title="Gated Recurrent Units(GRU) - 降低了LSTM的复杂度"></a>Gated Recurrent Units(GRU) - 降低了LSTM的复杂度</h3><p>不使用cell存储单元，只采用们的思想控制历史隐藏状态以及当前隐藏状态对于结果的影响。、</p>
<ol>
<li>$u^t$ 更新门：控制当前生成当前隐藏状态时保留的历史状态和当前隐藏状态的比例</li>
<li>$r^t$ 重置门：历史隐藏状态信息 流入 当前候选隐藏状态的量</li>
</ol>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210915191812848.png" class="" title="image-20210915191812848">
<p>与LTSM应用效果区别不大，如果问题特别重视长距离信息的保存，默认使用LSTM</p>
<h3 id="其他类型神经网络中同样存在梯度消失爆炸问题"><a href="#其他类型神经网络中同样存在梯度消失爆炸问题" class="headerlink" title="其他类型神经网络中同样存在梯度消失爆炸问题"></a>其他类型神经网络中同样存在梯度消失爆炸问题</h3><p>随着网络层数的不断增加，梯度消失/爆炸更容易出现，导致深层神经网络的训练难度更高，出现了几种特殊类型网络，解决此类问题</p>
<ol>
<li><p>resnet 残差网络</p>
<blockquote>
<p><a href="https://www.cnblogs.com/shine-lee/p/12363488.html">相比于让$F(x)$学习成恒等映射，让$F(x)$学习成0要更加容易——后者通过L2正则就可以轻松实现</a></p>
</blockquote>
<p>残差网络采用 shortcut-connection的思想，将几个layer成为block，假设每个block拟合函数为$F(x)$ ,目标函数为$H(x)$。传统网络思想通过不断训练用拟合函数$F(x)$映射目标函数$H(x)$，resnet将拟合目标修改为$H(x)-x$ ,在输出增加一个<strong>shortcut connection</strong>，直接将$x$ 加到拟合结果上，实现与传统网络相同的效果。</p>
<p>也正是因为<strong>shortcut connection</strong>的存在，实现了长距离信息的保存</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210915194728672.png" class="" title="image-20210915194728672">
</li>
<li><p>densenet（<strong>待详细了解</strong>）</p>
<p>每一层的输入都包括前面所有层的输出</p>
</li>
</ol>
<h3 id="双向RNN（bidirectional）"><a href="#双向RNN（bidirectional）" class="headerlink" title="双向RNN（bidirectional）"></a>双向RNN（bidirectional）</h3><p>为了解决单向RNN存在的句子歧义问题（I am terribly exciting 是难受 还是特别兴奋？），通过两个独立的RNN网络，分别正/逆输入句子，将两个网络相同词语获得的隐藏状态拼接，形成新的隐藏状态，输入到输入层获得输入结果。</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210915195839882.png" class="" title="image-20210915195839882">
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210915200357516.png" class="" title="image-20210915200357516">
<h5 id="双向RNN的局限性"><a href="#双向RNN的局限性" class="headerlink" title="双向RNN的局限性"></a>双向RNN的局限性</h5><p>双向RNN只有在输入数据完整句子的情况下才能够使用，LM模型不适用。</p>
<h3 id="多层RNN（multi-layer）"><a href="#多层RNN（multi-layer）" class="headerlink" title="多层RNN（multi-layer）"></a>多层RNN（multi-layer）</h3><p>多个RNN单元堆积在一起组成深度网络，类似于卷积网络的思想，通过增加网络的深度，实现从浅层特征学习=&gt;深层特征学习。</p>
<img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210915200931271.png" class="" title="image-20210915200931271">
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><img src="/2021/09/15/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210915201518821.png" class="" title="image-20210915201518821">
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-6.机器翻译</title>
    <url>/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/</url>
    <content><![CDATA[<h3 id="早期机器翻译"><a href="#早期机器翻译" class="headerlink" title="早期机器翻译"></a>早期机器翻译</h3><img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923165912958.png" class="" title="image-20210923165912958">
<p>采用 单词对应词典的形式，存储在磁带上，翻译时通过查字典的形式，找到对应词组合成句子。</p>
<h3 id="基于统计的机器翻译（Statistical-Machine-Translation-SMT）"><a href="#基于统计的机器翻译（Statistical-Machine-Translation-SMT）" class="headerlink" title="基于统计的机器翻译（Statistical Machine Translation SMT）"></a>基于统计的机器翻译（Statistical Machine Translation SMT）</h3><img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923165944670.png" class="" title="image-20210923165944670">
<p>从概率的角度解决机器翻译问题， 首先语料中学习概率模型（不同语言单词之间的概率和语言内的语言模型），通过建立的概率模型实现翻译功能。</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923170157170.png" class="" title="image-20210923170157170">
<span id="more"></span>
<h3 id="基于神经网络的机器翻译（Nerual-Machine-Translation）2014"><a href="#基于神经网络的机器翻译（Nerual-Machine-Translation）2014" class="headerlink" title="基于神经网络的机器翻译（Nerual Machine Translation）2014"></a>基于神经网络的机器翻译（Nerual Machine Translation）2014</h3><p>采用了一种叫做seq2seq（sequence to sequence）的模型,使用两个RNN单元（可以是任意类型的RNN）实现机器翻译问题</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923171554655.png" class="" title="image-20210923171554655">
<ol>
<li>Eecoder RNN：输入源语言句子，对该句子进行编码</li>
<li>Decoder RNN：输入编码器生成的编码（隐藏状态）和开始符，不断地生成预测单词，最后组合成为翻译句子</li>
</ol>
<p>几个主要的seq2seq类型任务</p>
<ol>
<li>总结摘要（原文章 到 摘要/总结）</li>
<li>对话 （前一句话 到 后一句话）</li>
<li>代码生成等</li>
</ol>
<p>seq2seq模型是一种<strong>条件语言</strong>模型</p>
<ol>
<li>seq2seq的作用仍然是给定上文推断出下一个词语，所以是语言模型</li>
<li><strong>条件</strong>体现在这种推断是基于源语言段落输入（在源语言的条件下）</li>
</ol>
<p>seq2seq相较于SMT更加优越是在于</p>
<ol>
<li><p>seq2seq模型直接模拟 目标语言在源语言条件下的语言模型</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923190656093.png" class="" title="image-20210923190656093">
</li>
<li><p>SMT 将这种模拟拆成两个子问题，源语言到目标语言的概率 和  目标语言的语言模型</p>
</li>
</ol>
<h3 id="如何训练seq2seq模型"><a href="#如何训练seq2seq模型" class="headerlink" title="如何训练seq2seq模型"></a>如何训练seq2seq模型</h3><p>输入 平行语料（源语言-目标语言的句子对），分别作为Encoder和Decoder的输入，Decoder每个时间步的预测输出与目标结果单独计算loss函数，最后以所有时间步的平均loss作为优化目标loss，反向传播实现训练。</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923191307147.png" class="" title="image-20210923191307147">
<ul>
<li>与预测不同，Decoder的每个时间步的输出不再作为下一个时间步的输入，输入来自于源语言对应的目标语言（类似于监督学习）。</li>
</ul>
<h3 id="解码器的解码策略"><a href="#解码器的解码策略" class="headerlink" title="解码器的解码策略"></a>解码器的解码策略</h3><h4 id="贪婪解码（greedy-decoding）"><a href="#贪婪解码（greedy-decoding）" class="headerlink" title="贪婪解码（greedy decoding）"></a>贪婪解码（greedy decoding）</h4><p>每个时间步选择预测概率最大的词作为预测结果，但是存在无法回退的问题</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923192446719.png" class="" title="image-20210923192446719">
<h4 id="束搜索解码（beam-search-decoding）"><a href="#束搜索解码（beam-search-decoding）" class="headerlink" title="束搜索解码（beam search decoding）"></a>束搜索解码（beam search decoding）</h4><p>由于穷举搜索的成本过高，贪婪解码又可能错过最优解，采用一种折中的思路，每次选取预测概率最大的k个词作为备选词，进入下一次预测，下一次同样选取概率最大的k个序列,其中k为束搜索的宽度。</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923193034552.png" class="" title="image-20210923193034552">
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923193403947.png" class="" title="image-20210923193403947">
<h5 id="如何结束搜索？"><a href="#如何结束搜索？" class="headerlink" title="如何结束搜索？"></a>如何结束搜索？</h5><ol>
<li>限定束搜索的最大时间步数</li>
<li>限定生成完整翻译句子的数字（最后一个时间步输出\<end\>标签）</li>
</ol>
<h5 id="解决束搜索倾向问题"><a href="#解决束搜索倾向问题" class="headerlink" title="解决束搜索倾向问题"></a>解决束搜索倾向问题</h5><p>由于score函数的计算方式，预测的翻译序列越长，其得分越高，导致束搜索更加倾向于短的翻译结果，通过对score归一化解决</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923194001899.png" class="" title="image-20210923194001899">
<h4 id="NMT的优势"><a href="#NMT的优势" class="headerlink" title="NMT的优势"></a>NMT的优势</h4><img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923194329188.png" class="" title="image-20210923194329188">
<h3 id="如何评价机器翻译效果"><a href="#如何评价机器翻译效果" class="headerlink" title="如何评价机器翻译效果"></a>如何评价机器翻译效果</h3><h4 id="BLUE-Bilingual-Evaluation-Understudy"><a href="#BLUE-Bilingual-Evaluation-Understudy" class="headerlink" title="BLUE(Bilingual Evaluation Understudy)"></a>BLUE(Bilingual Evaluation Understudy)</h4><p>将机器翻译结果语句（candidate）与一系列人类翻译的参考语句（references）相比较，比较相似度：</p>
<ol>
<li>依赖n-gram语法(以长度为n的子句子作为衡量的单元),计算 candidate中 n-gram在references中出现的个数 比上references中所有n-gram出现的次数。</li>
<li>并给倾向于生成过短翻译的系统以惩罚</li>
</ol>
<p>基本计算公式如下</p>
<script type="math/tex; mode=display">
BLUE=\frac{\sum_{n-gram \sub references}\sum_{n-gram \sub candidate \\ \& \\ n-gram \sub reference} Count_{clip}(n-gram)}{\sum_{n-gram \sub references}\sum_{n-gram \sub reference}}</script><p>通过召回率和惩罚因子，解决了重复出现和翻译较短给分较高的问题</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923200515633.png" class="" title="image-20210923200515633">
<h3 id="机器翻译目前问题"><a href="#机器翻译目前问题" class="headerlink" title="机器翻译目前问题"></a>机器翻译目前问题</h3><ol>
<li>遇到语料库之外的词语，翻译效果较差</li>
<li>如果训练材料较为局限，训练出来的模型也不具备普适性（Domain mismatch）</li>
<li>无法解决长文本、书籍等的翻译问题（聚焦于句子，没有对全文信息的参考）</li>
<li>翻译好的 训练数据较少</li>
</ol>
<h3 id="Attention机制"><a href="#Attention机制" class="headerlink" title="Attention机制"></a>Attention机制</h3><p>编码器最后将整个句子的信息编码进入一个输出向量中，该向量可能无法包含所有的句子信息，导致解码器在信息缺失的情况下进行生成翻译短文（information bottleneck），注意力机制提供了一种解决这种问题的方法。</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210924144203179.png" class="" title="image-20210924144203179">
<p>在Decoder的每个时间步，不再直接使用隐藏状态作为预测输出层的输入，而是需要计算注意力输出</p>
<ol>
<li><p>当前时间步与编码器的每个时间隐藏状态输出点乘获得一个多个注意力分数，组成向量</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210924145511050.png" class="" title="image-20210924145511050">
</li>
<li><p>输入注意力分数向量到softmax计算概率分布</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210924145544523.png" class="" title="image-20210924145544523">
</li>
<li><p>对Encoder的每个时间步的隐藏状态进行概率分布加权平均（”注意力“ 就是 对哪个实践步隐藏状态的概率值大小，越大说明我对对应词语注意力越集中）</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210924145757503.png" class="" title="image-20210924145757503">
</li>
<li><p>将注意力输出与当前隐藏状态拼接，作为当前时间步输出层的输入，获得预测词语</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210924145905073.png" class="" title="image-20210924145905073">
</li>
</ol>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210924144729428.png" class="" title="image-20210924144729428">
<p>Attention机制的优势</p>
<ol>
<li>显著提升了NMT系统的效果</li>
<li>解决了编码器存在的信息瓶颈问题</li>
<li>有助于解决梯度消失问题</li>
<li>提供了一定的可解释性（注意力分数）</li>
</ol>
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-7.机器问答</title>
    <url>/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-7.%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94/</url>
    <content><![CDATA[<h3 id="Question-Answering"><a href="#Question-Answering" class="headerlink" title="Question Answering"></a>Question Answering</h3><p>相较于检索更进一步，给出一个问题，自动的找出这个问题的最合适答案，可以将这个问题划分为两个步骤</p>
<ol>
<li>找到包含问题答案的文档</li>
<li>在文档中找到当前问题的答案（阅读理解 Reading Comprehension）</li>
</ol>
<p>如果一个机器理解了一段问题，机器应该能够提供问题的正确答案，且答案中不包含与问题无关的相关信息</p>
<h3 id="SQuAD-Stanford-Question-Answering-Dataset"><a href="#SQuAD-Stanford-Question-Answering-Dataset" class="headerlink" title="SQuAD(Stanford Question Answering Dataset)"></a>SQuAD(Stanford Question Answering Dataset)</h3><p>每个问题对应一篇文章，答案是文章内的一段单词序列。</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-7.%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94/image-20210926161057907.png" class="" title="image-20210926161057907">
<p>为每个问题提供多个可选的标准答案</p>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-7.%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94/image-20210926161326100.png" class="" title="image-20210926161326100">
<h4 id="SQuAD如何评估（V1-1）"><a href="#SQuAD如何评估（V1-1）" class="headerlink" title="SQuAD如何评估（V1.1）"></a>SQuAD如何评估（V1.1）</h4><ul>
<li>为每个问题提供三个标准答案</li>
<li>使用两种评分机制<ol>
<li>Exact match：按照字面意思理解，如果答案在三个标准答案中(1),不在标准答案中(0)</li>
<li>F1-score：分别计算对于每个问题回答的F1-score(<strong>具体怎么算法不知道</strong>)，对整个数据集上求平均后得到结果</li>
</ol>
</li>
<li>两种评分机制军忽略标点符号和无关的词汇（a,an,the）</li>
</ul>
<span id="more"></span>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-7.%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94/image-20210926164152196.png" class="" title="image-20210926164152196">
<h4 id="SQuAD如何评估（V2-0）"><a href="#SQuAD如何评估（V2-0）" class="headerlink" title="SQuAD如何评估（V2.0）"></a>SQuAD如何评估（V2.0）</h4><p>SQuAD2.0中1/2的问题的答案在文章中，1/2的问题的答案不在文章中（并不是所有的问题的答案都在文章中）</p>
<ul>
<li>对于无答案问题的评价，答案是无答案，给1；找到了某个答案，给0。（对于原有的两种评分机制都一样）</li>
</ul>
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-7.%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94/image-20210926165758040.png" class="" title="image-20210926165758040">
<img src="/2021/09/30/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-7.%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94/image-20210926165628432.png" class="" title="image-20210926165628432">
<h4 id="SQuAD存在的问题"><a href="#SQuAD存在的问题" class="headerlink" title="SQuAD存在的问题"></a>SQuAD存在的问题</h4><ul>
<li>所有问题的答案都是文章的子序列，没有难度更高的隐含、推理等类型的答案（后者问题更为常见）</li>
<li>SQuAD中问题的构建是基于文章的，问题倾向于与文章相似（体现在结构、用词等），降低了回答的难度</li>
</ul>
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-8.NLP中的CNN</title>
    <url>/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-8.NLP%E4%B8%AD%E7%9A%84CNN/</url>
    <content><![CDATA[<h3 id="从RNN到CNN"><a href="#从RNN到CNN" class="headerlink" title="从RNN到CNN"></a>从RNN到CNN</h3><p>RNN的输入是一个完整的序列，其每一个时间步的输出均受到之前时间步的影响，RNN最终捕捉的是整个序列的特征信息，而一些NLP问题可能更加关注于句子的局部信息(例如本文分类)，这一点是CNN的强项。</p>
<h4 id="CNN解决NLP问题的出发点"><a href="#CNN解决NLP问题的出发点" class="headerlink" title="CNN解决NLP问题的出发点"></a>CNN解决NLP问题的出发点</h4><p>按照窗口大小，在原序列上进行滑动，获得不同相同长度的子序列（语义可能无关），对于这些个子序列分别计算向量信息</p>
<ul>
<li>忽略了语法和语义信息，只是距离上的临近</li>
<li>没有RNN的语法语义上的说服性（你就这样随便划分，提取出来的向量能有用？）</li>
</ul>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-8.NLP%E4%B8%AD%E7%9A%84CNN/image-20211005093410774.png" class="" title="image-20211005093410774">
<span id="more"></span>
<p>NLP中的CNN类似于图像处理中的多通道一维卷积问题(conv1d)</p>
<ul>
<li>同样通过增加空向量，实现padding操作</li>
<li>使用多个卷积核，实现多通道输出</li>
<li>pooling over time，k-max pooling over time,dilation pooling</li>
</ul>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-8.NLP%E4%B8%AD%E7%9A%84CNN/image-20211005095925832.png" class="" title="image-20211005095925832">
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-8.NLP%E4%B8%AD%E7%9A%84CNN/image-20211005104515907.png" class="" title="image-20211005104515907">
<h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><blockquote>
<p>解释来自 <a href="https://www.cnblogs.com/shine-lee/p/11989612.html">Batch Normalization详解</a> </p>
</blockquote>
<p>作为网络的一层，对输入的一个batch进行标准化处理（减均值，除以方差），能够有效的降低深度网络的学习难度</p>
<p><strong>batch分布不断变化导致模型拟合偏差</strong></p>
<ul>
<li>每次梯度下降根据输入batch的分布计算，或者说拟合的是输入的分布</li>
<li>不同batch分布不同，导致每次拟合不断改变方向（类似于无头苍蝇），导致学习速率减慢</li>
<li>在机器学习或者浅层模型中，这种问题并不严重，而在深度网络中，每一层都在进行着（无头苍蝇拟合），为了避免震荡，必须将学习率设置的小一些（<strong>Internal Covariate Shift</strong>）。</li>
</ul>
<p><strong>BN的主要操作：</strong></p>
<ol>
<li>对batch数据进行标准化操作（减均值，除以方差）<ul>
<li>这一步的参数是由输入batch决定的不需要学习</li>
</ul>
</li>
<li>对标准化后的数据进行平移和放缩（修改均值，和方差）<ul>
<li>平移和放缩的量由网络学习得到，即batch分布的均值和方差由学习得到</li>
</ul>
</li>
</ol>
<p><strong>BN带来的好处：</strong></p>
<ol>
<li>所有变换均为线性变换，反向传播求导较为简单</li>
<li>可以使用更大的学习率</li>
<li>权重的大小和初始化值不再重要，bias也可以设置为0</li>
</ol>
<h3 id="CNN的应用"><a href="#CNN的应用" class="headerlink" title="CNN的应用"></a>CNN的应用</h3><p><strong>Translation</strong></p>
<p>使用cnn作为encoder，rnn作为decoder</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-8.NLP%E4%B8%AD%E7%9A%84CNN/image-20211005145436480.png" class="" title="image-20211005145436480">
<p><strong>使用cnn实现深度nlp文本分类系统</strong></p>
<h3 id="Quasi-Recurrent-Neural-Network"><a href="#Quasi-Recurrent-Neural-Network" class="headerlink" title="Quasi-Recurrent Neural Network"></a>Quasi-Recurrent Neural Network</h3><h3 id="Q-RNN-Experiments-语言模型"><a href="#Q-RNN-Experiments-语言模型" class="headerlink" title="Q-RNN Experiments: 语言模型"></a>Q-RNN Experiments: 语言模型</h3>]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-9.子词模型</title>
    <url>/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h3 id="语言学背景-划分更小单位的词"><a href="#语言学背景-划分更小单位的词" class="headerlink" title="语言学背景-划分更小单位的词"></a>语言学背景-划分更小单位的词</h3><h4 id="语音学和音韵学"><a href="#语音学和音韵学" class="headerlink" title="语音学和音韵学"></a>语音学和音韵学</h4><p>语音学中将语音看作连续不断变化的声音流，音韵学将语音划分为不同的单位-音位（phoneme），同一个词的读法中音位的不同，对于不同群体理解可能有不同的含义。但是由于发音对于文本的理解并无意义，将此思想借鉴到单词形态分析上，形成了这种（parts of word）的思想</p>
<h4 id="形态学：部分词（part-of-word）"><a href="#形态学：部分词（part-of-word）" class="headerlink" title="形态学：部分词（part of word）"></a>形态学：部分词（part of word）</h4><p>如何对词进行拆分以更好地理解当前的单词（有点中文里的看半边猜词的味道，英文里去掉前缀后缀看词根）</p>
<ul>
<li>传统方式是将单词划分为最小语义单位</li>
<li>使用字符级n-grams对单词进行拆分</li>
</ul>
<span id="more"></span>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211009184508969.png" class="" title="image-20211009184508969">
<h4 id="不同语言词语组成形式各不相同"><a href="#不同语言词语组成形式各不相同" class="headerlink" title="不同语言词语组成形式各不相同"></a>不同语言词语组成形式各不相同</h4><img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211009185807612.png" class="" title="image-20211009185807612">
<h4 id="为什么我们需要小于词语级别的模型"><a href="#为什么我们需要小于词语级别的模型" class="headerlink" title="为什么我们需要小于词语级别的模型"></a>为什么我们需要小于词语级别的模型</h4><ul>
<li><p>部分语言的单词空间过大</p>
</li>
<li><p>音译</p>
</li>
<li><p>非正式拼写</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211009190155726.png" class="" title="image-20211009190155726">
</li>
</ul>
<h3 id="Character-level-Model-字符级别模型"><a href="#Character-level-Model-字符级别模型" class="headerlink" title="Character-level Model 字符级别模型"></a>Character-level Model 字符级别模型</h3><p>当前字符级别模型主要有两个主要方向</p>
<ol>
<li>与词语级别模型相同架构，只是将单位缩小为 “word pieces”</li>
<li>复合架构，主要采用词语模型，对于特殊情况（未知词）等使用字符级模型</li>
</ol>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010152953701.png" class="" title="image-20211010152953701">
<h4 id="纯字符级别NMT模型"><a href="#纯字符级别NMT模型" class="headerlink" title="纯字符级别NMT模型"></a>纯字符级别NMT模型</h4><p>English-Czech WMT 2015 Results</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211009191631221.png" class="" title="image-20211009191631221">
<p>Fully Character-Level Neural Machine Translation without Explicit Segmentation</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211009192226372.png" class="" title="image-20211009192226372">
<p>Stronger character results with depth in LSTM seq2seq model</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211009192454633.png" class="" title="image-20211009192454633">
<p>模型较小使用word-level，较大使用character-level</p>
<h4 id="Byte-Pair-Encoding"><a href="#Byte-Pair-Encoding" class="headerlink" title="Byte Pair Encoding"></a>Byte Pair Encoding</h4><img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010154156409.png" class="" title="image-20211010154156409">
<p>源于一种字符压缩算法，将共同出现频率较高的两个压缩成字典中不存在的新字符。</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010154131682.png" class="" title="image-20211010154131682">
<p><strong>为了解决NMT翻译中的<UNK>问题以及英语中不同后缀含义不同</strong>，使用子词单元嵌入代替词语，利用BPE思想，每次选择词库中出现频率最高的词语对（不一定长度为2），作为新词典中的一个词，不断按照上述方式选择，直到达到词典目标大小。</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010155718892.png" class="" title="image-20211010155718892">
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010155859930.png" class="" title="image-20211010155859930">
<h4 id="Wordpiece-Sentencepiece-model"><a href="#Wordpiece-Sentencepiece-model" class="headerlink" title="Wordpiece/Sentencepiece model"></a>Wordpiece/Sentencepiece model</h4><p>google在BPE的基础上形成了两种类型的模型</p>
<ol>
<li>Wordpiece  单词为整体，以字母为单位，进行划分</li>
<li>Sentencepiece  句子为整体，以单词为单位，进行划分（有点<strong>意群</strong>的味道）</li>
</ol>
<p>bert使用了一种变种wordpiece模型</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010171122417.png" class="" title="image-20211010171122417">
<h4 id="Character-level-to-build-Word-level"><a href="#Character-level-to-build-Word-level" class="headerlink" title="Character level to build Word level"></a>Character level to build Word level</h4><img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010171600247.png" class="" title="image-20211010171600247">
<p>对字符卷积获得词嵌入向量</p>
<h4 id="Character-level-based-LSTM-to-build-word-repesetation"><a href="#Character-level-based-LSTM-to-build-word-repesetation" class="headerlink" title="Character level based LSTM to build word repesetation"></a>Character level based LSTM to build word repesetation</h4><img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010171833154.png" class="" title="image-20211010171833154">
<p>不使用卷积，使用bi-LSTM输入字符，输出词语嵌入</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010171929903.png" class="" title="image-20211010171929903">
<h3 id="Hybrid-NMT"><a href="#Hybrid-NMT" class="headerlink" title="Hybrid NMT"></a>Hybrid NMT</h3><p>将两种划分方式区分开</p>
<ul>
<li>主要使用词语层级的划分</li>
<li>使用字符划分作为补充</li>
</ul>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010173107643.png" class="" title="image-20211010173107643">
<h4 id="Fast-Test-Embedding"><a href="#Fast-Test-Embedding" class="headerlink" title="Fast-Test Embedding"></a>Fast-Test Embedding</h4><img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010174250157.png" class="" title="image-20211010174250157">
<p>基于character-level 和 word2vec，改善word2vec获得的词向量对于词典外词语（oov）以及词语的各种变形不适应的情况。</p>
<ul>
<li><p>将单词拆成字符级别的n-gram表示</p>
</li>
<li><p>使用n-grams中所有子词的向量作为词语的向量表示进行嵌入训练</p>
</li>
<li><p>训练完成后，简单求和作为词语的词嵌入形式</p>
<img src="/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010175316928.png" class="" title="image-20211010175316928"></li>
</ul>
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>Subword分词:BPE&amp;word-piece</title>
    <url>/2022/03/27/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/BPE&amp;wordpiece/</url>
    <content><![CDATA[<h2 id="Subword分词-BPE-amp-word-piece"><a href="#Subword分词-BPE-amp-word-piece" class="headerlink" title="Subword分词:BPE&amp;word-piece"></a>Subword分词:BPE&amp;word-piece</h2><p>在读transforme论文时，论文中在两个NMT任务中分别使用了两种编码算法byte pair encoding和word-piece，似乎子词嵌入模型是解决OOV问题不二选择，稍微了解一下</p>
<h3 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h3><p>为了解决NMT中的OOV问题，基于子词模型（sub-word）以及Byte pair encoding思想提出了BPE算法，解决了词表大小压缩问题</p>
<h4 id="Byte-pair-encoding"><a href="#Byte-pair-encoding" class="headerlink" title="Byte pair encoding"></a>Byte pair encoding</h4><p>一种简单的数据压缩算法，寻找byte串中重复出现多次的byte对(pair of consecutive bytes)，使用串中未出现过的byte替代，直到byte串中不存在重复多次的byte串。</p>
<p>wekipedia的例子：</p>
<span id="more"></span>
<img src="/2022/03/27/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/BPE&wordpiece/image-20220315101051756.png" class="" title="image-20220315101051756">
<p>BPE算法与原Byte pair encoding算法完全一致，只是运行在character层级上，论文中思路主要为</p>
<ul>
<li>初始词表为所有单词拆分的字母+ 特殊的单词结束符（a special end-of word symbol ‘·’）</li>
<li>重复遍历使用频率最高的pair替换<ul>
<li>如  (‘A’, ‘B’) 用  ‘AB’ 替换（一个词替代两个词）</li>
<li>replace each occurrence of the most frequent pair (‘A’, ‘B’) with a new symbol ‘AB’.</li>
</ul>
</li>
<li>重复迭代直到满足要求</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re, collections</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stats</span>(<span class="params">vocab</span>):</span></span><br><span class="line">    pairs = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(symbols) - <span class="number">1</span>):</span><br><span class="line">            pairs[symbols[i], symbols[i + <span class="number">1</span>]] += freq</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_vocab</span>(<span class="params">pair, v_in</span>):</span></span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(<span class="string">&#x27; &#x27;</span>.join(pair))</span><br><span class="line">    p = re.<span class="built_in">compile</span>(<span class="string">r&#x27;(?&lt;!\S)&#x27;</span> + bigram + <span class="string">r&#x27;(?!\S)&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        w_out = p.sub(<span class="string">&#x27;&#x27;</span>.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    <span class="keyword">return</span> v_out</span><br><span class="line">vocab = &#123;<span class="string">&#x27;l o w &lt;/w&gt;&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;l o w e r &lt;/w&gt;&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">         <span class="string">&#x27;n e w e s t &lt;/w&gt;&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;w i d e s t &lt;/w&gt;&#x27;</span>: <span class="number">3</span>&#125;</span><br><span class="line">num_merges = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_merges):</span><br><span class="line">    pairs = get_stats(vocab)</span><br><span class="line">    //寻找出现次数最多的pair对</span><br><span class="line">    best = <span class="built_in">max</span>(pairs, key=pairs.get)</span><br><span class="line">    vocab = merge_vocab(best, vocab)</span><br><span class="line">    <span class="built_in">print</span>(best)</span><br></pre></td></tr></table></figure>
<h4 id="joint-BPE"><a href="#joint-BPE" class="headerlink" title="joint BPE"></a>joint BPE</h4><p>NMT需要输入源语言、输出目标语言，因此需要构建两个分别包括两种语言的词表，论文中提出了两种类型的BPE</p>
<ul>
<li>源语言与目标语言分别运行BPE算法，构建子词词表</li>
<li>源语言与目标语言合并在一起，共建一个BPE词表（joint BPE）</li>
</ul>
<p>两种方法优略</p>
<ol>
<li>第一种方法，确保两种语言词表中都只包含出现过的子词，不会存在另一种语言的子词干扰，保证了词表的大小</li>
<li>第二种方法，确保了基于统计切词在两种语言上运行的一致性，相同的词切分方式保持一致（比如说相同的人名）</li>
</ol>
<p>没细看实验</p>
<img src="/2022/03/27/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/BPE&wordpiece/image-20220315112028772.png" class="" title="image-20220315112028772">
<h3 id="wordpiece"><a href="#wordpiece" class="headerlink" title="wordpiece"></a>wordpiece</h3><p>方法来自于论文：<a href="http://ieeexplore.ieee.org/document/6289079">Japanese and Korean voice search</a>， 该方法的主要思路与BPE基本相同，只是每次不再按照频率的大小选取词对，而是选择能够使得语言模型最大似然增加（increases the likelihood），算法基本流程如下（贪心思路）：</p>
<ol>
<li>首先以字符为单位在语料集上构建词表</li>
<li>使用词表+语料训练一个语言模型</li>
<li>遍历词表，选取字符对（word unit pair）,使用字符对替换的词表+语料再训练一个语言模型，最终选择使得<strong>语言模型最大似然最大</strong>的字符对作为<strong>此轮迭代选择的字符对</strong>，更新词表</li>
<li>重2，3直到词表大小满足预期要求</li>
</ol>
<p>每次迭代都需要遍历整个词表，假设词表大小为k，能够找的 word unit pair 个数为 $k^2$, 也就需要训练 $k^2$ 个语言模型，计算复杂度多得离谱，因此原论文提出了几个加速方法</p>
<ol>
<li><p>每次选择字符对，只选择训练语料中已经存在(字面意思理解就是 在语料中相邻的 word unit)</p>
</li>
<li><p>只选择那些很有可能成为最优字符对的进行比较（不太懂如何判断最有可能）</p>
</li>
<li><p>c和d没太理解</p>
<img src="/2022/03/27/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/BPE&wordpiece/image-20220316085911584.png" class="" title="image-20220316085911584"></li>
</ol>
]]></content>
      <tags>
        <tag>NLP理论</tag>
        <tag>分词算法</tag>
      </tags>
  </entry>
  <entry>
    <title>读论文4-GPT系列</title>
    <url>/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p>GPT作为NLP预训练语言模型的基石之一，从gpt1,gpt2到gpt3，不断扩大模型规模，将问题从fine-tuning拓展到zero-shot,试图解决更基础，但是更困难的无监督学习问题，三篇论文中的模型结构相同，不同的是试图解决的问题。</p>
<h3 id="GPT1"><a href="#GPT1" class="headerlink" title="GPT1"></a>GPT1</h3><blockquote>
<p><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a></p>
</blockquote>
<h4 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h4><p>NLP中有监督训练数据较少，存在大量的无标签文本数据，如何使用这些数据解决NLP领域中各类问题？GPT提出了 <strong>自监督预训练+fine tuning</strong> 的方式，主要解决两个问题</p>
<ol>
<li>预训练的<strong>训练目标是什么</strong>？损失函数是什么？</li>
<li>预训练得到的特征表示<strong>如何迁移</strong>到下游任务中？</li>
</ol>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>使用语言模型作为预训练的目标，在下游任务上做有监督的fine tuning</p>
<span id="more"></span>
<p><strong>预训练</strong></p>
<p>预训练使用无标签语料作为输入，以语言模型的最大似然函数为训练目标（给定前n-1个token，预测第n个token的条件概率）</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328083725904.png" class="" title="image-20220328083725904">
<p>模型使用transformer-decoder结构，最后一层通过softmax计算预测词的概率</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328084046552.png" class="" title="image-20220328084046552">
<p><strong>下游任务-fine tuning</strong></p>
<p>fine-tuning 使用下游任务的有标签语料，增加一个全连接sfotmax输出层，预测对应标签，损失函数为所有训练数据的log最大似然</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328084332027.png" class="" title="image-20220328084332027">
<p>另外增加了一个辅助训练目标，在下游任务上训练语言模型，使用两个损失函数求和作为<strong>最终的训练目标</strong>（$\lambda$ 为控制辅助训练目标对结果影响程度的参数）</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328084506127.png" class="" title="image-20220328084506127">
<p>针对不任务，仅仅使用softmax预测概率无法满足任务要求（如句子相似度衡量，需要输入两个句子，判断两个句子之间的相似度），gpt针对不同任务设计了不同类型的输入（<strong>Task-specific input transformations</strong>）</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328084857519.png" class="" title="image-20220328084857519">
<h4 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h4><p>模型细节</p>
<ol>
<li>12层的transformer-decoder，768隐藏层维度+12注意力头数，去掉了与encoder输出共同计算的注意力层（或者说<strong>使用 masked Multi Self Attention的encoder</strong>）</li>
<li>使用GLUE作为激活函数</li>
<li>使用 bytepair encoding (BPE) 词典，包含4000词</li>
<li>使用<strong>模型学习position encoding</strong>，而不是transformer论文中的三角函数计算</li>
</ol>
<p>训练细节</p>
<ol>
<li>训练数据集为 BooksCorpus dataset（contains over 7,000 unique unpublished books）</li>
<li>预训练：优化器为Adam，最大学习率为 $2.5*10^{-4}$，dropout=0.1，在batchsize=64,seq_length=512的条件下，训练100个epoch</li>
<li>微调：lr = 6.25e-5 , batchsize = 32, dropout = 0.1，训练3个epoch</li>
</ol>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>本文主要针对两个问题提出了解决方案，并在不同NLP上验证，得到了不错的效果</p>
<ol>
<li><p>预训练的<strong>训练目标是什么</strong>？</p>
<p>使用<strong>语言模型</strong>作为与训练目标（predict next token）</p>
</li>
<li><p>预训练得到的特征表示<strong>如何迁移</strong>到下游任务中？</p>
<p><strong>Task-specific input transformations + softmax predict层</strong> 在下游任务微调</p>
</li>
</ol>
<h3 id="GPT2"><a href="#GPT2" class="headerlink" title="GPT2"></a>GPT2</h3><blockquote>
<p><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">Language Models are Unsupervised Multitask Learners</a></p>
</blockquote>
<p>在BERT刷榜之后，GPT2带着更大的模型、更多的训练数据卷土重来，不再将关注点聚焦于 预训练+微调，转而研究语言模型在无监督多任务学习上的可能性</p>
<h4 id="要解决的问题-1"><a href="#要解决的问题-1" class="headerlink" title="要解决的问题"></a>要解决的问题</h4><p>论文中提出目前NLP领域主流的 <strong>预训练+微调</strong> 存在问题，即该方法仍需要监督学习，需要下游任务的大量有标签训练数据。本文将NLP的一般任务和特定任务定义为两种条件分布</p>
<ol>
<li>一般任务为 $p(output|input)$ 给定输入，对应输出的条件分布</li>
<li>特定任务  $p(output|input,task)$ 在特定任务上，输出不仅取决于输入，还取决于任务类型</li>
</ol>
<p>传统的预训练+fine-tuning方式</p>
<ol>
<li>预训练即模拟 $p(output|input)$条件分布</li>
<li>fine-tuning 通过在不同任务上<strong>调整模型架构、参数</strong>等，模拟  $p(output|input,task)$条件分布</li>
</ol>
<p>gpt2想要<strong>避免监督学习过程</strong>，即不在下游任务上做微调，引出了两个问题</p>
<ol>
<li>如何预训练能使得预训练过程中学到下游任务的信息？（原文中的句子：<strong>the global minimum of the unsupervised objective</strong><br><strong>is also the global minimum of the supervised objective.</strong> 全局非监督的收敛 等价于 全局监督学习的收敛）<ul>
<li>举个例子：像人学英语一样，背单词+看文章看看多了，就算从来没刷过题，题目也能做得效果不错</li>
</ul>
</li>
<li>如何<strong>不调整模型参数或者架构</strong>，<strong>实现从  $p(output|input)$ 到  $p(output|input,task)$</strong> 转变，以适应下游任务？</li>
</ol>
<h4 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h4><p>基本思路和GPT1区别不大，预训练语言模型+下游任务输入调整，不进行微调训练，个人觉得他的第二章Approach 前面关于理论来源的部分写的非常好，我读完感觉非常科学（也可能是我知识比较薄，看不出问题），分问题阐述一遍</p>
<p><strong>如何预训练能使得预训练过程中学到下游任务的信息</strong></p>
<blockquote>
<p>Preliminary experiments confirmed that sufficiently large <strong>language models are able to perform multitask learning</strong> in this toy-ish setup but learning is much slower than in explicitly supervised approaches.</p>
</blockquote>
<p>从两个角度解决该问题：</p>
<ol>
<li><p><strong>训练具备多任务学习能力的模型</strong>：语言模型 </p>
<ul>
<li>通过论文证明，语言模型具备多任务学习的能力，缺点是训练速度较慢</li>
</ul>
</li>
<li><p><strong>输入包含多任务信息的数据</strong>：WebText 数据集</p>
<ul>
<li><p>本文认为网络文本信息量巨大，包括各种下游任务中需要的数据信息，使用大量网络文本训练，能够使得模型学到不同任务的信息（multi task learning）</p>
</li>
<li><p>本文构造了一个WebText数据集，作者举了一些包含下游任务(NMT)数据的例子</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328101903526.png" class="" title="image-20220328101903526">
</li>
</ul>
</li>
</ol>
<p><strong>如何不调整模型参数或者架构，就能适应下游任务</strong></p>
<p>延续了GPT中的一些思路，通过修改输入表达方式，实现对下游任务的适应，以从英文到法文的机器翻译任务为例，在输入源句子的同时，以”english sentence = french sentence”为条件合并输入模型（文中成为 task hint）</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328143447270.png" class="" title="image-20220328143447270">
<p>在摘要任务中，论文尝试剔除了这种 <strong>”task hint“</strong>，发现任务指标下降了6.4个点，作者认为这证明了 使用自然语言提示模型针对任务改变的可行性</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328143917773.png" class="" title="image-20220328143917773">
<h4 id="顺带探讨的问题"><a href="#顺带探讨的问题" class="headerlink" title="顺带探讨的问题"></a>顺带探讨的问题</h4><p>在研究论文主要问题时，还顺带提了一下<strong>数据污染</strong>问题，即训练数据集和测试数据集之间重叠的问题，作者推荐在构建划分新的NLP数据训练集和测试集时，使用基于n-gram重叠的方法验证是否存在该问题</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328144755337.png" class="" title="image-20220328144755337">
<h4 id="实现细节-1"><a href="#实现细节-1" class="headerlink" title="实现细节"></a>实现细节</h4><p>模型细节</p>
<ol>
<li><p>使用<strong>BPE构建子词字典</strong>，为了避免无意义单词出现（”dog.“,”dog?”），限制不同类型字符共同出现，<strong>字典大小为 50257</strong></p>
</li>
<li><p>将transformer檐式结构结构中的 Post-LN 修改为了 Pre-LN(即把layer normalization移动到每个子层的前面，输入做归一化，输出不做)</p>
<ul>
<li>图片来自 <a href="https://arxiv.org/pdf/2002.04745v2.pdf">On Layer Normalization in the Transformer Architecture</a></li>
</ul>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328151023322.png" class="" title="image-20220328151023322">
</li>
</ol>
<p>训练细节</p>
<ol>
<li><p>batchsize=512，seq_length=1024</p>
</li>
<li><p>共有四个模型大小，最小的和gpt一一致，第二个与bert_large一致，最大的为GPT2</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328151832377.png" class="" title="image-20220328151832377">
</li>
</ol>
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p>本文使用更大的模型尝试证明语言模型在zero-shot上的可行性，在许多任务上取得不错的成绩。</p>
<h3 id="GPT3"><a href="#GPT3" class="headerlink" title="GPT3"></a>GPT3</h3><p>GPT3是对GPT2在zero shot learning上的进一步推进，GPT2效果没有达到预期，就在GPT3上继续增加模型和训练数据规模，提升效果，在完全预训练模型上继续推广，提出了三种将预训练应用到下游任务的非预训练方式（所谓的 <strong>in-context learning</strong>）</p>
<ol>
<li><p>Few-Shot 解决下游任务时，在输入中提供几个任务样例</p>
<ul>
<li><p>一个任务提示</p>
</li>
<li><p>几个任务样例（prompt）</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220329154323252.png" class="" title="image-20220329154323252">
</li>
</ul>
</li>
<li><p>One-shot</p>
<ul>
<li><p>只给一个任务提示</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220329154359207.png" class="" title="image-20220329154359207">
</li>
</ul>
</li>
<li><p>Zero-shot 完全不给任务提示，只告诉任务是什么</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220329154433953.png" class="" title="image-20220329154433953">
</li>
</ol>
<p>GPT3将模型规模增大到原来的1000倍</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220329154640571.png" class="" title="image-20220329154640571">
<p>又构建了一个集合以往各种文本数据的巨大数据集</p>
<img src="/2022/04/03/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220329154729495.png" class="" title="image-20220329154729495">
<h4 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h4><p>GPT3文章太长，我只粗略地读了一遍，总体思路还是企图证明大规模预训练语言模型，在下游任务中，即使没有经过预训练，也能实现不错的效果，相较于前两篇论文没有太多新的东西</p>
]]></content>
  </entry>
  <entry>
    <title>基于统计的文档语义表示</title>
    <url>/2022/04/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E6%96%87%E6%A1%A3%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA/</url>
    <content><![CDATA[<h2 id="基于统计的文档语义表示"><a href="#基于统计的文档语义表示" class="headerlink" title="基于统计的文档语义表示"></a>基于统计的文档语义表示</h2><p>在深度学习模型到来之前，通常使用统计学方法获得表示的文档语义的特征向量，主要方法包括</p>
<ol>
<li>TF-IDF</li>
<li>基于SVD分解的LSA（潜在语义索引/分析 Latent Semantic Indexing/Analysis）</li>
<li>LDiA 隐形迪利克雷分布</li>
</ol>
<p>其中前两种分布理解较为简单，LDiA涉及比较多的概率分布知识</p>
<h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p><strong>t</strong>erm <strong>f</strong>requency–<strong>i</strong>nverse <strong>d</strong>ocument <strong>f</strong>requency，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加（TF），但同时会随着它在语料库中出现的频率成反比下降（IDF）</p>
<span id="more"></span>
<p><strong>TF</strong></p>
<p>term frequency，给定一个词，当前词在当前文件出现的频率（通常需要除文档长度），计算公式如下</p>
<script type="math/tex; mode=display">
tf_{ij}= \frac{n_{j}}{n_i}</script><p><strong>IDF</strong></p>
<p>inverse document frequency，给定一个词，IDF值为文档库中总文档数除以包含该词的文档数</p>
<script type="math/tex; mode=display">
idf = log\frac{n_{total}}{n_{contains}}</script><p>将两个值相即为当前文档中该词的TF-IDF值，计算文档在词表上所有词的TF-IDF值，即可获得<strong>相较于词袋向量更加稠密但维度相同</strong>的文档特征向量,TF-IDF作为语义特征向量还存在着一些问题</p>
<ol>
<li>无法解决不同拼写但意思相近的词，TF-IDF计算是会将其看作不同的特征</li>
<li>TF-IDF特征向量的维度还是过大，计算量大且在样本量小（维度与样本数量接近）的时候，容易出现过拟合</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 快速计算TFIDF值，问题是处理不了中文</span></span><br><span class="line">sklearn.feature_extraction.text.TfidfVectorizer</span><br><span class="line"><span class="comment"># 将词频矩阵快速转化为IDF值（中文只能用这个了）</span></span><br><span class="line">sklearn.feature_extraction.text.TfidfTransformer</span><br></pre></td></tr></table></figure>
<h3 id="LSA-潜在语义分析"><a href="#LSA-潜在语义分析" class="headerlink" title="LSA 潜在语义分析"></a>LSA 潜在语义分析</h3><p>LSA实际上就是SVD矩阵分解应用到NLP领域中换的一个名字，结合SVD和PCA对别进行理解</p>
<h4 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h4><p>常用的针对非方阵矩阵的一种分解方法</p>
<img src="/2022/04/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E6%96%87%E6%A1%A3%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA/image-20220411100221445.png" class="" title="image-20220411100221445">
<p>其中矩阵 U和V满足:</p>
<img src="/2022/04/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E6%96%87%E6%A1%A3%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA/image-20220411100348208.png" class="" title="image-20220411100348208">
<p>迁移到NLP领域中,使用截断的SVD分解（取前p大的奇异值）,定义为所谓的<strong>潜在语义主题</strong></p>
<ul>
<li>矩阵$M_{m<em>n}$为 **词 </em>文档** 的特征向量矩阵，每一列均为一篇文档的特征向量</li>
<li>矩阵 $U_{m<em>p}$ 为 **词 </em> 潜在语义主题** 的关系矩阵</li>
<li>矩阵 $V_{n<em>p}$ 为 **文档 </em> 潜在语义主题** 的关系矩阵</li>
</ul>
<p>通过矩阵分解，获得了文档的潜在语义主题表示，实现了<strong>文档特征向量的降维</strong>（从词维度降低到定义的主题维度）</p>
<h4 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a><strong>PCA</strong></h4><p>Principal Component Analysis，从降维的角度出发，发现PCA与SVD都是在做类似于矩阵分解的操作，实现降维的目的。PCA出发点为找到对$X$的一种降维方法$Y=WX$，使得损失的信息量最少，如何实现这个目标？</p>
<ol>
<li>降维后某个特征（或者说维度）的方差最大（组内多样化）</li>
<li>不同特征之间的协方差为0（组间相关性尽量小）</li>
</ol>
<p>根据降维要求和方差大小，舍弃较小方差特征，保留较大方差特征，实现降维。</p>
<ul>
<li>如何转化为数学问题求解？引出了<strong>协方差矩阵</strong>，上述两个目标即为将协方差矩阵相似对角化过程（非对角线元素（协方差）转化为0，对角线元素（方差）转化为特征值）</li>
<li>如何快速找到目标矩阵 $X$ 的协方差矩阵？当$X$不同特征内归一化处理后，$XX^T$即为协方差矩阵</li>
</ul>
<p>最终转化为求解 $XX^T$的特征向量矩阵 $W$</p>
<h4 id="PCA-VS-SVD"><a href="#PCA-VS-SVD" class="headerlink" title="PCA VS SVD"></a>PCA VS SVD</h4><p>经过分析两者均最终转化为了 $XX^T$的特征向量矩阵的求解过程，两者在计算上实际等价，不同点在于</p>
<ol>
<li>PCA目的为降维，SVD目的为分解</li>
<li>PCA只能应用于方阵，SVD均可</li>
<li>PCA需要对数据进行中心化处理</li>
</ol>
<p>sklearn中的PCA实际上就是用SVD方法求解的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 截断SVD</span></span><br><span class="line">sklearn.decomposition.TruncatedSVD([n_components, ...])</span><br><span class="line"><span class="comment"># PCA</span></span><br><span class="line">sklearn.decomposition.PCA([n_components, copy, ...])</span><br><span class="line"><span class="comment"># 增量PCA 解决数量过大内存不足问题的近似PCA</span></span><br><span class="line">sklearn.decomposition.IncrementalPCA([n_components, ...])</span><br><span class="line"><span class="comment"># 稀疏PCA</span></span><br><span class="line">sklearn.decomposition.SparsePCA</span><br></pre></td></tr></table></figure>
<h3 id="LDiA"><a href="#LDiA" class="headerlink" title="LDiA"></a>LDiA</h3><p>Latent Dirichlet allocation 隐含迪利克雷分布，即它认为一篇文档是由一组词构成的一个集合，词与词之间没有顺序以及先后的关系。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。过程涉及到两个分布关系</p>
<ol>
<li>文档的主题<strong>分布</strong></li>
<li>每个主题的词<strong>分布</strong></li>
</ol>
<p>LDiA假设这两个分布均满足多项式分布，其中多项式参数个数为主题个数和词个数，在此假设下，我们已知<strong>数据+分布</strong>，<strong>如何求出目标分布的参数</strong>？LDiA假设主题分布和词分布的<strong>先验分布均为Dirichlet分布</strong>，即</p>
<img src="/2022/04/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E6%96%87%E6%A1%A3%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA/image-20220411153017543.png" class="" title="image-20220411153017543">
<ol>
<li>文档主题多项式分布参数 $p_1,p_2,p_3….p_n$满足Dirichlet分布（$p_i$ 为某文档中 $topic_i$ 出现的概率）</li>
<li>主题词多项分布 $p_1,p_2,p_3….p_n$满足Dirichlet分布（$p_i$ 即为某主题中 $word_i$ 出现的概率）</li>
</ol>
<p>常用方法有下面两种，不太清楚是否与贝叶斯估计有关系（<strong>不深入了解了，浪费时间也没必要</strong>）</p>
<ol>
<li>EM算法</li>
<li>Gibbs Sampling算法</li>
</ol>
<h4 id="想到了上学期学的贝叶斯估计求未知参数"><a href="#想到了上学期学的贝叶斯估计求未知参数" class="headerlink" title="想到了上学期学的贝叶斯估计求未知参数"></a>想到了上学期学的贝叶斯估计求未知参数</h4><p>我想到了数理统计里的贝叶斯估计，通过先验分布和后验分布，可以得到参数关于样本的概率分布，公式如下(应用数理统计-孙荣恒)</p>
<script type="math/tex; mode=display">
h(y|x_1,...,x_n) = \frac{\pi(y)f(x_1,...,x_n|y)}{g(x1,...,x_n)}\propto\pi(y)f(x_1,...,x_n|y)</script><p>其中 $y$ 后验分布中的未知参数，$\pi(y)$ 为先验分布（LDiA中的Dirichlet分布），$f(x_1,…,x_n|y)$ 为后验分布（LDiA中的多项式分布），得到未知参数关于实验数据（输入文档）的分布后，采取特定方法即可求得未知参数的估计值</p>
<h4 id="简单总结"><a href="#简单总结" class="headerlink" title="简单总结"></a>简单总结</h4><img src="/2022/04/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E6%96%87%E6%A1%A3%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA/image-20220411154536747.png" class="" title="image-20220411154536747">
<p>根据上述分析，我们可以得到LDA<strong>输入有两个超参数</strong></p>
<ol>
<li><strong>文档-主题</strong>先验Dirichlet分布的参数</li>
<li><strong>主题-词</strong>先验Dirichlet分布的参数</li>
</ol>
<p>LDA实际在求解的参数为</p>
<ol>
<li><strong>文档-主题</strong>多项式分布的参数</li>
<li><strong>主题-词</strong> 多项式分布的参数</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://zh.wikipedia.org/wiki/Tf-idf">tf-idf</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3">奇异值分解</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83">隐含狄利克雷分布</a></li>
<li><a href="https://www.cnblogs.com/yifanrensheng/p/13143970.html">NLP-04 隐含狄利克雷分布(LDA)</a></li>
<li>应用数理统计-孙荣恒</li>
</ol>
]]></content>
      <categories>
        <category>ML/DL理论学习</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title>Mapreduce论文总结</title>
    <url>/2022/04/24/Mapreduce%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="Mapreduce"><a href="#Mapreduce" class="headerlink" title="Mapreduce"></a>Mapreduce</h3><blockquote>
<p>论文地址: <a href="https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf">MapReduce: Simplified Data Processing on Large Clusters</a></p>
</blockquote>
<p>Mapreduce分布式编程模型，理解起来比较简单，主要总结一下模型+实现细节</p>
<h4 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h4><p>将任务划分为Map和Reduce两个阶段由用户实现，每个阶段输入输出key-value对（形象理解可以看论文中的例子）</p>
<img src="/2022/04/24/Mapreduce%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220420082702737.png" class="" title="image-20220420082702737">
<ol>
<li>Map 输入key-value，经过处理输出新的中间 key-value对，由MapReduce执行程序，将相同<strong>中间key</strong>聚集发送给某一个reduce执行程序</li>
<li>Reduce 输入一个中间key和key对应的value列表，reduce执行具体聚集操作后，获得最终的输出key-value</li>
</ol>
<span id="more"></span>
<h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>基本执行流程包括</p>
<ol>
<li>将输入文件划分为m份（16-64mb），对应m个map任务。在集群上启动多份应用程序</li>
<li>其中一个master程序，将map任务和reduce任务分配给空闲的worker(不同的worker可能是一台机器)</li>
<li>map任务程序首先执行，读取对应的文件块，将输出中间键值对缓存在内存中</li>
<li>每隔一段时间，map worker将缓存中的中间键值对存储到本地磁盘（根据key-&gt;reduce的映射进行partition）。这些中间键值对的地址，将传给master worker,供reduce worker远程读取</li>
<li>reduce worker由master唤醒后，通过RPC读取映射到本reduce worker的中间键值对输出</li>
<li>当属于某个reduce worker的中间键值对读取完成后，按照中间键值排序，对不同的key分组处理，也就是所谓的reduce输入key-value list</li>
<li>所有reduce程序完成后，结束mapreduce过程</li>
</ol>
<p>总结一下，就是map-&gt;partition-&gt;sort-&gt;reduce</p>
<img src="/2022/04/24/Mapreduce%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220420083217188.png" class="" title="image-20220420083217188">
<h4 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h4><p>为map和reduce任务定义了执行状态存储在master结点中，通过状态实现容错，每个任务的状态为</p>
<ol>
<li>idle 等待处理</li>
<li>in-progress 处理中</li>
<li>completed 处理完成</li>
</ol>
<p>master定期ping所有执行或者执行过任务的worker，若worker失效，将worker上执行过的所有任务（in-progress或者completed）状态设置为idle，等待分配给其他worker处理。</p>
<ul>
<li>若mapreduce任务执行在类似GFS的文件系统上，则complete类型任务不需要重新执行，因为输出文件不仅仅存储在失效的worker上</li>
<li>每当一个map任务重新执行后，需要通知所有的reduce任务该map任务的新执行</li>
</ul>
<h4 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h4><ol>
<li>map任务分配任务时遵循“靠近输入文件的原则”，首先考虑分配在包含输入文件的机器上，其次考虑靠近存储文件的机器上（移动计算比移动数据更有效）</li>
<li>用一些backup worker，替代拖后腿的worker，提升系统效率下限</li>
<li>map和reduce任务数量M和R要比机器数量大得多，以更好地负载均衡（每个workerp平均一个任务都分不到，谈何负载均衡?）和恢复错误（没太理解？）</li>
</ol>
<h4 id="问题思考"><a href="#问题思考" class="headerlink" title="问题思考"></a>问题思考</h4><ol>
<li><p>如何把map任务的输出分配给R个reduce worker（<strong>partition</strong>）</p>
<ul>
<li><p>原则：相同key必须分配到同一个reduce worker</p>
</li>
<li><p>最简单方式就是使用key的哈希值进行映射</p>
<img src="/2022/04/24/Mapreduce%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220420092210098.png" class="" title="image-20220420092210098">
</li>
</ul>
</li>
<li><p>既然有了partition，为什么要combiner?</p>
<ul>
<li>相同key值的key-value对可能很多(例如wordcount)，通过网络传播对带宽压力较大，</li>
<li>在map端先进行<strong>一部分的reduce操作</strong>，合并重复key，也能一定程度减轻reduce的计算压力</li>
</ul>
</li>
<li><p>直觉上为什么mapreduce能解决分布式计算问题？</p>
<p>从程序角度看，不管的单机还是分布式，其本质都是<strong>程序读取输入-&gt;程序计算-&gt;程序输出结果</strong>，我要实现分布式程序，无非要实现 <strong>分布计算 = 单机计算</strong></p>
<ol>
<li><strong>程序输入：</strong> 分布式文件存储在不同机器上，自然而然能够想到<strong>多个map程序读取文件</strong>的操作</li>
<li><strong>程序计算：</strong>难点在于分布式读入文件，我如何实现等价于单机计算的效果？我觉得这就reduce设计巧妙地地方，<strong>文件的分块不等于计算的逻辑分块</strong>，通过map—&gt;reduce程序的计算，实际上将<strong>文件的分块映射到计算的逻辑分块</strong></li>
<li><strong>程序输出：</strong> reduce输出实际逻辑子问题的输出-&gt;实际问题的输出（这一点我还没想明白，类似于归并排序 reudce制作到了归没有做并）</li>
</ol>
<p>以wordcount为例，讲一下我的理解</p>
<ol>
<li><strong>文件分块：</strong>整个文本文件-&gt;子文本文件块 （<strong>问题数据规模上分布</strong>）</li>
<li><strong>计算逻辑分块</strong>：统计每个不同字的字数-&gt;单独统计每个字的字数（<strong>问题逻辑规模上分布</strong>）</li>
</ol>
<p>map解决每个文本文件内字数的统计，reduce解决每个字字数统计</p>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>mapreduce只看理论理解还是太表面，还是需要写代码实战，学到了继续深化吧</p>
]]></content>
      <categories>
        <category>大数据理论</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title>GFS论文总结</title>
    <url>/2022/04/17/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/%E7%90%86%E8%AE%BA%E6%80%BB%E7%BB%93/GFS%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="GFS"><a href="#GFS" class="headerlink" title="GFS"></a>GFS</h3><blockquote>
<p>论文地址：<a href="https://research.google/pubs/pub51/">The Google File System</a></p>
</blockquote>
<p>google file system，入门大数据必读三篇文章之一，最近懒得看视频，小研究一波这篇论文，读完发现这种系统论文要比人工智能论文难读一些，涉及的技术比较多，粗浅的总结一下。</p>
<h4 id="论文结构"><a href="#论文结构" class="headerlink" title="论文结构"></a>论文结构</h4><p><del>论文结构有点像我的毕业论文</del>，首先从系统的结构出发，描述系统架构、组成等,从静态角度了解系统组成，然后从动态系统交互出发，描述系统交互，主要数据和操作流，最后单独两章描述mater节点的主要职责以及系统如何实现fault tolerance.</p>
<ol>
<li><p>intro 介绍</p>
<p>没讲很多背景，直接讲GFS这套解决方案与以往的GFS不同点，解决不同问题：</p>
<ol>
<li>分布式系统中组件经常失效（norm rather than exception）</li>
<li>按照以往的标准，目前文件大小都很大</li>
<li>目前对文件操作主要是添加和顺序读</li>
<li>面向应用设计GFS，增加了整个系统的灵活性</li>
</ol>
</li>
<li><p>DESIGN OVERVIEW </p>
<p>系统的组成+一致性模型，基本讲明白了系统怎么实现的文件系统功能</p>
</li>
<li><p>SYSTEM INTERACTIONS</p>
<p>描述了系统与client读写文件的交互流程，在这过程中如何保证GFS特性（从使用的角度，描述系统）</p>
</li>
<li><p>MASTER OPERATION</p>
<p>阐述master节点的职责，client与GFS交互中不直接相关的操作（从管理的角度）</p>
</li>
<li><p>FAULT TOLERANCE AND DIAGNOSIS</p>
<p>阐述容错机制（从策略角度）</p>
</li>
<li><p>MEASUREMENTS + EXPERIENCES + RELATED WORK + CONCLUSIONS</p>
<p>实验验证+经验总结+相关工作+总结</p>
</li>
</ol>
<span id="more"></span>
<h4 id="前提假设"><a href="#前提假设" class="headerlink" title="前提假设"></a>前提假设</h4><p>论文中说了GFS的设计是<strong>文件系统API</strong>和<strong>服务于应用</strong>的co-designing（需求来源于生活）， 对其解决的问题做了一定的假设和限定：</p>
<ol>
<li>分布式系统构建于便宜、容易出错的硬件上</li>
<li>系统存储一定数量的大文件（GB级别）</li>
<li>系统将要面临的读操作主要为大量的顺序读和少量随机读</li>
<li>系统面临的写操作主要为大量顺序添加操作，但也支持随机写</li>
<li>系统必须高效支持多客户端并发append写操作</li>
<li>重要性：高带宽&gt;低延时</li>
</ol>
<p>从前提假设中，我们就能得到GFS设计目标中的重点</p>
<ol>
<li><strong>重点</strong>：为<strong>多客户端</strong>的<strong>大文件存储</strong>以及<strong>顺序读+追加写</strong>提供<strong>高稳定高带宽</strong>服务</li>
<li><strong>相对而言不重要的</strong>（系统支持，但不一定效率高）：随机读+随机写+低延时</li>
</ol>
<h4 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h4><p>单管理节点（master）+多存储节点（chunk server）+多客户端（client）架构</p>
<ol>
<li>master节点职责<ul>
<li><strong>管理文件系统的元数据</strong>，包括访问控制信息，文件命名空间，文件到存储块的映射，存储块到存储节点的映射</li>
<li><strong>进行系统管理活动</strong>，包括存储块释放，垃圾回收，不同存储节点上存储块的迁移、复制，定时获取chunk server状态</li>
</ul>
</li>
<li>chunk server节点职责<ul>
<li>存储文件存储块（chunk）</li>
<li>通过HeartBeat消息，告知master节点自身状态</li>
<li>与client进行文件操作交互</li>
</ul>
</li>
<li>client主要操作<ul>
<li>与master节点进行元数据操作（获取文件存储的chunkserver等）</li>
<li>与chunk server进行文件操作（文件实际读写）</li>
</ul>
</li>
</ol>
<img src="/2022/04/17/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/%E7%90%86%E8%AE%BA%E6%80%BB%E7%BB%93/GFS%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220413194306905.png" class="" title="image-20220413194306905">
<h4 id="几个主要概念"><a href="#几个主要概念" class="headerlink" title="几个主要概念"></a>几个主要概念</h4><h5 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h5><blockquote>
<p>The master stores three major types of metadata: the file and chunk namespaces, the mapping from files to chunks, and the locations of each chunk’s replicas</p>
</blockquote>
<p>文件系统的元数据，存储在master节点的<strong>内存中</strong>（论文里不断强调这一点，为了方便master扫描，执行一些列管理操作，主要包括三种类型</p>
<ol>
<li>文件和文件块的命名空间信息（前缀压缩减少空间占用）</li>
<li>文件到文件块的映射</li>
<li>文件块的存储位置<ul>
<li>master节点在每次启动时，向chunk server请求其拥有的chunk信息，初始化文件块存储位置信息</li>
<li>并通过不断的Heatbeat Message 保证chunk信息不过时。（某种程度的低耦合）</li>
</ul>
</li>
<li>operation Log 操作记录</li>
</ol>
<h5 id="Operation-Log"><a href="#Operation-Log" class="headerlink" title="Operation Log"></a>Operation Log</h5><p>记录系统操作、作为系统逻辑时间的最重要的元数据</p>
<ul>
<li>operation log持久存储，多处备份，每次操作只有真正的记录到Operation Log中时，才会对客户端可见</li>
<li>operation log大小超过一定程度时，系统创建checkpoint，将当前operation log存储到本地</li>
<li>以compact B-tree的形式存储checkpoint，方便快速读取和加载</li>
<li>checkpoint的创建与切换新 operation log file并行进行</li>
</ul>
<h5 id="atomically-at-least-once"><a href="#atomically-at-least-once" class="headerlink" title="atomically at least once"></a>atomically at least once</h5><p>追加写操作保证”原子性“，我理解的是：真实追加写入的offset相较于client发出请求offset不一定一致，但是我保证所有副本最后在追加的offeset一定相同，并将这个offset返回给客户端。</p>
<p>举个例子，例如向A,B,C三个chunk server的同一文件的副本追加文件，<strong>A，B写入成功，C写入失败</strong>，GFS并不会单独重新在C上追加，而是在C上补充空白（insert padding or record duplicates in between.），使得三个文件的偏移量相同，重新写入ABC。</p>
<p>这样做会导致文件中出现<strong>无效数据</strong>，但是论文中说这些数据和用户数据相比<strong>微不足道</strong>（are typically dwarfed by the amount of user data）</p>
<h5 id="一致性模型中的consistent-和-defined"><a href="#一致性模型中的consistent-和-defined" class="headerlink" title="一致性模型中的consistent 和 defined"></a>一致性模型中的consistent 和 defined</h5><p>两者定义</p>
<ul>
<li><p>consistent 指所有的client看到相同的数据，即所有副本均相同</p>
</li>
<li><p>defined 指看到自己操作对于数据的改变 = 期望中的改变</p>
</li>
</ul>
<p>成功和失败的操作定义为：</p>
<ul>
<li>成功：可能会导致操作后<strong>undefined</strong>(无法预期操作的结果)，但数据仍为<strong>consistent</strong></li>
<li>失败：导致unconsistent，即不同数据备份不一致</li>
</ul>
<h5 id="Leases-and-Mutation-Order"><a href="#Leases-and-Mutation-Order" class="headerlink" title="Leases and Mutation Order"></a>Leases and Mutation Order</h5><p>lease用来记录对于一份文件多个并发操作的执行顺序，由master节点选取其中chunk server一个作为primary lease决定执行顺序，所有文件副本均按照primary lease操作顺序执行</p>
<ul>
<li>每个lease 60秒失效</li>
<li>在与master节点的Heartbeat中primary的授权信息（These extension requests and grants are piggybacked on the HeartBeat messages regularly exchanged between the master and all chunk server）</li>
</ul>
<h4 id="系统交互"><a href="#系统交互" class="headerlink" title="系统交互"></a>系统交互</h4><img src="/2022/04/17/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/%E7%90%86%E8%AE%BA%E6%80%BB%E7%BB%93/GFS%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220415142928379.png" class="" title="image-20220415142928379">
<p>以写流程流程为例，涉及client、Master、Chunk server之间的交互</p>
<ol>
<li>Client 向 Master请求写入文件的Chunk Server地址（包括备份的Chunk Server）</li>
<li>Master 返回给Client请求的Chunk Server地址，Client拿到地址后，将写入文件发送到所有目标Chunk Server中（不是<strong>分发</strong>，而是<strong>链式传输</strong>，论文中称“decoupling the data flow from the control flow”，提高了系统performance）</li>
<li>所有接收文件的Chunk Server确认接收完成后，client向被选为primary lease的Chunk Server发送写请求，由改Chunk Server确定包括该请求在内的其他并发请求的执行顺序，通知其他Replica Chunk Server，包括自在内按照该顺序执行操作（forwards the write request to all secondary replicas）</li>
<li>Replica Chunk Server向Primary Chunk Server返回执行完成确认，Primary Chunk Server向Client返回执行完成确认。</li>
</ol>
<p>一旦其中有一步失败，即向Client汇报失败，由Client自己进行处理（重试操作）</p>
<h4 id="几个主要操作（策略）"><a href="#几个主要操作（策略）" class="headerlink" title="几个主要操作（策略）"></a>几个主要操作（策略）</h4><h5 id="Data-Flow-数据传输"><a href="#Data-Flow-数据传输" class="headerlink" title="Data Flow-数据传输"></a>Data Flow-数据传输</h5><p>在Client向多个备份Chunk Server传输文件时，并不是采取传统的一个Client对多个Server的集中发送的方式，而是采取<strong>链式传输</strong></p>
<ol>
<li>Client首先选取最近(<strong>IP地址上的近</strong>)的Chunk Server传输文件</li>
<li>该Chunk Server再选取离他最近的未传输过文件的ChunkServer传输文件</li>
<li>重复传输，直到所有待传输Chunk Server获得文件</li>
</ol>
<h5 id="Atomic-Record-Appends"><a href="#Atomic-Record-Appends" class="headerlink" title="Atomic Record Appends"></a>Atomic Record Appends</h5><p>传统基于offset的写入，同一个区域的并发写入操作是无法序列化的（需要严格同步机制），针对record append操作，GFS舍弃了由应用输入offset的机制，应用只需要写入数据，由GFS写入后，向应用返回offset，其中一致性保证 <strong>atomically at least once</strong> </p>
<ul>
<li>若添加的文件使得chunk大小超出了最大限制，Primary Chunk Server会在填充当前Chunk剩余空间，创建新Chunk存储添加的文件内容</li>
<li>GFS限制写入文件大小小于Chunk块大小的四分之一，避免padding导致的碎片问题过于严重</li>
</ul>
<h5 id="Namespace-Management-and-Locking"><a href="#Namespace-Management-and-Locking" class="headerlink" title="Namespace Management and Locking"></a>Namespace Management and Locking</h5><p><del>说实话没理解并不是很深刻</del>， GFS为系统中的每个路径前缀均设置了读写锁</p>
<ol>
<li><p>读操作需要获取叶子节点所有前缀read-lock</p>
</li>
<li><p>写操作只需要叶子节点路径write-lock(因为目录并不是实际的数据结构，不需要修改目录，只需修改文件)</p>
<img src="/2022/04/17/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/%E7%90%86%E8%AE%BA%E6%80%BB%E7%BB%93/GFS%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220415152805791.png" class="" title="image-20220415152805791">
</li>
</ol>
<h5 id="Snapshot"><a href="#Snapshot" class="headerlink" title="Snapshot"></a>Snapshot</h5><p>系统快照，快速保存文件系统状态，其主要步骤为</p>
<ol>
<li>master首先撤下快照涉及文件的primary lease(避免在快照过程中文件上发生操作)</li>
<li>master将log record 操作记录存储到磁盘中</li>
<li>快照完成后，master接收到来自client对快照文件访问的新请求（通过操作记录数大小判断）时，延迟返回结果，创建一个与原Chunk相同的新Chunk供Client访问（懒备份）</li>
</ol>
<p>这一过程对于Cilent来说是无感的</p>
<h5 id="Garbage-Collection"><a href="#Garbage-Collection" class="headerlink" title="Garbage Collection"></a>Garbage Collection</h5><p>当删除一个文件时，GFS并直接回收文件占用资源，而是将文件名修改为hidden状态，待日后删除，基本流程</p>
<ol>
<li>删除文件，master结点记录删除操作，在chunk中将文件修改为hidden状态</li>
<li>master扫描到hidden状态文件，删除meta数据中关于hidden文件信息</li>
<li>chunk server与master 的Heatbeat message交换中，发现master已没有了这部分信息，<strong>chunk server回收空间</strong></li>
</ol>
<p>同样通过Garbage Collection回收stale Replica</p>
<h5 id="Data-Integrity"><a href="#Data-Integrity" class="headerlink" title="Data Integrity"></a>Data Integrity</h5><p>checksum+chunk version number机制保证数据正确性和及时性（up-to-date）</p>
<ol>
<li>每次读操作时，cunkserver验证本机数据check sum是否正确，不正确返回失败</li>
<li>为每个副本维护一个版本号，操作递增，当版本号不同时，由master在Garbage Collection回收过期副本</li>
</ol>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>按照自己的理解对GFS论文简单的理解了一下，感觉有些东西理解的还不是很透彻，慢慢深入了解</p>
]]></content>
      <categories>
        <category>大数据理论</category>
      </categories>
      <tags>
        <tag>GFS</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
</search>
