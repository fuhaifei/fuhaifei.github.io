<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>分布式一致性复习</title>
    <url>/2023/07/11/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%A4%8D%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="分布式一致性与共识"><a href="#分布式一致性与共识" class="headerlink" title="分布式一致性与共识"></a>分布式一致性与共识</h2><p>在学习微服务RPC，服务注册发现等相关概念时，总是绕不开一个话题分布式一致性，虽然之前在MIT6.824的课程中已经进行深入的学习，但是每每遇到还是对这些概念不甚清晰，因此这篇博客主要是从整合的角度复习之前学过的知识。</p>
<h3 id="常见概念"><a href="#常见概念" class="headerlink" title="常见概念"></a>常见概念</h3><p>分布式和单机本质的不同在于没有一个统一的时钟以及一份数据有多个副本，不同的用户请求于不同时间发出，由于网络延迟等问题以不同的顺序，对不同数据副本进行操作，这就带来两个问题：</p>
<ol>
<li>如何确定谁的请求先来，谁的请求后来，怎么确定一个所谓的“先来后到”？（可以类比Mysql并发控制的感觉）</li>
<li>副本的操作如何及时同步，用户能不能看到最新的副本？</li>
</ol>
<span id="more"></span>
<p><strong>分布式一致性</strong> 实际上就是对<strong>分布式服务对上述两个问题解决程度</strong>的一种抽象描述，定义了一个分布式应用能够有像“单机应用”的衡量标准，几个分布式一致性的概念不在赘述，见<a href="https://fuhaifei.github.io/2022/07/31/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%80%BB%E7%BB%93/">分布式事务总结</a>。而<strong>共识算法</strong>则是保证分布式一致性的手段，采取合适的共识算法达到不同程度的分布式一致性,简单理解就是如何在一群人中对一个方案达成共识，常见的共识算法包括：</p>
<ol>
<li>Paxos：所有共识算法的亲爹，见 <a href="https://fuhaifei.github.io/2022/06/08/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Paxos/">共识算法-Paxos</a></li>
<li>Raft：目前广泛应用的共识算法， 见 <a href="https://fuhaifei.github.io/2022/06/08/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Raft/">共识算法-Raft</a></li>
<li>ZAB：zookeeper中实现的共识算法，见 <a href="https://fuhaifei.github.io/2022/07/13/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/ZooKeeper%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/">ZooKeeper论文总结</a></li>
<li>Gossip：经典的弱一致性共识算法</li>
<li>Distro：Nacos实现的弱一致性共识算法</li>
</ol>
<p>上述是从理论的角度分析分布式一致性问题，当我们把视角转到分布式应用又会引出两个概念：</p>
<ul>
<li>CAP：当发生网络分区时（Partition），一个分布式应用是选择保C（Consitincy）还是选择保A（Availability）</li>
<li>BASE：作为一个服务提供商，在网络分区问题出现时我不可能舍弃A，在牺牲一定程度C的情况下，继续提供一定程度的A。<ul>
<li>Basically Available：提供一定程度的可用性</li>
<li>Soft State: 软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性， 即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时</li>
<li>Eventually Consistency: 经过一段时间同步后，所有副本都达到一致性的</li>
</ul>
</li>
</ul>
<p>主要的概念其实就这么多，其他都是深入实现细节后引出的拓展概念，下面简单总结一下之前没学过的分布式共识算法。</p>
<h3 id="共识算法"><a href="#共识算法" class="headerlink" title="共识算法"></a>共识算法</h3><p>三个强一致性协议在之前就已经学习过，不再废话，主要新整理一下最近接触到的弱一致性共识算法。</p>
<h4 id="Gossip算法"><a href="#Gossip算法" class="headerlink" title="Gossip算法"></a>Gossip算法</h4><blockquote>
<p>原论文：<a href="https://link.juejin.cn/?target=http%3A%2F%2Fbitsavers.trailing-edge.com%2Fpdf%2Fxerox%2Fparc%2FtechReports%2FCSL-89-1_Epidemic_Algorithms_for_Replicated_Database_Maintenance.pdf">《Epidemic Algorithms for Replicated Database Maintenance》</a></p>
<p>博客写的太好了，我在他的基础上简单总结强化记忆，建议直接看：<a href="https://juejin.cn/post/6930774718114955278">[分布式系列]Gossip协议</a></p>
</blockquote>
<p>论文中描述了三种节点见数据同步的方式：</p>
<ol>
<li>直接邮寄（direct mail）：广播模式，每个结点的更新都会通知到其他所有节点。</li>
<li>反熵传播（anti-entropy）：每个节点都会定期随机选择节点池中的一些节点，交换数据实现同步。</li>
<li>谣言传播（rumor mongering）：当节点更新时，周期性随机选择向周围固定数量的节点同步更新，接收到的节点更新的节点执行相同逻辑（不会向发送个自己的节点发送），直到节点节点发现周边节点都获取到这个更新</li>
</ol>
<p>其中通信模式包括：</p>
<ol>
<li>推模式：推送数据到目标节点，目标节点更新</li>
<li>拉模式：发送节点通知目标节点自己版本号，目标节点根据版本号推送新数据到发送节点，发送节点完成更新</li>
<li>推拉模式：在拉模式最后，发送节点再发送自己更新的数据给目标节点。</li>
</ol>
<p>Gossip协议存在消息冗余，收敛速度不可控等问题</p>
<h4 id="Distro算法"><a href="#Distro算法" class="headerlink" title="Distro算法"></a>Distro算法</h4><p> Distro算法是Nacos自己实现的AP共识算法，直观感觉就是Gossip算法的变形优化，其主要特点为：</p>
<ol>
<li><p>Nacos每个节点是平等的都可以处理写请求，同时把新数据同步到其他节点</p>
</li>
<li><p>每个节点只负责部分数据，定时发送自己负责数据的校验值到其他节点来保持数据一致性</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> target = distroHash(serviceName) % healthyList.size();</span><br></pre></td></tr></table></figure>
</li>
<li><p>每个节点独立处理读请求，及时从本地发出响应</p>
</li>
</ol>
<p>当一个Nacos集群进程上线时，会轮询集群中所有的节点获取全量数据，正常运行时定期相互发送带有元数据信息的心跳，当接收方发现数据不一致时，会主动发起拉取任务。</p>
<p>由于每个节点负责部分数据，存储全量数据，因此读操作可以在任意实例上执行，写操作需要转发到对应的实例上执行。</p>
<p><strong>总结</strong>：本质上还是Gossip协议，通过数据分片管理缓解了Gossip协议的消息冗余问题</p>
<ul>
<li>eureka类似于Nacos，同样基于一个类似于Gossip的协议实现了AP性质</li>
<li>Consul基于raft实现了强一致，但是提供了三种读取模式，default（基于lease的leader读一致），consistent（基于确认消息的leader读一致），stale（任意节点均可读）</li>
</ul>
<h4 id="Raft补充"><a href="#Raft补充" class="headerlink" title="Raft补充"></a>Raft补充</h4><blockquote>
<p>开头废话：之前在面试中回答完Raft协议本身，总要被问到一个问题：“你怎么保证客户端一定读取到最新的数据？“，对于这个问题我每次都会回答只要保证每次读取在leader节点读即可，然而往往会引入下一个问题：”只读主节点那么是不是浪费了副本节点的性能？有没有其他的解决方法？“。问题到这里我就回答不上来了，下面补充一下相关知识。</p>
</blockquote>
<p>回顾上文中提到的分布式环境带来的两个主要问题，不难发现类似Raft一系列的强共识算法的解决方案基本相同：</p>
<ol>
<li>通过定义逻辑时钟解决分布式环境下的”统一时钟问题“。如Raft的term，zab的zxid中高 32 位的epoch号</li>
<li>通过选举唯一的leader解决请求先来后到的问题，所有请求都由唯一leader执行，自然就有了顺序。</li>
</ol>
<p>所以”读到最新副本问题“从何而来，本质上有两个原因：</p>
<ol>
<li>”读操作“并不改变副本状态，不是一致性算法考虑的问题，一致性算法只保证修改状态的”写操作“的一致性。</li>
<li>一致性算法只能保证所有副本一致性的修改状态，不能保证何时达到一致性的状态（参考分布式中的safety &amp; liveness概念）</li>
</ol>
<p>因此要保证”读到最新副本“ 有两种解决方案：一是读leader，leader一定是最新数据；二是拓展一致性协议，使之兼顾follower 读。</p>
<p><strong>leader读</strong></p>
<p>leader读看起来美好，但是在遇到”脑裂“问题是还是会读到过期数据：</p>
<ul>
<li>”脑裂“场景：网络出现分区划分为多数和少数节点分区，旧leader在少数节点分区仍认为自己是leader，然而此时多数节点分区已经选举出新的leader并执行了写操作</li>
<li>此时如果客户端向旧leader发起读操作，旧leader认为自己还是leader返回请求，就导致客户端读到了旧数据。</li>
</ul>
<p>针对上述问题两个解决方案：</p>
<ul>
<li><strong>follower read</strong>：leader在选举成功后维护一个lease（大于选举超时时间）,在当前lease内认为不会有其他的<strong>leader</strong>出现（即自己不会被取代），在lease时间内响应客户端请求，lease失效请求多数节点更新lease</li>
<li><strong>ReadIndex</strong>：leader在响应前通过心跳多数节点确认自身的leader身份</li>
</ul>
<p><strong>follower读取</strong></p>
<p>leader读再怎么改进还是存在着单点性能问题，实现follower读方式为：</p>
<ul>
<li><strong>follower read</strong>：follower向leader查询commitIndex，若commitIndex &gt; applyIndex，则副本等待日志应用到对应index后再响应用户请求。</li>
</ul>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>根据上文中分析能够得到通过增加节点通信的方式能够解决”读不到“最新数据的问题，需要根据应用场景考虑是否一定需要保证读到最新数据，下面举几个例子：</p>
<ul>
<li>zookeeper 只提供弱一致性支持，即不保证客户端读到最新数据，除非客户端显式调用<code>sync()</code>方法</li>
<li>redis cluster基于raft选主，但是基于gossip实现同步集群信息同步，基于主从同步实现副本同步</li>
<li>TiKV 默认支持 lease read（基于raft log 更新lease），后续版本更新了对于 follower read的支持</li>
<li>etcd基于ReadIndex实现强一致性</li>
</ul>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://juejin.cn/post/6930774718114955278">[分布式系列]Gossip协议</a></p>
<p><a href="https://cn.pingcap.com/blog/lease-read">TiKV 功能介绍 - Lease Read</a></p>
<p><a href="https://docs.pingcap.com/zh/tidb/stable/follower-read#follower-read">TiKV Follower Read</a></p>
]]></content>
      <categories>
        <category>分布式理论</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>博客建成</title>
    <url>/2021/07/01/%E5%8D%9A%E5%AE%A2%E5%BB%BA%E6%88%90/</url>
    <content><![CDATA[<h3 id="为什么要搭建这个博客"><a href="#为什么要搭建这个博客" class="headerlink" title="为什么要搭建这个博客"></a>为什么要搭建这个博客</h3><ol>
<li>项目不会做，不知道干啥，就来搞点没用的</li>
<li>想把笔记放在网站上，起到保存和督促的作用</li>
</ol>
<h3 id="当前博客的计划"><a href="#当前博客的计划" class="headerlink" title="当前博客的计划"></a>当前博客的计划</h3><ol>
<li>每周末把这周的笔记整理发布到网站上</li>
<li>其他感想之类的废话，还有待思考</li>
</ol>
<p>总结：闲的没事干，整点歪门邪道</p>
]]></content>
      <categories>
        <category>杂文</category>
      </categories>
  </entry>
  <entry>
    <title>2022年终总结和新年展望</title>
    <url>/2023/01/02/%E6%9D%82%E6%96%87/2022%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%E5%92%8C%E6%96%B0%E5%B9%B4%E5%B1%95%E6%9C%9B/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="加付海飞QQ793515331." data-whm="不能看.">
  <script id="hbeData" type="hbeData" data-hmacdigest="4bfb1ef421b009134c23f2d1d8ad84d33f4a7b75277e5c855cc1b86b1b288eae">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">不知道密码就别看了.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>杂文</category>
      </categories>
      <tags>
        <tag>废话连篇</tag>
      </tags>
  </entry>
  <entry>
    <title>死亡搁浅通关感想:一场关于‘连接’的快递之旅</title>
    <url>/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/</url>
    <content><![CDATA[<p>最开始从b站老戴的全流程视频里了解到了死亡搁浅这款游戏，听说这个游戏好看不好玩，原计划在b站云通关，但是看了几期视频后，发现游戏所营造的世界观和剧情深深的吸引了我，于是等到了Epic夏促打折入手，这几天终于通关了游戏，心里有一些小感受，写一点东西，算是对死亡搁浅体验的个人总结。</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/20210617193238_1.jpg" class="" title="20210617193238_1">
<span id="more"></span>
<h4 id="我玩到了什么"><a href="#我玩到了什么" class="headerlink" title="我玩到了什么"></a>我玩到了什么</h4><p>说实话网上说死亡搁浅更像电影，不像游戏有一定道理的，全流程体验下来，感觉主要的游戏方式主要就局限于<strong>送快递、打BT、抢劫米尔人</strong>这几种，修公路、建滑索算也勉强算是游戏内容一部分。</p>
<ul>
<li><strong>跑图送快递</strong>，从一个点到另一个点跑图，有车开车，没车硬跑，还要维持身体平衡，比较枯燥，但是小岛配的音乐真的有味道，和场景搭配起来恰到好处，一定程度解决了长距离跑图的枯燥</li>
<li><strong>干BT</strong>，说实话我感觉体验不是很好，虽然武器和怪物随着剧情发展种类都会不断增加，但是实际体验下来还是换汤不换药，基本就是站桩打枪，尤其是背着一堆货物，是真不想打BT</li>
<li><strong>抢劫米尔人</strong>，这部分类似于其他游戏里的暗杀，不能击杀只能靠近用绳索勒晕，或者后期用步枪什么的，体验多了就有点枯燥了，尤其是后面米尔人有枪之后，我基本能绕着走就绕着走（另外我修了三条路，全是靠米尔人的寄存桶资源，谢谢米尔人为美国做出的贡献）</li>
</ul>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/20210721223211_1.jpg" class="" title="20210721223211_1">
<h5 id="体验不好的点"><a href="#体验不好的点" class="headerlink" title="体验不好的点"></a>体验不好的点</h5><blockquote>
<p>你想想，你带着货箱，开着小摩托，吃着隐生虫唱着歌，突然被米尔人给劫了</p>
</blockquote>
<p>游戏难度我选的默认普通，基本不会遇到怪打不过重新加载存档点，或者遣返重生的情况，最多的情况就是被米尔人抢劫了，或者掉河里货丢了这两种情况，尤其是开着摩托，带着一堆货，突然遇到一堆米尔人，他们追我跑，结果开的太快冲进河里，车也没了，货也没了，真的我当时就想把游戏性卸载了，ZTMD难受。</p>
<p>我感觉我游戏体验不好部分主要就是</p>
<ul>
<li><strong>载具手感真差</strong>，拿手柄开摩托车、汽车是真的折磨，弯转不过来，前进后退真的迷，难道为了让我们修公路故意这么做的吗？</li>
<li><strong>传送点没啥用</strong>，后期用fragile的伞给我们整了个传送功能，但是不能带货的传送有啥用？反过来想如果能带货那就没意思了，总而言之传送功能没什么用处。</li>
<li><strong>战斗太单一了</strong>，从头到尾就是打BT，从手榴弹打狮子狗BT，到步枪打希格斯BT,最后的榴弹炮打鲸鱼BT，真的没什么区别，玩到后面真的审美疲劳。</li>
<li><strong>不时需要安慰的bb</strong>，有时候真的稍微摔一下bb就开始哭，这就是照顾孩子的心情吗？一次两次还好，次数多了真的着急</li>
<li><strong>剥洋葱式讲剧情</strong>，不能说是缺点，但是一上来就给你扔进入一个未知的世界，陌生的世界观，玩起来真的晕头转向，不知道自己在干啥，后面一点一点展开就好点了，确实提高了游戏的入门门槛。</li>
</ul>
<h5 id="印象深刻的战斗"><a href="#印象深刻的战斗" class="headerlink" title="印象深刻的战斗"></a>印象深刻的战斗</h5><p>第一个是游戏快接近结束时和希格斯的从现实到冥滩的三百回合大战，在前面的剧情里把希格斯塑造的太无敌，每次出来都是在主角的脸上跳舞，以至于在最后一场和希格斯的肉搏战里，看着把希格斯脸锤得变形是真的解气，就ntm叫希格斯啊！！！</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/Death_Stranding_2021_8_4_15_48_50.png" class="" title="Death_Stranding_2021_8_4_15_48_50">
<p>第二个是拔叔饰演的昂格尔剧情部分的越战冥滩，虽然进入以后还是老一套的打BT，但是场景中枪林弹雨、各种飞机火箭的轰鸣，战场的紧张，肾上腺素喷薄，真的让我感觉仿佛置身于战场之中，我真真切切的被场景所震撼了。</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/20210802145157_1.jpg" class="" title="20210802145157_1">
<h4 id="死亡搁浅讲了什么（剧透警告）"><a href="#死亡搁浅讲了什么（剧透警告）" class="headerlink" title="死亡搁浅讲了什么（剧透警告）"></a>死亡搁浅讲了什么（<strong>剧透警告</strong>）</h4><p>死亡搁浅的出现让人类世界支离破碎，人们虽然居住在一篇大陆，却只能困于自己小小的避难所之中，无法与其他人产生联系，在这样的背景下，主角sam肩负起了连接世界的重任，通过连接一个一个节点的网络，让人们重新连接在一起，随着连接的不断进行，主角渐渐发现的自己的身世和死亡搁浅的真相，那就是人类的第六次灭绝，而自己是解决这一切的关键，是选择破而后立还是负重前行？面对决定人类命运最终的抉择，主角做出了自己的选择。死亡搁浅故事内核在我看来还是<strong>美式个人英雄主义</strong>式的，一人carry全场后，了却世俗，隐于山林。</p>
<h5 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h5><p>游戏从到到尾都在连接开罗尔网络，但我觉得小岛所传达出的出的‘连接’不仅仅局限于物理意义上的连接，更重要的是末世下人与人心之间的连接，游戏不断地用一个一个支线故事调这一点。玛玛与洛克妮的合体，废品商与女朋友的重归于好等等，相比连接开罗尔网络，更重要的是连接破碎的人心。</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/20210803110221_1.jpg" class="" title="20210803110221_1">
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/20210802102701_1.jpg" class="" title="20210802102701_1">
<p>在连接世界的过程中，山姆原本冰冷的心也渐渐的与世界连接在一起，从最开始的密切接触恐惧症，到最后主动与亡人紧紧拥抱，主动接受芙拉吉尔隐生虫，山姆接受了世界，也融入了这个世界。</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/20210802144048_1.jpg" class="" title="20210802144048_1">
<p>在游戏机制上，小岛也设计了玩家与玩家之间的连接，玩家之间可以相互帮忙送货，一起搞基建，相互点赞。从剧情到游戏，真的让我感觉紧紧连接在一起。（经常骑别人的摩托车，丢了也不心疼）</p>
<h5 id="关于死亡"><a href="#关于死亡" class="headerlink" title="关于死亡"></a>关于死亡</h5><blockquote>
<p>世界上只有一种英雄主义,就是看清生活的真相之后依然热爱生活</p>
</blockquote>
<p>游戏的最后，小岛给我们抛出了一个问题，当你面临关于人类命运的选择，你会做出哪个选项？</p>
<ol>
<li><strong>破而后立</strong> 选择灭亡，毁灭是为了更好的重生</li>
<li><strong>负重前行</strong> 选择阻止死亡搁浅的发生，接受这一事实，继续生活</li>
</ol>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/Death_Stranding_2021_8_10_17_52_30.png" class="" title="Death Stranding 2021_8_10 17_52_30">
<p>然而小岛并没有给我们选择的权力，我拿着枪一顿射，发现根本打不死amile，试了好几次才发现只能拥抱amile组织死亡搁浅的发生，最后只能违背自己的愿望，继续活下去，其实我心里更喜欢选项1。或许正如罗曼罗兰所说的：世界上只有一种英雄主义,就是看清生活的真相之后依然热爱生活，负重前行也许是更好的选择。</p>
<h5 id="山姆离去"><a href="#山姆离去" class="headerlink" title="山姆离去"></a>山姆离去</h5><p>其实最后山姆拒绝芙拉吉尔，选择自己走了我是有点难受的，我觉得他就应该和芙拉吉尔在一起，我觉得小岛一直在用山姆对待隐生虫的态度象征两个人的关系，最开始在山洞相遇山姆拒绝芙拉吉尔的隐生虫，后面开始接受，再到主动给芙拉吉尔隐生虫，象征着两人的关系越来越亲密，最后在首都节点城，拒绝芙拉吉尔后，天空也出现了隐生虫，但是山姆放弃了，真的难以接受。</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/image-20210812103152564.png" class="" title="image-20210812103152564">
<p>至于山姆为什么要离去？我觉得首先山姆最牵挂的就是amile、bb以及芙拉吉尔，但是当他完成最终选择，他发现amile留在冥滩，永远回不来了，bb也死了，他所有的牵挂都离他而去。山姆发现自己牵挂的东西最后都不得善终，因此主动放弃了芙拉吉尔，选择自己一个人享受孤独。（忘记截图了，截一张老戴视频里的）</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/image-20210812104025400.png" class="" title="image-20210812104025400">
<h4 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h4><p>死亡搁浅以自己独特的世界观讲了一个好故事，值回票价，如果你没那么重视玩法，喜欢剧情导向的游戏，那么死亡搁浅准没错，最后像吐槽一下，什么时候美利坚能像游戏里塑造的那么正直高大，我觉得不太可能</p>
<img src="/2021/08/12/%E6%9D%82%E6%96%87/%E6%AD%BB%E4%BA%A1%E6%90%81%E6%B5%85/Death_Stranding_2021_8_10_19_49_58.png" class="" title="Death_Stranding_2021_8_10 19_49_58.png">
]]></content>
      <categories>
        <category>杂文</category>
      </categories>
      <tags>
        <tag>玩后感</tag>
      </tags>
  </entry>
  <entry>
    <title>前缀（字典）树总结</title>
    <url>/2021/11/21/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%89%8D%E7%BC%80%EF%BC%88%E5%AD%97%E5%85%B8%EF%BC%89%E6%A0%91%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="什么是前缀树（Trie）"><a href="#什么是前缀树（Trie）" class="headerlink" title="什么是前缀树（Trie）"></a>什么是前缀树（Trie）</h3><blockquote>
<p>In <a href="https://en.wikipedia.org/wiki/Computer_science">computer science</a>, a <strong>trie</strong>, also called <strong>digital tree</strong> or <strong>prefix tree</strong>, is a type of <a href="https://en.wikipedia.org/wiki/Search_tree">search tree</a>, a <a href="https://en.wikipedia.org/wiki/Tree_(data_structure">tree</a>) <a href="https://en.wikipedia.org/wiki/Data_structure">data structure</a> used for locating specific keys from within a set. These keys are most often <a href="https://en.wikipedia.org/wiki/String_(computer_science">strings</a>), with links between nodes defined not by the entire key, but by individual <a href="https://en.wikipedia.org/wiki/Character_(computing">characters</a>). - wikepidea</p>
</blockquote>
<p>前缀树（又叫做字典树）是一种特殊类型的<strong>多叉树</strong>，每条边代表一个一个字母，每个节点代表一个字符串（前缀），该字符串由从根节点到当前节点路径字母组成，由于节点间的父子关系，父节点字符串就相当于子节点字符串的前缀，因此称为<strong>前缀树</strong>。</p>
<p>需要特殊注意的是前缀树的根节点由于没有父节点，代表<strong>空字符串</strong></p>

<span id="more"></span>
<h4 id="前缀树结构"><a href="#前缀树结构" class="headerlink" title="前缀树结构"></a>前缀树结构</h4><p>假设给定字符串中只有<strong>小写字母</strong>，则每个节点可能有26种类型边指向孩子节点，数据结构定义如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrieTreeNode</span></span>&#123;</span><br><span class="line">    # 长度为<span class="number">26</span>的子节点数组</span><br><span class="line">    TrieTreeNode[] children;</span><br><span class="line">    # 当前节点是否为一个单词（还能够存储以当前前缀为前缀的单词数量）</span><br><span class="line">    <span class="keyword">boolean</span> isWord;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>重点在于孩子节点数组的定义，其他属性可以根据具体问题设计</p>
<h4 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h4><ul>
<li><p>insert  向前缀树中插入一个单词</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">inert</span><span class="params">(String word)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 临时的初始节点</span></span><br><span class="line">    TrieTreeNode curNode = root;</span><br><span class="line">    <span class="comment">//将word转换成从根节点到(非)叶子节点的一条路</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; word.length();i++)&#123;</span><br><span class="line">        <span class="comment">//如果不存在当前前缀，添加</span></span><br><span class="line">        <span class="keyword">if</span>(curNode.children[word.charAt(i) - <span class="string">&#x27;a&#x27;</span>] == <span class="keyword">null</span>)&#123;</span><br><span class="line">            curNode.children[word.charAt(i) - <span class="string">&#x27;a&#x27;</span>]； = <span class="keyword">new</span> TrieTreeNode();</span><br><span class="line">        &#125;</span><br><span class="line">        curNode = curNode.children[word.charAt(i) - <span class="string">&#x27;a&#x27;</span>]；</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//标示当前词语</span></span><br><span class="line">    curNode.isWord = <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>search 在前缀树中查找一个单词或者前缀</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">search</span><span class="params">(String word)</span></span>&#123;</span><br><span class="line">    <span class="comment">//从根节点搜索</span></span><br><span class="line">    TrieTreeNode curNode = root;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; word.length();i++)&#123;</span><br><span class="line">        <span class="comment">//不包含当前字母</span></span><br><span class="line">        <span class="keyword">if</span>(curNode.children[word.charAt(i)] == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        curNode = curNode.children[word.charAt(i)];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//如果是前缀,可以直接返回</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    <span class="comment">//如果是找单词，需要查看节点标识符</span></span><br><span class="line">    <span class="keyword">return</span> curNode.isWord;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="几个典型例题"><a href="#几个典型例题" class="headerlink" title="几个典型例题"></a>几个典型例题</h4><ol>
<li>lc:<a href="https://leetcode-cn.com/problems/design-add-and-search-words-data-structure/">211. 添加与搜索单词 - 数据结构设计</a><ul>
<li>前缀树结构练习题</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>算法相关</category>
        <category>算法整理</category>
      </categories>
      <tags>
        <tag>前缀树（Trie）</tag>
      </tags>
  </entry>
  <entry>
    <title>并查集整理</title>
    <url>/2021/07/04/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%B9%B6%E6%9F%A5%E9%9B%86%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h4 id="什么是并查集"><a href="#什么是并查集" class="headerlink" title="什么是并查集"></a>什么是并查集</h4><blockquote>
<p>并查集是一种树型的数据结构，用于处理一些不相交集合的合并及查询问题。</p>
<p>并查集的思想是用一个数组表示了整片森林（parent），树的根节点唯一标识了一个集合，我们只要找到了某个元素的的树根，就能确定它在哪个集合里。</p>
<p>百度百科</p>
</blockquote>
<p>并查集包括两种操作：</p>
<ol>
<li>find(x) 查询元素所属的集合</li>
<li>union(x, y) 合并两个不相关的集合</li>
</ol>
<p>我理解的并查集，就是对于一系列元素，不同元素组成不同的集合（使用树的形式描述集合），多个集合共同构成并查集（森林），提供集合与集合的合并（两棵的合并）和 查找元素所属的集合（在哪棵树）<br><span id="more"></span></p>
<h4 id="如何实现并查集"><a href="#如何实现并查集" class="headerlink" title="如何实现并查集"></a>如何实现并查集</h4><p>并查集中只关注元素属于哪个集合，集合之间的合并操作，以树形式表示集合，每个元素只需要知道自己所属树的根节点就能知道自己属于哪个集合(属于哪个树)，集合合并也可转化为树的合并，因此可使用类似于完全二叉树的数组存储方式实现并查集。</p>
<p>存储结构实现：</p>
<ul>
<li><p>使用father数组，存储元素的父节点(不直接存储根节点的原因在于，合并操作无法保证该性质)</p>
</li>
<li><p>每个元素初始化父节点为本身，表示并查集初始每个元素自成一个集合</p>
<p><code>int father[elements];</code></p>
</li>
</ul>
<p>操作实现：</p>
<ol>
<li><p>find(x) 查询元素所属集合</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 不断的向上查询父节点，直到找到当前集合的根</span><br><span class="line">int find(x)&#123;</span><br><span class="line">	root = x;</span><br><span class="line">	while(father[root] != root)&#123;</span><br><span class="line">		parent = father[root];</span><br><span class="line">	&#125;</span><br><span class="line">	return root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>union 合并集合操作</p>
<ul>
<li>找到两个集合的根节点，将其中一个根节点父节点设置为另一集合根节点</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int uninon(x,y)&#123;</span><br><span class="line">	int root_x = find(x)</span><br><span class="line">	int root_y = find(y)</span><br><span class="line">	// 以x插入到y为例</span><br><span class="line">	parent[root_x] = root_y</span><br><span class="line">    return root_y</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li><p>java实现</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DisjointSetUnion</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span>[] parent;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DisjointSetUnion</span><span class="params">(<span class="keyword">int</span> nums)</span></span>&#123;</span><br><span class="line">        parent = <span class="keyword">new</span> <span class="keyword">int</span>[nums];</span><br><span class="line">        <span class="comment">//初始化</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; nums;i++)&#123;</span><br><span class="line">            parent[i] = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(parent[x] != x)&#123;</span><br><span class="line">            x = parent[x];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">union</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> parent_x = find(x);</span><br><span class="line">        <span class="keyword">int</span> parent_y = find(y);</span><br><span class="line">        parent[parent_x] = parent_y;</span><br><span class="line">        <span class="keyword">return</span> parent_y;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="并查集优化"><a href="#并查集优化" class="headerlink" title="并查集优化"></a>并查集优化</h4><h5 id="1-路径压缩降低查找集合标示的复杂度"><a href="#1-路径压缩降低查找集合标示的复杂度" class="headerlink" title="1.路径压缩降低查找集合标示的复杂度"></a>1.路径压缩降低查找集合标示的复杂度</h5><p>在find的过程中，直接将当前的根节点接到所在集合的根节点</p>
<ul>
<li>该方法只有在查询的过程中才会优化，且只优化树的一条路径</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(x)</span></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(father[x] == x)&#123;</span><br><span class="line">		<span class="keyword">return</span> x;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="comment">//同样的找父根节点，增加了赋值操作</span></span><br><span class="line">		father[x] = find(father[x]);</span><br><span class="line">        <span class="keyword">return</span> father[x];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//一行简写法</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x == father[x] ? x : (father[x] = find(fa[x]));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="2-按秩合并的方式"><a href="#2-按秩合并的方式" class="headerlink" title="2.按秩合并的方式"></a>2.按秩合并的方式</h5><p>出发点是把简单的树往复杂的树上合并，以减少合并增加的平均树深度，定义节点的秩为以当前节点为根子树的深度，开辟秩数组，每个节点对应一个秩。</p>
<p>初始化方法修改为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//所有的秩均初始化为1</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">	father = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">	rank = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; n;i++)&#123;</span><br><span class="line">		father[i] = i;</span><br><span class="line">		rank[i] = i;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>合并方法修改为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">union</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> x = find(i), y = find(j);    <span class="comment">//先找到两个根节点</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (rank[x] &lt;= rank[y])</span><br><span class="line">        fa[x] = y;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        fa[y] = x;</span><br><span class="line">    <span class="comment">//深度相同时，根节点深度+1,深度不同，插入子树不影响深度</span></span><br><span class="line">    <span class="keyword">if</span> (rank[x] == rank[y] &amp;&amp; x != y)</span><br><span class="line">        rank[y]++;                   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="并查集应用"><a href="#并查集应用" class="headerlink" title="并查集应用"></a>并查集应用</h4><ol>
<li><p>洛谷p1551 亲戚问题</p>
 <img src="/2021/07/04/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%B9%B6%E6%9F%A5%E9%9B%86%E6%95%B4%E7%90%86/image-20210509112121807.png" class="">
<ul>
<li><p>简答的并查集思路，求是否具有亲戚关系，即判断两元素是否在一个集合中</p>
</li>
<li><p>java代码实现:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Main</span></span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span>[] parent;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(parent[x] != x)&#123;</span><br><span class="line">            x = parent[x];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">union</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> parent_x = find(x);</span><br><span class="line">        <span class="keyword">int</span> parent_y = find(y);</span><br><span class="line">        parent[parent_x] = parent_y;</span><br><span class="line">        <span class="keyword">return</span> parent_y;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">int</span> n,m,p;</span><br><span class="line">        n = scanner.nextInt();</span><br><span class="line">        m = scanner.nextInt();</span><br><span class="line">        p = scanner.nextInt();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//初始化并查集</span></span><br><span class="line">        parent = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; parent.length;i++)&#123;</span><br><span class="line">            parent[i] = i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//读取关系合并并查集</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; m;i++)&#123;</span><br><span class="line">            union(scanner.nextInt() - <span class="number">1</span>,scanner.nextInt() - <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//判断两个集合是否在同一个集合中</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; p;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(find(scanner.nextInt() - <span class="number">1</span>) == find(scanner.nextInt() - <span class="number">1</span>))&#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;Yes&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;No&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>岛屿数量问题(LC <a href="https://leetcode-cn.com/problems/number-of-islands/">200. 岛屿数量</a>)</p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="comment">//并查集方法</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">DisjointUnion</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> count;</span><br><span class="line">        <span class="keyword">int</span>[] parents;</span><br><span class="line">        <span class="keyword">int</span>[] rank;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">DisjointUnion</span><span class="params">(<span class="keyword">char</span>[][] grid)</span></span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.count = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">this</span>.parents = <span class="keyword">new</span> <span class="keyword">int</span>[grid.length * grid[<span class="number">0</span>].length];</span><br><span class="line">            <span class="keyword">this</span>.parents = <span class="keyword">new</span> <span class="keyword">int</span>[grid.length * grid[<span class="number">0</span>].length];</span><br><span class="line">            <span class="keyword">this</span>.rank = <span class="keyword">new</span> <span class="keyword">int</span>[grid.length * grid[<span class="number">0</span>].length];</span><br><span class="line">            <span class="comment">//初始化集合，每个元素1自成一个集合</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; grid.length;i++)&#123;</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j &lt; grid[<span class="number">0</span>].length;j++)&#123;</span><br><span class="line">                    <span class="keyword">if</span>(grid[i][j] == <span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">                        <span class="keyword">this</span>.count++;</span><br><span class="line">                        <span class="comment">//集合标识为自己</span></span><br><span class="line">                        parents[i * grid[<span class="number">0</span>].length + j] = i * grid[<span class="number">0</span>].length + j;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">this</span>.rank[i * grid[<span class="number">0</span>].length + j] = <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(x != parents[x])&#123;</span><br><span class="line">                parents[x] = find(parents[x]);</span><br><span class="line">                x = parents[x];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> x;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">union</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">            <span class="keyword">int</span> parentX = find(x);</span><br><span class="line">            <span class="keyword">int</span> parentY = find(y);</span><br><span class="line">            <span class="comment">//是否为不同的集合</span></span><br><span class="line">            <span class="keyword">if</span>(parentX != parentY)&#123;</span><br><span class="line">                <span class="keyword">this</span>.count--;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//添加秩的合并操作</span></span><br><span class="line">                <span class="keyword">if</span>(rank[parentX] &lt;= rank[parentY])&#123;</span><br><span class="line">                    parents[parentX] = parentY;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span>&#123;</span><br><span class="line">                    parents[parentY] = parentX;</span><br><span class="line">                    <span class="keyword">if</span>(rank[parentX] == parentY)&#123;</span><br><span class="line">                        rank[parentY]++;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getCount</span><span class="params">()</span></span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">numIslands</span><span class="params">(<span class="keyword">char</span>[][] grid)</span> </span>&#123;</span><br><span class="line">        DisjointUnion disjointUnion = <span class="keyword">new</span> DisjointUnion(grid);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; grid.length;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j &lt; grid[<span class="number">0</span>].length;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(grid[i][j] == <span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">                    <span class="keyword">if</span>(i - <span class="number">1</span> &gt;= <span class="number">0</span> &amp;&amp; grid[i - <span class="number">1</span>][j] == <span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">                        disjointUnion.union(i * grid[<span class="number">0</span>].length + j, (i - <span class="number">1</span>) * grid[<span class="number">0</span>].length + j);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span>(j - <span class="number">1</span> &gt;= <span class="number">0</span> &amp;&amp; grid[i][j - <span class="number">1</span>] == <span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">                        disjointUnion.union(i * grid[<span class="number">0</span>].length + j, i * grid[<span class="number">0</span>].length + j - <span class="number">1</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span>(i + <span class="number">1</span> &lt; grid.length &amp;&amp; grid[i + <span class="number">1</span>][j] == <span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">                        disjointUnion.union(i * grid[<span class="number">0</span>].length + j, (i +<span class="number">1</span>) * grid[<span class="number">0</span>].length + j);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span>(j + <span class="number">1</span> &lt; grid[<span class="number">0</span>].length &amp;&amp; grid[i][j + <span class="number">1</span>] == <span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">                        disjointUnion.union(i * grid[<span class="number">0</span>].length + j, i * grid[<span class="number">0</span>].length + j + <span class="number">1</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> disjointUnion.getCount();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法相关</category>
        <category>算法整理</category>
      </categories>
      <tags>
        <tag>并查集</tag>
      </tags>
  </entry>
  <entry>
    <title>快速幂整理</title>
    <url>/2021/07/09/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%BF%AB%E9%80%9F%E5%B9%82%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h3 id="1-什么是快速幂"><a href="#1-什么是快速幂" class="headerlink" title="1 什么是快速幂"></a>1 什么是快速幂</h3><p>在求幂次操作时，一般采用逐个相乘的方式，求多少阶幂次，就需要进行多少次乘法，乘法的时间复杂度为<strong>O(N)</strong>，通过引入”备忘录“和二分思想，将乘法次数从 <script type="math/tex">N</script> 降低到 <script type="math/tex">log_2N</script></p>
<h4 id="主要思想："><a href="#主要思想：" class="headerlink" title="主要思想："></a>主要思想：</h4><ol>
<li>求 N幂次问题 转化为 求两个 N/2幂次的乘积</li>
<li>两个相同的 N/2幂次子问题，只需要求解一次<span id="more"></span>
<h4 id="1-1-递归伪代码："><a href="#1-1-递归伪代码：" class="headerlink" title="1.1 递归伪代码："></a>1.1 递归伪代码：</h4></li>
</ol>
<ul>
<li>递归实现较为简洁，但是会存在递归栈占用，递归树深度为<script type="math/tex">logN</script></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getPow</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(e == <span class="number">0</span>)&#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//计算子问题</span></span><br><span class="line">    <span class="keyword">int</span> temp = getPow(x, e / <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">int</span> result = temp * temp;</span><br><span class="line">    <span class="keyword">if</span>(e % <span class="number">2</span> != <span class="number">0</span>)&#123;</span><br><span class="line">        result *= x;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="1-2-迭代伪代码："><a href="#1-2-迭代伪代码：" class="headerlink" title="1.2 迭代伪代码："></a>1.2 <strong>迭代伪代码</strong>：</h4><p>观察递归代码，可以得到递归过程类似于将指数e转化为二进制的过程</p>
<ul>
<li>在底数转化二进制的某位为1时，对应递归子问题为奇数时乘以底数，</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if(e % 2 != 0)&#123;</span><br><span class="line">    result *= x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>在递归返回时，每向上返回一层 该位置乘的底数，都要平方一次</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> result = temp * temp;</span><br></pre></td></tr></table></figure>
<p>根据以上分析可得递归代码为</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getPow</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> e)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> result = <span class="number">1</span>;</span><br><span class="line">	<span class="keyword">while</span>(e &gt; <span class="number">0</span>)&#123;</span><br><span class="line">		<span class="keyword">if</span>(e % <span class="number">2</span> != <span class="number">0</span>)&#123;</span><br><span class="line">			result *= x;</span><br><span class="line">		&#125;</span><br><span class="line">        <span class="comment">//阶数向右移动</span></span><br><span class="line">        x *= x;</span><br><span class="line">        <span class="comment">//等价于从底向上递归</span></span><br><span class="line">		e &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="1-3-添加了避免越界的模板"><a href="#1-3-添加了避免越界的模板" class="headerlink" title="1.3 添加了避免越界的模板"></a>1.3 添加了避免越界的模板</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getPow</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> e)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> result = <span class="number">1</span>;</span><br><span class="line">	<span class="keyword">while</span>(e &gt; <span class="number">0</span>)&#123;</span><br><span class="line">		<span class="keyword">if</span>(e % <span class="number">2</span> != <span class="number">0</span>)&#123;</span><br><span class="line">			result = result * x mod number;</span><br><span class="line">		&#125;</span><br><span class="line">        <span class="comment">//阶数向右移动</span></span><br><span class="line">        x = x * x mod number;</span><br><span class="line">        <span class="comment">//等价于从底向上递归</span></span><br><span class="line">		e &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="1-4-快速幂扩展"><a href="#1-4-快速幂扩展" class="headerlink" title="1.4 快速幂扩展"></a>1.4 快速幂扩展</h4><p>快速幂思路不止可以应用在整数乘积上，同样可扩展到类似的矩阵乘积</p>
<h3 id="2-典型例题"><a href="#2-典型例题" class="headerlink" title="2 典型例题"></a>2 典型例题</h3><h4 id="2-1-洛谷-P3390-【模板】矩阵快速幂"><a href="#2-1-洛谷-P3390-【模板】矩阵快速幂" class="headerlink" title="2.1 洛谷 P3390 【模板】矩阵快速幂"></a>2.1 洛谷 P3390 【模板】<a href="https://www.luogu.com.cn/problem/solution/P3390">矩阵快速幂</a></h4><p>难点在于避免越界和矩阵乘法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="keyword">static</span> <span class="keyword">int</span> MOD_NUMBER = <span class="number">100000007</span>;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">long</span>[][] matMul(<span class="keyword">long</span>[][] mat1, <span class="keyword">long</span>[][] mat2)&#123;</span><br><span class="line">      <span class="keyword">if</span>(mat1 == <span class="keyword">null</span>)&#123;</span><br><span class="line">          <span class="keyword">return</span> mat2;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//乘积结果存储在result中</span></span><br><span class="line">      <span class="keyword">long</span>[][] result = <span class="keyword">new</span> <span class="keyword">long</span>[mat1.length][mat1.length];</span><br><span class="line">      <span class="keyword">long</span> temp;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; mat1.length;i++)&#123;</span><br><span class="line">          <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j &lt; mat1.length;j++)&#123;</span><br><span class="line">              temp = <span class="number">0</span>;</span><br><span class="line">              <span class="comment">//第i行与第j列相乘</span></span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> m = <span class="number">0</span>; m &lt; mat1.length;m++)&#123;</span><br><span class="line">                  <span class="comment">//避免越界的关键点</span></span><br><span class="line">                  temp  = (temp + mat1[i][m] * mat2[m][j] % MOD_NUMBER) % MOD_NUMBER;</span><br><span class="line">              &#125;</span><br><span class="line">              result[i][j] = temp;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">      Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">      <span class="keyword">int</span> n = scanner.nextInt();</span><br><span class="line">      Long k = scanner.nextLong();</span><br><span class="line">      <span class="keyword">long</span>[][] aimMatrix = <span class="keyword">new</span> <span class="keyword">long</span>[n][n];</span><br><span class="line">      <span class="comment">//读取matrix数组</span></span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; n;i++)&#123;</span><br><span class="line">          <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j &lt; n;j++)&#123;</span><br><span class="line">              aimMatrix[i][j] = scanner.nextLong();</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">long</span>[][] result = <span class="keyword">null</span>;</span><br><span class="line">      <span class="keyword">while</span>(k &gt; <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="keyword">if</span> (k % <span class="number">2</span> != <span class="number">0</span>) &#123;</span><br><span class="line">              result = matMul(result, aimMatrix);</span><br><span class="line">          &#125;</span><br><span class="line">          k &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">          aimMatrix = matMul(aimMatrix, aimMatrix);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//输出结果集合,省略</span></span><br><span class="line">System.....</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<img src="/2021/07/09/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%BF%AB%E9%80%9F%E5%B9%82%E6%95%B4%E7%90%86/image-20210706154401051.png" class="" title="image-20210706154401051">
<h4 id="2-2-力扣-1922-统计好数字的数目"><a href="#2-2-力扣-1922-统计好数字的数目" class="headerlink" title="2.2 力扣 1922. 统计好数字的数目"></a>2.2 力扣 <a href="https://leetcode-cn.com/problems/count-good-numbers/">1922. 统计好数字的数目</a></h4><p>难点在于将题目转化为求幂形式，<strong>通过归纳得到数组位数每增加两位，好数字数目*20</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">countGoodNumbers</span><span class="params">(<span class="keyword">long</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//设置初始化值</span></span><br><span class="line">        <span class="keyword">long</span> result = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(n % <span class="number">2</span> == <span class="number">1</span>)&#123;</span><br><span class="line">            result = <span class="number">5</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        n = n / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">long</span> a = <span class="number">20</span>;</span><br><span class="line">        <span class="comment">//快速幂模板</span></span><br><span class="line">        <span class="keyword">while</span>(n &gt; <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(n % <span class="number">2</span> == <span class="number">1</span>)&#123;</span><br><span class="line">                result = result * a % <span class="number">1000000007</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//右移一位</span></span><br><span class="line">            n &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">            a = a * a % <span class="number">1000000007</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> (<span class="keyword">int</span>)result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<img src="/2021/07/09/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%BF%AB%E9%80%9F%E5%B9%82%E6%95%B4%E7%90%86/image-20210706155441634.png" class="" title="image-20210706155441634">]]></content>
      <categories>
        <category>算法相关</category>
        <category>算法整理</category>
      </categories>
      <tags>
        <tag>快速幂</tag>
      </tags>
  </entry>
  <entry>
    <title>树状DP整理</title>
    <url>/2021/07/21/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%A0%91%E7%8A%B6DP%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h3 id="什么是树状DP"><a href="#什么是树状DP" class="headerlink" title="什么是树状DP"></a>什么是树状DP</h3><p>建立在 ”树“ 这一数据结构上的DP问题，难度介于线性dp和图dp之间，当前节点的状态可能取决于父亲节点或者孩子节点，即存在两种状态转移方向，树可能包括二叉树和多叉树。</p>
<p>难点还是如何找到状态转移方程</p>
<h4 id="主要题目类型"><a href="#主要题目类型" class="headerlink" title="主要题目类型"></a>主要题目类型</h4><ol>
<li><p>最大独立子集合问题</p>
<p>给一无向图，找出一个点集，使得任意两点之间都没有连边，这个点集就是独立集。而点最多的独立集，就是最大独立集，针对不问题，选取点的条件可能发生变化，但总体还是在限定条件下，选择最优的点集</p>
</li>
<li><p>最小点覆盖</p>
</li>
<li><p>最小支配集</p>
<span id="more"></span>
</li>
</ol>
<h3 id="典型例题"><a href="#典型例题" class="headerlink" title="典型例题"></a>典型例题</h3><h4 id="1-最大独立子集合问题-洛谷P1352-没有上司的舞会"><a href="#1-最大独立子集合问题-洛谷P1352-没有上司的舞会" class="headerlink" title="1. 最大独立子集合问题-洛谷P1352 没有上司的舞会"></a>1. 最大独立子集合问题-洛谷P1352 <a href="https://www.luogu.com.cn/problem/P1352">没有上司的舞会</a></h4><p>每个节点均有两个状态，当前节点参加和当前节点不参加的最优解，状态转移公式如下</p>
<script type="math/tex; mode=display">
dp_{in}[parent] = dp_{out}[child_1] + dp_{out}[child_1]+...+dp_{out}[child_n]</script><ul>
<li>如果选取父节点，孩子节点不能选取</li>
</ul>
<script type="math/tex; mode=display">
dp_{out}[parent] = max(dp_{out}[child_1],dp_{in}[child_1]) + ...+max(dp_{out}[child_n],dp_{in}[child_n])</script><ul>
<li>如果不选取父节点，子节点可选可不不选，本题选择两者较大</li>
</ul>
<h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span></span>&#123;</span><br><span class="line">        List&lt;TreeNode&gt; children;</span><br><span class="line">        <span class="keyword">int</span> happiness;</span><br><span class="line">        TreeNode(<span class="keyword">int</span> happiness)&#123;</span><br><span class="line">            <span class="keyword">this</span>.children = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="keyword">this</span>.happiness = happiness;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] dfs(TreeNode cur)&#123;</span><br><span class="line">        <span class="comment">//选择当前节点与不选择当前节点两种解</span></span><br><span class="line">        <span class="keyword">int</span>[] totalHappiness = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">2</span>];</span><br><span class="line">        totalHappiness[<span class="number">0</span>] = cur.happiness;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span>[] childHappiness;</span><br><span class="line">        <span class="keyword">for</span>(TreeNode child:cur.children)&#123;</span><br><span class="line">            childHappiness = dfs(child);</span><br><span class="line">            <span class="comment">//父亲选了，孩子不能选</span></span><br><span class="line">            totalHappiness[<span class="number">0</span>] += childHappiness[<span class="number">1</span>];</span><br><span class="line">            <span class="comment">//父亲没选，孩子可选可不选</span></span><br><span class="line">            totalHappiness[<span class="number">1</span>] += Math.max(childHappiness[<span class="number">0</span>],childHappiness[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> totalHappiness;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		<span class="comment">//建立二叉树的过程省略</span></span><br><span class="line">        <span class="comment">//....</span></span><br><span class="line">        <span class="comment">//dfs遍历</span></span><br><span class="line">        <span class="keyword">int</span>[] result = dfs(root);</span><br><span class="line">        System.out.println(Math.max(result[<span class="number">0</span>], result[<span class="number">1</span>]));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<img src="/2021/07/21/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%A0%91%E7%8A%B6DP%E6%95%B4%E7%90%86/image-20210721144347304.png" class="" title="image-20210721144347304">
<h4 id="2-洛谷P2015-二叉苹果树"><a href="#2-洛谷P2015-二叉苹果树" class="headerlink" title="2. 洛谷P2015 二叉苹果树"></a>2. 洛谷P2015 <a href="https://www.luogu.com.cn/problem/P2015">二叉苹果树</a></h4><p>假设求节点 <script type="math/tex">i</script> 保留 <script type="math/tex">j</script> 条边的最优情况，其状态转移公式如下</p>
<script type="math/tex; mode=display">
dp[i][j] = max(dp[i][j], dp[u][i - k - 1] + dp[v][k] + weight[i][v])</script><p>其中 <script type="math/tex">v</script> 为当前节点的某个子节点，<script type="math/tex">weight[i][v]</script> 为连接子节点的遍历的权重，需要遍历所有子节点以及所有保留边情况，计算得到当前节点的所有候选状态。</p>
<p><strong>难理解的几个点</strong></p>
<ol>
<li><p>为什么状态转移是 <script type="math/tex">dp[u][i - k - 1] + dp[v][k] + weight[i][v]</script></p>
<ul>
<li>我们通常理解的二叉树或者多叉树的子问题划分，一定是将保留的边分配给所有的子节点，看每个子节点最最优情况，最后求和为当前节点最优情况，以二叉树为例子，即 <script type="math/tex">dp[left][i - k - 1] + dp[right][k] + weight[left][v] + + weight[right][v]</script></li>
<li>该状态转移公式，并不从所有的子节点出发，而是将保留的边分配给当前已遍历的子节点，即默认未遍历子树中的边全部删除情况，每遍历到一个新的孩子节点，重新考虑当前子节点分配边的情况</li>
<li>形象的例子就是，分苹果给张三、李四、王五，第一种思路是直接把三个人叫过来，看如何分成三份；另一种是先把张三叫过来，记录下来 把苹果从 0-全部 给他的情况，再叫李四，看给张三后，剩下不同苹果的基础上，给李四 0-全部 的最优情况。最后把王五拉过来，看给张三李四分完剩下，给他分怎么最优。</li>
</ul>
<p>即一种是直接考虑所有的节点之间分配，另一种是分成 已遍历 + 未遍历，未遍历节点不断地加入已遍历集合。<strong>第二种思路适用范围更广</strong>，如果问题变为多叉或者不定叉树，第一种思路代码无法实现</p>
</li>
<li><p>遍历部分的实现，<strong>为什么要倒序DP</strong>，即保留边数从大到小</p>
<ul>
<li>第一层遍历，遍历当前节点保留不同边的情况，最大边数 <script type="math/tex">j</script> 取<strong>已遍历子节点子树中边的个数和</strong>与<strong>要求保留边个数</strong>的较小值，<strong>倒序DP</strong></li>
<li>第二层遍历，遍历当前新节点的所有保留边数情况，最大边数 <script type="math/tex">k</script> 取 <strong><script type="math/tex">j-1</script></strong> 与 <strong>当前子节点子树边数</strong>的较小值，<strong>顺序无所谓</strong></li>
</ul>
<p>划分为子问题后，需要与原数组的前部元素求和，所以求大时会用到小，如果正序，前部修改导致后部计算使用的新计算的状态</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = Math.min(childEdges[curNode], m);j &gt;= <span class="number">1</span>;j--)&#123;</span><br><span class="line">    <span class="comment">//k为当前子节点中保留边的个数</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> k = Math.min(j - <span class="number">1</span>, childEdges[i]);k &gt;= <span class="number">0</span>;k--)&#123;</span><br><span class="line">        dpArray[curNode][j] = Math.max(dpArray[curNode][j], dpArray[curNode][j - k - <span class="number">1</span>] + dpArray[i][k] + adjustMatrix[curNode][i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h5 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h5><p>基于dfs+邻接矩阵实现</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> curNode, <span class="keyword">int</span>[][] adjustMatrix,<span class="keyword">int</span>[][] dpArray,<span class="keyword">int</span>[] childEdges, <span class="keyword">int</span>[] visited, <span class="keyword">int</span> m)</span></span>&#123;</span><br><span class="line">        visited[curNode] = <span class="number">1</span>;</span><br><span class="line">        <span class="comment">//首先dfs子树，求子问题解并获得子结点个数</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; adjustMatrix.length;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(adjustMatrix[curNode][i] != -<span class="number">1</span> &amp;&amp; visited[i] != <span class="number">1</span>)&#123;</span><br><span class="line">                visited[i] = <span class="number">1</span>;</span><br><span class="line">                dfs(i, adjustMatrix, dpArray, childEdges, visited, m);</span><br><span class="line">                <span class="comment">//已遍历子树边数和</span></span><br><span class="line">                childEdges[curNode] += childEdges[i] + <span class="number">1</span>;</span><br><span class="line">                <span class="comment">//遍历当前节点保留不同数量树枝的解</span></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> j = Math.min(childEdges[curNode], m);j &gt;= <span class="number">1</span>;j--)&#123;</span><br><span class="line">                    <span class="comment">//k为当前子节点中保留边的个数</span></span><br><span class="line">                    <span class="keyword">for</span>(<span class="keyword">int</span> k = Math.min(j - <span class="number">1</span>, childEdges[i]);k &gt;= <span class="number">0</span>;k--)&#123;</span><br><span class="line">                        dpArray[curNode][j] = Math.max(dpArray[curNode][j], dpArray[curNode][j - k - <span class="number">1</span>] + dpArray[i][k] + adjustMatrix[curNode][i]);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="comment">// 节点和边的数量</span></span><br><span class="line">        <span class="keyword">int</span> m,n;</span><br><span class="line">        <span class="keyword">int</span>[][] adjustMatrix;</span><br><span class="line">        m = scanner.nextInt();</span><br><span class="line">        n = scanner.nextInt();</span><br><span class="line">        adjustMatrix = <span class="keyword">new</span> <span class="keyword">int</span>[m][m];</span><br><span class="line">        <span class="comment">//初始化矩阵</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; m;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j &lt; m;j++)&#123;</span><br><span class="line">                adjustMatrix[i][j] = -<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//读取边</span></span><br><span class="line">        <span class="keyword">int</span> tempNode1;</span><br><span class="line">        <span class="keyword">int</span> tempNode2;</span><br><span class="line">        <span class="keyword">int</span> tempWeight;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; m - <span class="number">1</span>;i++)&#123;</span><br><span class="line">            tempNode1 = scanner.nextInt() - <span class="number">1</span>;</span><br><span class="line">            tempNode2 = scanner.nextInt() - <span class="number">1</span>;</span><br><span class="line">            tempWeight = scanner.nextInt();</span><br><span class="line">            adjustMatrix[tempNode1][tempNode2] = tempWeight;</span><br><span class="line">            adjustMatrix[tempNode2][tempNode1] = tempWeight;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//dfs动态规划</span></span><br><span class="line">        <span class="keyword">int</span>[] visited = <span class="keyword">new</span> <span class="keyword">int</span>[m];</span><br><span class="line">        <span class="keyword">int</span>[] childEdges = <span class="keyword">new</span> <span class="keyword">int</span>[m];</span><br><span class="line">        <span class="keyword">int</span>[][] dpArray = <span class="keyword">new</span> <span class="keyword">int</span>[m][n + <span class="number">1</span>];</span><br><span class="line">        dfs(<span class="number">0</span>, adjustMatrix, dpArray, childEdges, visited, n);</span><br><span class="line">        System.out.println(dpArray[<span class="number">0</span>][n]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<img src="/2021/07/21/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%A0%91%E7%8A%B6DP%E6%95%B4%E7%90%86/image-20210719171204593.png" class="" title="image-20210719171204593">
]]></content>
      <categories>
        <category>算法相关</category>
        <category>算法整理</category>
      </categories>
      <tags>
        <tag>树状DP</tag>
      </tags>
  </entry>
  <entry>
    <title>线段树整理</title>
    <url>/2022/07/13/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E7%BA%BF%E6%AE%B5%E6%A0%91%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h2 id="线段树总结"><a href="#线段树总结" class="headerlink" title="线段树总结"></a>线段树总结</h2><p>学完树状数组，很难控制住自己不学线段树，线段树是一种可以在$O(logN)$ 时间复杂度内实现区间和单点操作的数据结构：</p>
<ol>
<li>单点修改/查询</li>
<li>区间修改/查询</li>
</ol>
<p>与树状数组的区别在于：树状数组只能执行 单点修改+区间查询 或 区间修改+单点查询 。线段树能够均能支持，但是复杂度的常数系数显著大于树状数组（ps:能用树状数组就用树状数组）</p>
<span id="more"></span>
<p><strong>线段树从何而来？<del>我的简单理解</del></strong></p>
<ol>
<li>对于区间修改和区间查询，最朴素的思想就是遍历区间元素逐个修改，然而朴素的复杂度是我们无法接受的，所以我们如何改进？自然而然想到通过空间换时间的想法，通过提前存储要修改查询的“区间”，降低修改和查询的复杂度。由此我们确定了要<strong>“存区间”</strong></li>
<li>然而如何 <strong>“存区间”</strong>？每次查询和修改的区间左端点和右端点都是任意的，不可能把所有可能的左右端点组成形成的区间存下来，根据区间之间的重叠性质，我们能够想到：既然无法存储所有的区间，不如存储一定数量不重叠的区间，只要这些区间能够拼接出所有的目标区间即可，问题就变成了怎么<strong>拆分区间</strong>？</li>
<li>如何<strong>设计存储的现成区间</strong>？每次寻找给定一个区间，寻找已经存储的现成区间进行拼接，我们的目标自然而然是 <strong>拆分的次数越少越少</strong>（最好是我直接存了这个区间），这个<strong>拆分次数</strong>最少，自然而然想到，对 最大区间进行二分拆分，以树的形式进行存储，这样拆分次数最坏就是存储树的高度（<del>这一步逻辑还是有点问题</del>）</li>
<li>最终我们得到了线段树这一个存储结构</li>
</ol>
<h3 id="线段树定义"><a href="#线段树定义" class="headerlink" title="线段树定义"></a>线段树定义</h3><p>如下图所示，线段树每个结点对应一个区间，每个长度不为1的区间的结点包括两个子节点（划分为两个子区间），叶子节点为长度为1的区间。</p>
<ul>
<li>每个结点维护区间信息，例如区间和、最值等</li>
<li>通过递归修改查询等可以快速实现区间操作</li>
<li>线段树每次分裂两个子节点区间长度缩小两倍，所以线段树理想情况下为完全二叉树，普通情况下为近似完全二叉树，所以即可以使用树的方式存储，也可使用完全二叉树数组形式村存储</li>
</ul>
<img src="/2022/07/13/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E7%BA%BF%E6%AE%B5%E6%A0%91%E6%95%B4%E7%90%86/image-20220625113233442.png" class="" title="image-20220625113233442">
<h4 id="建树"><a href="#建树" class="headerlink" title="建树"></a>建树</h4><p><strong>建树</strong>代码如下：</p>
<ul>
<li><p>时间复杂度和线段树结点个数,空间复杂度为栈深度</p>
</li>
<li><p>空间复杂度： $log_2n$</p>
</li>
<li>时间复杂度： $2^{log_2{n} + 1} - 1 \approx 4 n$ (若使用二叉树数组存储，空间直接开4 * n)</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">buildTree</span><span class="params">(<span class="keyword">int</span> curPos, <span class="keyword">int</span> left, <span class="keyword">int</span> right, <span class="keyword">int</span>[] aimArray)</span></span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 建立线段树（区间和）</span></span><br><span class="line"><span class="comment">     * 1. 空间复杂度：o(4 * N)</span></span><br><span class="line"><span class="comment">     * 2. 时间复杂度：o(4 * N)</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="keyword">if</span>(left == right)&#123;</span><br><span class="line">        <span class="comment">//当前区间长度为1，叶子节点</span></span><br><span class="line">        heapArray[heapPos] = aimArray[left];</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> mid = left + (right - left) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="comment">//区间分裂，左节点+右节点</span></span><br><span class="line">    buildTree(curPos * <span class="number">2</span>, left, mid, aimArray);</span><br><span class="line">    buildTree(curPos * <span class="number">2</span> + <span class="number">1</span>, mid + <span class="number">1</span>, right, aimArray);</span><br><span class="line">    <span class="comment">//当前区间和等于两个子区间和求和</span></span><br><span class="line">    heapArray[curPos] = heapArray[curPos * <span class="number">2</span>] + heapArray[curPos * <span class="number">2</span> + <span class="number">1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h4><p><strong>更新</strong>代码如下：</p>
<ul>
<li>若每次更新修改所有包含当前结点的区间，时间复杂度将远超目标复杂度，在实现过程中引入了”懒标记”</li>
<li>当修改某个结点时，不递归修改当前结点的孩子节点，而是将当前结点的”懒标记”，在下次修改或者查询涉及到当前节点，再进行“修改”（这里的修改时特殊的修改，即修改孩子结点，并将懒标记下推到孩子结点）- <strong>pushdown</strong></li>
<li>通过“懒标记”确保了，更新的<strong>时间复杂度为O(logN)</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">pushDown</span><span class="params">(<span class="keyword">int</span> heapPos, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(lazyLabel[heapPos] != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">int</span> mid = left + ((right - left) &gt;&gt; <span class="number">1</span>);</span><br><span class="line">        <span class="comment">//首先将积累的更新修改到孩子节点</span></span><br><span class="line">        heapArray[heapPos * <span class="number">2</span>] += (mid - left + <span class="number">1</span>) * lazyLabel[heapPos];</span><br><span class="line">        heapArray[heapPos * <span class="number">2</span> + <span class="number">1</span>] += (right - (mid + <span class="number">1</span>) + <span class="number">1</span>) * lazyLabel[heapPos];</span><br><span class="line">        <span class="comment">//孩子结点记录&quot;懒标签&quot;,下一次遍历到孩子结点继续下推</span></span><br><span class="line">        <span class="keyword">if</span>(mid != left)&#123;</span><br><span class="line">            lazyLabel[heapPos * <span class="number">2</span>] += lazyLabel[heapPos];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(mid + <span class="number">1</span> != right)&#123;</span><br><span class="line">            lazyLabel[heapPos * <span class="number">2</span> + <span class="number">1</span>] += lazyLabel[heapPos];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//清除当前结点的&quot;懒标签&quot;</span></span><br><span class="line">    lazyLabel[heapPos] = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">(<span class="keyword">int</span> heapPos, <span class="keyword">int</span> left, <span class="keyword">int</span> right, <span class="keyword">int</span> leftRange, <span class="keyword">int</span> rightRange, <span class="keyword">int</span> value)</span></span>&#123;</span><br><span class="line">    <span class="comment">//当更新范围大于当前范围时，直接修改，并记录懒标记</span></span><br><span class="line">    <span class="keyword">if</span>(leftRange &lt;= left &amp;&amp; rightRange &gt;= right)&#123;</span><br><span class="line">        heapArray[heapPos] += (right - left + <span class="number">1</span>) * value;</span><br><span class="line">        <span class="comment">//叶子节点不标记</span></span><br><span class="line">        <span class="keyword">if</span>(right - left != <span class="number">0</span>)&#123;</span><br><span class="line">            lazyLabel[heapPos] += value;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="comment">//下推（</span></span><br><span class="line">        pushDown(heapPos, left, right);</span><br><span class="line">        <span class="comment">//向下更新</span></span><br><span class="line">        <span class="keyword">int</span> mid = left + ((right - left) &gt;&gt; <span class="number">1</span>);</span><br><span class="line">        <span class="comment">//左节点区间交叉</span></span><br><span class="line">        <span class="keyword">if</span>(mid &gt;= leftRange)&#123;</span><br><span class="line">            update(heapPos * <span class="number">2</span>, left, mid, leftRange, rightRange, value);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//右节点区间交叉</span></span><br><span class="line">        <span class="keyword">if</span>(mid &lt;= rightRange)&#123;</span><br><span class="line">            update(heapPos * <span class="number">2</span> + <span class="number">1</span>, mid + <span class="number">1</span>, right, leftRange, rightRange, value);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//更新当前区间值</span></span><br><span class="line">        heapArray[heapPos] = heapArray[heapPos * <span class="number">2</span>] + heapArray[heapPos * <span class="number">2</span> + <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h4><p><strong>查询</strong>代码如下：</p>
<ul>
<li>会写更新就会写查询，而且查询比更新还要简单</li>
<li><strong>时间复杂度为O(logN)</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">query</span><span class="params">(<span class="keyword">int</span> heapPos, <span class="keyword">int</span> left, <span class="keyword">int</span> right, <span class="keyword">int</span> leftRange, <span class="keyword">int</span> rightRange)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(leftRange &lt;= left &amp;&amp; rightRange &gt;= right)&#123;</span><br><span class="line">        <span class="keyword">return</span> heapArray[heapPos];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//首先pushDown</span></span><br><span class="line">    pushDown(heapPos, left, right);</span><br><span class="line">    <span class="comment">//区间拆分</span></span><br><span class="line">    <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> mid = left + ((right - left) &gt;&gt; <span class="number">1</span>);</span><br><span class="line">    <span class="comment">//与左节点区间有交叉</span></span><br><span class="line">    <span class="keyword">if</span>(mid &gt;= leftRange)&#123;</span><br><span class="line">        result += query(heapPos * <span class="number">2</span>, left, mid, leftRange, rightRange);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//与右节点区间有交叉</span></span><br><span class="line">    <span class="keyword">if</span>(mid &lt; rightRange)&#123;</span><br><span class="line">        result += query(heapPos * <span class="number">2</span>, mid + <span class="number">1</span>, right, leftRange, rightRange);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="线段树例题"><a href="#线段树例题" class="headerlink" title="线段树例题"></a>线段树例题</h3><p>套模板</p>
<ol>
<li><a href="https://www.luogu.com.cn/problem/P2357">洛谷 P2357 守墓人</a> 状态为和</li>
<li><a href="https://www.luogu.com.cn/problem/P3870">洛谷 P3870 [TJOI2009] 开关</a> 状态为值为1的个数</li>
<li><a href="https://www.luogu.com.cn/problem/P3373">洛谷 P3373 【模板】线段树 2</a> 两种区间修改操作<ul>
<li>难点在于 lazylabel的设计</li>
</ul>
</li>
</ol>
<p>特殊区间状态</p>
<ol>
<li><p><a href="https://leetcode.cn/problems/the-skyline-problem/">lc 218. 天际线问题</a> 每个区间代表<strong>线段</strong>（不再是离散的点）</p>
<ul>
<li><p>扫描线+线段树，这里每个区间不再是每个点的集合，而是真正的线段</p>
<ol>
<li><p>建树修改，<strong>左区间的右端点与右区间的左端点相同</strong>（线段一分为二，不再是点一分为二）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">//区间分裂，左节点+右节点</span></span><br><span class="line">buildTree(curPos * <span class="number">2</span>, left, mid, aimArray);</span><br><span class="line">buildTree(curPos * <span class="number">2</span> + <span class="number">1</span>, mid, right, aimArray);</span><br></pre></td></tr></table></figure>
</li>
<li><p>更新/查询修改，去掉了等号（线段相交，端点重合不叫线段相交）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(mid &gt; leftRange)&#123;</span><br><span class="line">    update(heapPos * <span class="number">2</span>, left, mid, leftRange, rightRange, value);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//右节点区间交叉</span></span><br><span class="line"><span class="keyword">if</span>(mid &lt; rightRange)&#123;</span><br><span class="line">    update(heapPos * <span class="number">2</span> + <span class="number">1</span>, mid, right, leftRange, rightRange, value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>好玩的一点，这个题目<strong>不需要pushdown</strong>（因为查询只查根节点不需要pushdown，且lazylabel记录覆盖次数无法pushdown）</p>
</li>
</ul>
</li>
<li><p><a href="https://www.luogu.com.cn/problem/P2003">P2003 [CRCI2007-2008] PLATFORME 平板</a></p>
<ul>
<li>与上一道题类似，更简单，都是存储<strong>线段</strong></li>
</ul>
</li>
</ol>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a href="https://oi-wiki.org/ds/seg/">OIWIKI 线段树</a></li>
<li><a href="https://baike.baidu.com/item/%E7%BA%BF%E6%AE%B5%E6%A0%91/10983506?fr=aladdin">百度百科：线段树</a></li>
<li><a href="https://www.luogu.com.cn/training/3079">洛谷：线段树模板题</a></li>
</ol>
]]></content>
      <categories>
        <category>算法相关</category>
        <category>算法整理</category>
      </categories>
      <tags>
        <tag>线段树算法</tag>
      </tags>
  </entry>
  <entry>
    <title>树状数组整理</title>
    <url>/2022/04/10/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h2 id="树状数组（Binary-indexed-tree）"><a href="#树状数组（Binary-indexed-tree）" class="headerlink" title="树状数组（Binary indexed tree）"></a>树状数组（Binary indexed tree）</h2><blockquote>
<p>A <strong>Fenwick tree</strong> or <strong>binary indexed tree</strong> is a data structure that can efficiently update elements and calculate <a href="https://en.wikipedia.org/wiki/Prefix_sum">prefix sums</a> in a table of numbers.</p>
<p>来自Wikipedia</p>
</blockquote>
<p>树状数组是一种求解前缀和问题的简单数据结构，能够在<strong>O(logn)</strong>的时间复杂度内解决区间求和问题，主要支持两种操作</p>
<ol>
<li><strong>单点修改</strong></li>
<li><strong>区间操作</strong></li>
</ol>
<h3 id="主要思路"><a href="#主要思路" class="headerlink" title="主要思路"></a>主要思路</h3><p>类比<strong>任何数</strong>均可以拆分为<strong>有限个不同2的幂次</strong>的求和，原论文作者认为<strong>一串序列</strong>的求和同样可以拆分为<strong>有限个不同长度为2的幂次的序列</strong>的和。在数字的拆分中，其二进制表示中1的个数即为不同2的幂次，例如 “10”，“12”</p>
<script type="math/tex; mode=display">
10 -> 1010 = 1000 + 0010\\
14 -> 1110 = 1000 + 0100 + 0010</script><span id="more"></span>
<p>自然想到一个问题：如何快速的找到<strong>任意数的不同二次幂的组合</strong>？引出了<strong>lowbit思路</strong>:</p>
<ol>
<li><p>通过<strong>原码与补码求逻辑与运算</strong>，求得目标数字的<strong>最右边的一个1</strong></p>
<script type="math/tex; mode=display">
lowbit(x) = (x)\&(-x)</script></li>
<li><p>原数字减去lowbit，获得少了一位1的数字</p>
<script type="math/tex; mode=display">
x_i = x_{i-1} - lowbit(x_{i-1})</script></li>
<li><p>重复操作，直到 $x_i = 0$ </p>
</li>
</ol>
<p>重复此过程每一步计算得到的 $lowbit(x_i)$ 即为原数字的不同2的幂次拆分，以14为例:</p>
<script type="math/tex; mode=display">
lowbit(14) = 0010 \\
\downarrow \\
lowbit(14 - 0010 = 12) = 0100\\
\downarrow \\
lowbit(12 - 0100 = 8) = 1000 \\
\downarrow \\
lowbit(0 - 1000 = 0) = 0000</script><h4 id="如何将对应拆分思路迁移到前缀和？"><a href="#如何将对应拆分思路迁移到前缀和？" class="headerlink" title="如何将对应拆分思路迁移到前缀和？"></a>如何将对应拆分思路迁移到前缀和？</h4><p>不妨用 $C[i]$ 表示 目标数组$f[1]…f[i]$的前缀和，通过$lowerbit$操作，我们可以快速找到<strong>任意区间长度</strong> 对应的<strong>二次幂区间长度的组合</strong>。</p>
<p>问题是我们如何设计存储方式，提前存储这些二次幂区间长度的区间和？观察14转化为组合的过程，对应到区间表达形式为:</p>
<script type="math/tex; mode=display">
(0,14] = (12, 14] + (8, 12] + (0, 8] \\
\updownarrow \\
(0,14] = (14 - lowbit(14), 14] + (12 - lowbit(12), 12] + (8 - lowbit(8), 8] \\
\updownarrow \\
(0,1110] = (1100, 1110] + (1000, 1100] + (0000, 1000]\\</script><p>可以得到每个区间右边界可以由上个区间右边界减去lobit得到，而区间的长度等于当前区间右边界的lowbit（<strong>拆分区间的递推关系</strong>），自然可以想到，将区间$(i - lowbit(i),i]$的和存储到 <strong>index = i</strong>的位置</p>
<ul>
<li>因此定义$tree$数组，对于<strong>任意$tree[i]$，其存储范围为 $(i-lowbit(i),i]$区间的和</strong>，如下：</li>
</ul>
<script type="math/tex; mode=display">
tree[i] = \sum^i_{j=i-lowbit(i) + 1}f[j]</script><p>任意前缀和 $C[i]$计算，按照上述拆分过程，可以转化为不断的减去最右边一的过程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for(int index = i;index &gt; 0;index -= lowbit(index)):</span><br><span class="line">	//index -= lowbit(index) 就是类似于14拆分过程中的找所有2次幂过程</span><br><span class="line">	C[i] += tree[index]</span><br></pre></td></tr></table></figure>
<h4 id="如何支持单点修改"><a href="#如何支持单点修改" class="headerlink" title="如何支持单点修改"></a>如何支持单点修改</h4><p>若修改 $f[i]$，需要修改所有包含$f[i]$ 的 $tree[index]$, 我们要找到所有包含当前元素的区间，假设 $tree[idx]$ 包含 $f[i]$，则 $i$一定满足:</p>
<script type="math/tex; mode=display">
idx - lowbit(idx) < i <= idx</script><p>来自博客<a href="http://duanple.blog.163.com/blog/static/7097176720081131113145832/"><strong>Binary indexed tree-树状数组</strong> </a>的形象解释</p>
<img src="/2022/04/10/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84%E6%95%B4%E7%90%86/image-20220405190706777.png" class="" title="image-20220405190706777">
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="lowbit-函数"><a href="#lowbit-函数" class="headerlink" title="lowbit()函数"></a>lowbit()函数</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">lowbit</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">	<span class="keyword">return</span> x &amp; -x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="求前缀和"><a href="#求前缀和" class="headerlink" title="求前缀和"></a>求前缀和</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//预先处理好的子区间和数组</span></span><br><span class="line"><span class="keyword">int</span> tree[maxIndex]</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">query</span><span class="params">(<span class="keyword">int</span> index)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span>(;index &gt; <span class="number">0</span>;index++)&#123;</span><br><span class="line">		result += tree[index];</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="更新方法"><a href="#更新方法" class="headerlink" title="更新方法"></a>更新方法</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//预先处理好的子区间和数组</span></span><br><span class="line"><span class="keyword">int</span> tree[maxIndex]</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">update</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">int</span> val)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> pos = index; pos &lt; maxIndex; pos += lowbit(pos))</span><br><span class="line">        <span class="comment">//以加法为例</span></span><br><span class="line">        tree[pos] += x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="初始化树状数组"><a href="#初始化树状数组" class="headerlink" title="初始化树状数组"></a>初始化树状数组</h4><p>wekipedia上的一个从数组[1,2,3,4,5]构建树状数组的过程：</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/dc/BITDemo.gif/220px-BITDemo.gif" alt="BITDemo.gif"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> tree[maxIndex]</span><br><span class="line"><span class="comment">//待求和数组</span></span><br><span class="line"><span class="keyword">int</span> aim[maxIndex]</span><br><span class="line"><span class="comment">//1.调用update方法（复杂度O(nlogn)）</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i &lt; maxIndex;i++)&#123;</span><br><span class="line">    update(i, aim[i]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.前缀和法(tree[i] = preSum[i] - preSum[i - lowbit(i)])</span></span><br><span class="line"><span class="comment">// 	复杂度O(n)</span></span><br><span class="line"><span class="keyword">int</span> preSum[maxIndex];</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i &lt; maxIndex;i++)&#123;</span><br><span class="line">    preSum[i] = preSum[i - <span class="number">1</span>] + aim[i];</span><br><span class="line">    tree[i] = preSum[i] - preSum[i - lowbit(i)];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="模板题目"><a href="#模板题目" class="headerlink" title="模板题目"></a>模板题目</h3><ol>
<li><a href="https://www.luogu.com.cn/problem/P3374">luogu P3374 :树状数组1</a><ul>
<li>使用scanner做输入会出现三个测试用例TLE<strong>（坑爹）</strong></li>
</ul>
</li>
<li><a href="https://www.luogu.com.cn/problem/P3368">luogu P3368 :树状数组2</a><ul>
<li>通过<strong>差分</strong>的思路，将树状数组支持的操作变为<ul>
<li><strong>单点修改-&gt;区间修改</strong></li>
<li><strong>区间查询-&gt;单点查询</strong></li>
</ul>
</li>
</ul>
</li>
<li><a href="https://www.luogu.com.cn/problem/P2880">luogu P2880 ：牛飞盘比赛</a><ul>
<li>树状数组维护值从 <strong>区间和</strong> 变为 <strong>区间内最值</strong></li>
<li>update操作变为最大值更新，查询操作递归完成</li>
</ul>
</li>
</ol>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ol>
<li><a href="https://zh.wikipedia.org/wiki/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84">wikipidia：树状数组</a></li>
<li><a href="http://duanple.blog.163.com/blog/static/7097176720081131113145832/">博客：Binary indexed tree-树状数组</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/93795692">博客：算法学习笔记(2) : 树状数组</a></li>
<li><a href="https://www.luogu.com.cn/training/3079">树状数组模板题</a></li>
</ol>
]]></content>
      <categories>
        <category>算法相关</category>
        <category>算法整理</category>
      </categories>
      <tags>
        <tag>树状数组</tag>
      </tags>
  </entry>
  <entry>
    <title>DataLoaders和DataSets使用总结</title>
    <url>/2021/07/09/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/DataLoaders%E5%92%8CDataSets%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="DataLoaders和DataSets使用总结"><a href="#DataLoaders和DataSets使用总结" class="headerlink" title="DataLoaders和DataSets使用总结"></a>DataLoaders和DataSets使用总结</h1><p>通过 <code>torch.utils.data.Dataset</code>类定义数据集，通过<code>torch.utils.data.DataLoader</code>与<code>Dataset</code>配合定义数据加载方式</p>
<p><code>torch.utils.data.Dataset</code>主要参数:</p>
<ol>
<li>dataset 指定构造的数据集，一般是数据+label的元组</li>
<li>shuffle 指定是否打乱</li>
<li>sampler 指定采样器，采取某种方式遍历数据（顺序，随机等）</li>
<li>collate_fn (callable, optional):将数据形成一个batch的tensor（在某些时候需要自定义）<span id="more"></span>
以加载minist数据集为例，基本的使用流程：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先加载数据集（pytorch现有的数据集） 返回的是DataSet对象</span></span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"><span class="comment"># 然后构造DataLoader对象 返回的是可迭代的DataLoader对象</span></span><br><span class="line">train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 通过for循环遍历进行训练</span></span><br><span class="line"><span class="keyword">for</span> batch_features,batch_labels <span class="keyword">in</span> train_iter:</span><br></pre></td></tr></table></figure>
<p>基本遍历原理</p>
<ul>
<li>通过sampler抽样index(默认为顺序抽样)，根据抽出的index从dataset中获取元素（getitem），调用自身的collate_fn方法，将原始数据转化为batch形式的tensor，返回。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># dataloader遍历的next方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__next__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.num_workers == <span class="number">0</span>:  </span><br><span class="line">            indices = <span class="built_in">next</span>(self.sample_iter)  <span class="comment"># Sampler</span></span><br><span class="line">            batch = self.collate_fn([self.dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices]) <span class="comment"># Dataset</span></span><br><span class="line">            <span class="keyword">if</span> self.pin_memory:</span><br><span class="line">                batch = _utils.pin_memory.pin_memory_batch(batch)</span><br><span class="line">            <span class="keyword">return</span> batch</span><br><span class="line"><span class="comment"># dataset 以[] 形式访问的方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br></pre></td></tr></table></figure>
<h2 id="1-自定义数据集"><a href="#1-自定义数据集" class="headerlink" title="1. 自定义数据集"></a>1. 自定义数据集</h2><h4 id="1-1-继承dataset类"><a href="#1-1-继承dataset类" class="headerlink" title="1.1 继承dataset类"></a>1.1 继承dataset类</h4><p>需要重写<code>init(),len(),getitem()</code>方法，以word2vec中负采样数据集生成为例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义数据读取类，可结合dataloader进行数据批量读取</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span>(<span class="params">torch.utils.data.Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, centers, contexts, negatives</span>):</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(centers) == <span class="built_in">len</span>(contexts) == <span class="built_in">len</span>(negatives)</span><br><span class="line">        self.centers = centers</span><br><span class="line">        self.contexts = contexts</span><br><span class="line">        self.negatives = negatives</span><br><span class="line">	<span class="comment"># 返回当前行数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> (self.centers[index], self.contexts[index], self.negatives[index])</span><br><span class="line">    <span class="comment"># 返回数据长度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.centers)</span><br></pre></td></tr></table></figure>
<h4 id="1-2-定义collate-fn方法"><a href="#1-2-定义collate-fn方法" class="headerlink" title="1.2 定义collate_fn方法"></a>1.2 定义collate_fn方法</h4><p>默认的collate_fn方法，必须保证每个训练数据的长度相同，通过自定义collate_fn方法解决这一问题</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_collate_fn</span>(<span class="params">data</span>):</span></span><br><span class="line">	<span class="comment"># 保证所有数据一致的统一长度</span></span><br><span class="line">    max_len = <span class="number">500</span></span><br><span class="line">    center_words = []</span><br><span class="line">    context_negatives = []</span><br><span class="line">    masks = []</span><br><span class="line">    labels = []</span><br><span class="line">    <span class="keyword">for</span> center, contexts, negatives <span class="keyword">in</span> data:</span><br><span class="line">        cur_len = <span class="built_in">len</span>(contexts) + <span class="built_in">len</span>(negatives)</span><br><span class="line">        center_words.append([center])</span><br><span class="line">        <span class="comment"># 对数据进行截断或者延长</span></span><br><span class="line">        <span class="keyword">if</span> cur_len &gt; max_len:</span><br><span class="line">        	context_negatives.append((contexts + negatives)[<span class="number">0</span>:max_len])</span><br><span class="line">        	labels.append([<span class="number">1</span>] * <span class="built_in">len</span>(contexts) + [<span class="number">0</span>] * (max_len - <span class="built_in">len</span>(contexts)))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">        	context_negatives.append(contexts + negatives + [<span class="number">0</span>] * (max_len - cur_len))</span><br><span class="line">        	labels.append([<span class="number">1</span>] * <span class="built_in">len</span>(contexts) + [<span class="number">0</span>] * (max_len - <span class="built_in">len</span>(contexts)))</span><br><span class="line">    <span class="keyword">return</span>  torch.tensor(center_words), torch.tensor(context_negatives)</span><br><span class="line"><span class="comment"># 使用即可</span></span><br><span class="line">train_iter = Data.DataLoader(MyDataset(all_centers, all_contexts, all_negatives), collate_fn = batchify, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="2-自定义sampler"><a href="#2-自定义sampler" class="headerlink" title="2.自定义sampler"></a>2.自定义sampler</h2><p>待补充</p>
]]></content>
      <categories>
        <category>AI编程</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch Scatter和gather函数使用</title>
    <url>/2021/07/09/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/Scatter%E5%92%8Cgather%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h3 id="Scatter函数"><a href="#Scatter函数" class="headerlink" title="Scatter函数"></a>Scatter函数</h3><blockquote>
<p>scatter_(<em>dim</em>, <em>index</em>, <em>src</em>, <em>reduce=None</em>) → Tensor</p>
<p>Writes all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor. For each value in <code>src</code>, its output index is specified by its index in <code>src</code> for <code>dimension != dim</code> and by the corresponding value in <code>index</code> for <code>dimension = dim</code>.</p>
</blockquote>
<p>简单理解就是将 src 张量中的元素散落到 self 张量中，具体选择哪个元素，选择的元素散落到哪个位置由index张量决定，具体的映射规则为:</p>
<figure class="highlight plaintext"><figcaption><span>三维张量为例</span></figcaption><table><tr><td class="code"><pre><span class="line"># 其中 i,j,k 为index张量中元素坐标。</span><br><span class="line">self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0</span><br><span class="line">self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1</span><br><span class="line">self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><ul>
<li><strong>dim(int)</strong> 指index数组元素替代的坐标（dim = 0 替代src中的横坐标）</li>
<li><strong>index (LongTensor)</strong> 可以为空，最大与src张量形状相同</li>
<li><strong>src(Tensor or float)</strong> 源张量</li>
<li><strong>reduce</strong> 聚集函数(src替换元素与self中被替换元素执行的操作，默认是替代，可以进行add,multiply等操作)</li>
</ul>
<p>具体例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>src = torch.arange(<span class="number">1</span>, <span class="number">11</span>).reshape((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>src</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>index = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros(<span class="number">3</span>, <span class="number">5</span>, dtype=src.dtype).scatter_(<span class="number">0</span>, index, src)</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure>
<p>以上例子中scatter函数执行的操作等价于：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 遍历index，在dim = 0 时,替换i</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(index.shape[<span class="number">0</span>]):</span><br><span class="line">	<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(index.shape[<span class="number">1</span>]):</span><br><span class="line">		self[index[i][j]][j] = src[i][j]</span><br></pre></td></tr></table></figure>
<h3 id="gather-函数"><a href="#gather-函数" class="headerlink" title="gather() 函数"></a>gather() 函数</h3><blockquote>
<p>Gathers values along an axis specified by dim.</p>
<p>沿着某条轴对元素进行聚集</p>
</blockquote>
<p>scatter的逆操作，挑选源张量的某些元素，放置到新张量中，具体映射规则与scatter类似：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out[i][j][k] = <span class="built_in">input</span>[index[i][j][k]][j][k]  <span class="comment"># if dim == 0</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][index[i][j][k]][k]  <span class="comment"># if dim == 1</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][j][index[i][j][k]]  <span class="comment"># if dim == 2</span></span><br></pre></td></tr></table></figure>
<h4 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h4><ul>
<li><strong>input</strong> 输入张量</li>
<li><strong>dim</strong> 聚集的轴</li>
<li><strong>index</strong> 聚集的index张量</li>
</ul>
<p>具体例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.gather(t, <span class="number">1</span>, torch.tensor([[<span class="number">0</span>, <span class="number">1</span>]]))</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>
<p>以上例子中gather()函数等价于scatter()函数的操作:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 遍历index，在dim = 1 时,替换j</span><br><span class="line">for i in range(index.shape[0]):</span><br><span class="line">	for j in range(index.shape[1]):</span><br><span class="line">		result[i][j] = input[i][index[i][j]]</span><br></pre></td></tr></table></figure>
<h3 id="scatter-vs-gather"><a href="#scatter-vs-gather" class="headerlink" title="scatter vs gather"></a>scatter vs gather</h3><p>相同点:</p>
<ul>
<li>scatter 和 gather 函数都是根据index数组从 src/input 源张量中选取指定元素</li>
</ul>
<p>不同点：</p>
<ul>
<li>scatter选取元素放置到目标张量中，放置位置由<code>index[i][j]</code>决定</li>
<li>gather选取元素放置组成新的张量，选取位置由<code>index[i][j]</code>决定</li>
</ul>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><h4 id="1-使用scatter函数将向量转化为one-hot形式"><a href="#1-使用scatter函数将向量转化为one-hot形式" class="headerlink" title="1. 使用scatter函数将向量转化为one-hot形式"></a>1. 使用scatter函数将向量转化为one-hot形式</h4><p>以转化为<code>n x 10</code>的n个10维行one-hot向量为例子:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([<span class="number">9</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t.view(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">tensor([[<span class="number">9</span>],</span><br><span class="line">        [<span class="number">8</span>],</span><br><span class="line">        [<span class="number">9</span>],</span><br><span class="line">        [<span class="number">5</span>],</span><br><span class="line">        [<span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>one_hot = torch.zeros(t.shape[<span class="number">0</span>],<span class="number">10</span>).scatter(<span class="number">1</span>,t.view(-<span class="number">1</span>,<span class="number">1</span>),<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ont_hot</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
<p>使用argmax转化回去:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; one_hot.argmax(dim = 1,keepdim = True)</span><br><span class="line">tensor([[9],</span><br><span class="line">        [8],</span><br><span class="line">        [9],</span><br><span class="line">        [5],</span><br><span class="line">        [6],</span><br><span class="line">        [7]])</span><br></pre></td></tr></table></figure>
<h3 id="待补充"><a href="#待补充" class="headerlink" title="待补充"></a>待补充</h3>]]></content>
      <categories>
        <category>AI编程</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>model_selection模块进行交叉验证</title>
    <url>/2021/07/09/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/model-selection%E6%A8%A1%E5%9D%97%E8%BF%9B%E8%A1%8C%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/</url>
    <content><![CDATA[<p>为了避免模型过拟合，通过添加<strong>验证集</strong>，模型训练完成后使用<strong>验证集</strong>验证模型的效果后，再对测试集进行测试。</p>
<h3 id="训练集和测试集划分"><a href="#训练集和测试集划分" class="headerlink" title="训练集和测试集划分"></a>训练集和测试集划分</h3><p>使用train_test_split函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">&gt;&gt; x_data, y_data = datasets.load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 划分测试集和训练集</span></span><br><span class="line">&gt;&gt; x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = <span class="number">0.2</span>, random_state = <span class="number">42</span>)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(x_train.shape)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(x_test.shape)</span><br><span class="line">(<span class="number">120</span>, <span class="number">4</span>)</span><br><span class="line">(<span class="number">30</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">&gt;&gt; clf = svm.SVC(kernel=<span class="string">&#x27;linear&#x27;</span>, C=<span class="number">1</span>).fit(X_train, y_train)</span><br><span class="line">&gt;&gt; clf.score(X_test, y_test)</span><br><span class="line"><span class="number">1.0</span></span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h3 id="cross-validation-交叉验证"><a href="#cross-validation-交叉验证" class="headerlink" title="cross validation 交叉验证"></a>cross validation 交叉验证</h3><p><img src="sklearn model_selection模块进行交叉验证/image-20210709165817836.png" alt="image-20210709165817836"></p>
<h4 id="cross-val-score函数进行模型交叉验证评价"><a href="#cross-val-score函数进行模型交叉验证评价" class="headerlink" title="cross_val_score函数进行模型交叉验证评价"></a>cross_val_score函数进行模型交叉验证评价</h4><p>简单使用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">&gt;&gt; clf = svm.SVC(kernel=<span class="string">&#x27;linear&#x27;</span>, C=<span class="number">1</span>, random_state=<span class="number">42</span>)</span><br><span class="line">&gt;&gt; scores = cross_val_score(clf, x_data, y_data, cv = <span class="number">5</span>)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(scores)</span><br><span class="line">[<span class="number">0.96666667</span> <span class="number">1.</span>         <span class="number">0.96666667</span> <span class="number">0.96666667</span> <span class="number">1.</span>        ]</span><br></pre></td></tr></table></figure>
<p>通过scoring参数，自定义评估函数，如果不显示指定，则调用estimator的默认score函数</p>
<p><img src="sklearn model_selection模块进行交叉验证/image-20210604090841624.png" alt="image-20210604090841624"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; scores = cross_val_score(clf, x_data, y_data, cv = <span class="number">5</span>, scoring = <span class="string">&#x27;f1_macro&#x27;</span>)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(scores)</span><br><span class="line">[<span class="number">0.96658312</span> <span class="number">1.</span>         <span class="number">0.96658312</span> <span class="number">0.96658312</span> <span class="number">1.</span>        ]</span><br></pre></td></tr></table></figure>
<p>cv参数不止可以传入k折的折数，还可以传入<strong>cross-validation generator or an iterable</strong>等</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 传入交叉验证生成器</span></span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit</span><br><span class="line">&gt;&gt; cv = ShuffleSplit(n_splits=<span class="number">5</span>, test_size=<span class="number">0.2</span>, random_state= <span class="number">42</span>)</span><br><span class="line">&gt;&gt; cross_val_score(clf, x_data, y_data, cv=cv)</span><br><span class="line">array([<span class="number">1.</span>        , <span class="number">1.</span>        , <span class="number">0.96666667</span>, <span class="number">0.93333333</span>, <span class="number">0.96666667</span>])</span><br><span class="line"><span class="comment"># 传入index生成迭代器（教程代码）</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">custom_cv_2folds</span>(<span class="params">X</span>):</span></span><br><span class="line"><span class="meta">... </span>    n = X.shape[<span class="number">0</span>]</span><br><span class="line"><span class="meta">... </span>    i = <span class="number">1</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">while</span> i &lt;= <span class="number">2</span>:</span><br><span class="line"><span class="meta">... </span>        idx = np.arange(n * (i - <span class="number">1</span>) / <span class="number">2</span>, n * i / <span class="number">2</span>, dtype=<span class="built_in">int</span>)</span><br><span class="line"><span class="meta">... </span>        <span class="keyword">yield</span> idx, idx</span><br><span class="line"><span class="meta">... </span>        i += <span class="number">1</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>custom_cv = custom_cv_2folds(X)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cross_val_score(clf, X, y, cv=custom_cv)</span><br><span class="line">array([<span class="number">1.</span>        , <span class="number">0.973</span>...])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>AI编程</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>pytroch如何对梯度追踪张量进行inplace操作</title>
    <url>/2021/07/09/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/pytorch%E5%A6%82%E4%BD%95%E5%AF%B9%E6%A2%AF%E5%BA%A6%E8%BF%BD%E8%B8%AA%E5%BC%A0%E9%87%8F%E8%BF%9B%E8%A1%8Cinplace%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h3 id="如何对梯度追踪张量进行inplace操作"><a href="#如何对梯度追踪张量进行inplace操作" class="headerlink" title="如何对梯度追踪张量进行inplace操作"></a>如何对梯度追踪张量进行inplace操作</h3><blockquote>
<p>别忘了梯度追踪张量必须为float</p>
<p>Only Tensors of floating point and complex dtype can require gradients</p>
</blockquote>
<h4 id="问题来源"><a href="#问题来源" class="headerlink" title="问题来源"></a>问题来源</h4><p>当对设置了requires_grad=True的张量进行原地操作时，pytorch会抛出运行错误：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt; X = torch.rand((3,4),requires_grad = True)</span><br><span class="line">&gt;&gt; X.fill_(0)</span><br><span class="line">RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p>什么情况下会进行原地操作（当前遇到的）:</p>
<ul>
<li>模型参数初始化</li>
<li>自定义实现梯度下降法</li>
<li>避免梯度积累，每次训练进行梯度清零</li>
</ul>
<h4 id="如何解决"><a href="#如何解决" class="headerlink" title="如何解决"></a>如何解决</h4><ol>
<li><p>使用 .data 或 .detach() 方法，获得原张量的同内存但不进行梯度追踪的张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="built_in">print</span>(X)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(X.data.requires_grad)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(X.detach().requires_grad)</span><br><span class="line">tensor([[<span class="number">0.2407</span>, <span class="number">0.3222</span>, <span class="number">0.4246</span>, <span class="number">0.3125</span>],</span><br><span class="line">        [<span class="number">0.1386</span>, <span class="number">0.7018</span>, <span class="number">0.1751</span>, <span class="number">0.0617</span>],</span><br><span class="line">        [<span class="number">0.3467</span>, <span class="number">0.5178</span>, <span class="number">0.2557</span>, <span class="number">0.9855</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>使用该不追踪梯度张量替代执行inplace操作</p>
<pre><code># 自定义实现梯度下降
net.weight.detach().sub_(net.weight.grad,alpha = lr)
net.bias.detach().sub_(net.bias.grad,alpha = lr)
</code></pre></li>
<li><p>.data 与 .detach()的区别在于</p>
<ul>
<li><p>.data 的inplace修改自动求导不监控，如果修改了梯度追踪节点的值，可能导致求导结果错误</p>
</li>
<li><p>.detach() 的inplace修改自动求导同样监控，如果修改了梯度追踪节点的值再进行求导，系统会报错</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<pre><code> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 正常使用</span><br><span class="line">&gt;&gt; X = torch.tensor([[5, 1, 9],[6, 5, 7]],requires_grad = True,dtype = torch.float)</span><br><span class="line">&gt;&gt; print(X)</span><br><span class="line">&gt;&gt; Y = X ** 2</span><br><span class="line">&gt;&gt; Y.sum().backward()</span><br><span class="line"># 梯队为2X</span><br><span class="line">&gt;&gt; print(X.grad)</span><br><span class="line">tensor([[5., 1., 9.],</span><br><span class="line">        [6., 5., 7.]], requires_grad=True)</span><br><span class="line">tensor([[10.,  2., 18.],</span><br><span class="line">        [12., 10., 14.]])</span><br></pre></td></tr></table></figure>

 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 使用data</span><br><span class="line">&gt;&gt; X = torch.tensor([[5, 1, 9],[6, 5, 7]],requires_grad = True,dtype = torch.float)</span><br><span class="line">&gt;&gt; print(X)</span><br><span class="line">&gt;&gt;  = X ** 2</span><br><span class="line">&gt;&gt; X.data *= 2</span><br><span class="line">&gt;&gt; Y.sum().backward()</span><br><span class="line">&gt;&gt; print(Y)</span><br><span class="line"># 梯队为(梯度相较于正常的放大了两倍)</span><br><span class="line">&gt;&gt; print(X.grad)</span><br><span class="line">tensor([[5., 1., 9.],</span><br><span class="line">        [6., 5., 7.]], requires_grad=True)</span><br><span class="line">tensor([[25.,  1., 81.],</span><br><span class="line">        [36., 25., 49.]], grad_fn=&lt;PowBackward0&gt;)</span><br><span class="line">tensor([[20.,  4., 36.],</span><br><span class="line">        [24., 20., 28.]])</span><br></pre></td></tr></table></figure>

 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 使用detach()</span><br><span class="line">X = torch.tensor([[5, 1, 9],[6, 5, 7]],requires_grad = True,dtype = torch.float)</span><br><span class="line">print(X)</span><br><span class="line">Y = X ** 2</span><br><span class="line">X.detach().add_(100)</span><br><span class="line">Y.sum().backward()</span><br><span class="line">print(Y)</span><br><span class="line"># 梯队为(梯度相较于正常的放大了两倍)</span><br><span class="line">print(X.grad)</span><br><span class="line">直接报错:</span><br><span class="line">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2, 3]] is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</span><br></pre></td></tr></table></figure>
</code></pre><ul>
<li>detach() 与 detach_()的区别<ul>
<li>detach_()不仅获得未追踪梯度同内存tensor，还将当前节点设置为叶子节点（即求梯度是求到当前节点停止，不在向前继续计算，截断方向传播计算图）</li>
</ul>
</li>
</ul>
<ol>
<li><p>使用pytorch的init模块初始化模块参数</p>
<blockquote>
<p>torch.nn.init+初始化函数名</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正态分布初始化</span></span><br><span class="line">net = nn.Linear(feature_num, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    nn.init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="自动求导相关属性"><a href="#自动求导相关属性" class="headerlink" title="自动求导相关属性"></a>自动求导相关属性</h3><ul>
<li><p><strong>requires_grad</strong> 是否进行梯度追踪</p>
<blockquote>
<p>Is <code>True</code> if gradients need to be computed for this Tensor, <code>False</code> otherwise.</p>
</blockquote>
</li>
<li><p><strong>grad</strong> 存储梯度的tensor数组，未进行反向传播计算时为None，多次计算梯度会进行累加</p>
<blockquote>
<p>This attribute is <code>None</code> by default and becomes a Tensor the first time a call to <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.backward"><code>backward()</code></a> computes gradients for <code>self</code>. The attribute will then contain the gradients computed and future calls to <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.backward"><code>backward()</code></a> will accumulate (add) gradients into it.</p>
</blockquote>
</li>
<li><p><strong>is_leaf</strong>  所有用户创建的梯度追踪结点为叶子节点,只有叶子节点的梯度值会被计算,可通过retrain_grad()获得非叶子节点的梯度</p>
<blockquote>
<p>All Tensors that have <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.requires_grad"><code>requires_grad</code></a> which is <code>False</code> will be leaf Tensors by convention.</p>
<p>For Tensors that have <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.requires_grad"><code>requires_grad</code></a> which is <code>True</code>, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so <code>grad_fn</code> is None.</p>
<p>Only leaf Tensors will have their <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.grad"><code>grad</code></a> populated during a call to <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.backward"><code>backward()</code></a>. To get <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.grad"><code>grad</code></a> populated for non-leaf Tensors, you can use <a href="https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.Tensor.retain_grad"><code>retain_grad()</code></a>.</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>AI编程</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch模型定义</title>
    <url>/2021/07/09/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/pytorch%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/</url>
    <content><![CDATA[<h2 id="pytorch模型定义的几种方式"><a href="#pytorch模型定义的几种方式" class="headerlink" title="pytorch模型定义的几种方式"></a>pytorch模型定义的几种方式</h2><p>主要包括 标准继承模式 和 使用常用容器 两种方式</p>
<h3 id="1-继承nn-Module实现模型"><a href="#1-继承nn-Module实现模型" class="headerlink" title="1. 继承nn.Module实现模型"></a>1. 继承nn.Module实现模型</h3><p>通过继承nn模块的Module基类，并实现初始化<strong>init</strong>()以及forward()方法，实现模型定义</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 模型的初始化方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 首先调用父类构造方法</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        <span class="comment"># 定义各层模型，层与层之间的权重</span></span><br><span class="line">        self.hidden = nn.Linear(<span class="number">128</span>, <span class="number">16</span>) <span class="comment"># 隐藏层</span></span><br><span class="line">        self.activate = nn.ReLU()</span><br><span class="line">        self.output = nn.Linear(<span class="number">16</span>, <span class="number">10</span>)  <span class="comment"># 输出层</span></span><br><span class="line">    <span class="comment"># 模型的计算方法  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.activate(self.hidden(x))</span><br><span class="line">        <span class="keyword">return</span> self.output(x)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p>模型的使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">test_input = torch.rand((256,128))</span><br><span class="line">net = Model()</span><br><span class="line">print(net(test_input))</span><br><span class="line">print(net)</span><br><span class="line">输出:</span><br><span class="line">tensor([[ 0.2073, -0.0732,  0.1259,  ...,  0.1715,  0.1540,  0.1089],</span><br><span class="line">        [ 0.2999, -0.0407,  0.1167,  ...,  0.1734,  0.2581,  0.0695],</span><br><span class="line">        [ 0.3761, -0.0145,  0.1217,  ...,  0.1910,  0.2665,  0.0340],</span><br><span class="line">        ...,</span><br><span class="line">        [ 0.3084, -0.1336,  0.1632,  ...,  0.2174, -0.0351,  0.1205],</span><br><span class="line">        [ 0.3323, -0.1693,  0.1411,  ...,  0.1700,  0.1171, -0.0442],</span><br><span class="line">        [ 0.3395,  0.0367, -0.0068,  ...,  0.2706,  0.2510,  0.0017]],</span><br><span class="line">       grad_fn=&lt;AddmmBackward&gt;)</span><br><span class="line">Model(</span><br><span class="line">  (hidden): Linear(in_features=128, out_features=16, bias=True)</span><br><span class="line">  (activate): ReLU()</span><br><span class="line">  (output): Linear(in_features=16, out_features=10, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="2-快速定义模型"><a href="#2-快速定义模型" class="headerlink" title="2. 快速定义模型"></a>2. 快速定义模型</h3><p>pytorch提供了一系列继承自nn.Module的实现类，类如Sequential等用来快速定义模型</p>
<h4 id="1-nn-Sequential"><a href="#1-nn-Sequential" class="headerlink" title="1. nn.Sequential"></a>1. nn.Sequential</h4><blockquote>
<p>A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ordered dict of modules can also be passed in.</p>
</blockquote>
<p>向Sequential中传入一系列的层/其他模型（Module），按照传入的顺序或者OrderedDict中的顺序进行前向传播计算，实现模型定义</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">          (<span class="string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu2&#x27;</span>, nn.ReLU())</span><br><span class="line">        ]))</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line">输出:</span><br><span class="line">Sequential(</span><br><span class="line">  (conv1): Conv2d(<span class="number">1</span>, <span class="number">20</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (relu1): ReLU()</span><br><span class="line">  (conv2): Conv2d(<span class="number">20</span>, <span class="number">64</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (relu2): ReLU()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="2-nn-ModuleList"><a href="#2-nn-ModuleList" class="headerlink" title="2.nn.ModuleList"></a>2.nn.ModuleList</h4><blockquote>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html?highlight=modulelist#torch.nn.ModuleList"><code>ModuleList</code></a> can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"><code>Module</code></a> methods.</p>
</blockquote>
<p>存储module的列表，支持列表的append，insert操作，并可通过坐标形式访问</p>
<ul>
<li>与sequence不同点，在于ModuList就是个List，不支持forward前向传播运算</li>
<li>与List不同点，在于ModuList中所有模型的参数均加入到了反向传播梯度计算监控中</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class MyModule(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyModule, self).__init__()</span><br><span class="line">        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # ModuleList can act as an iterable, or be indexed using ints</span><br><span class="line">        for i, l in enumerate(self.linears):</span><br><span class="line">            x = self.linears[i // 2](x) + l(x)</span><br><span class="line">        return x</span><br><span class="line">net = MyModule()</span><br><span class="line">print(net)</span><br><span class="line">输出:</span><br><span class="line">MyModule(</span><br><span class="line">  (linears): ModuleList(</span><br><span class="line">    (0): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (1): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (2): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (3): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (4): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (5): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (6): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (7): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (8): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">    (9): Linear(in_features=10, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>与列表存储模型不同点比较</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class MyModule1(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyModule1, self).__init__()</span><br><span class="line">        self.linearList = [nn.Linear(i,i) for i in range(1,3)]</span><br><span class="line">class MyModule2(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyModule2, self).__init__()</span><br><span class="line">        self.linearList = nn.ModuleList([nn.Linear(i,i) for i in range(1, 3)])</span><br><span class="line">net1 = MyModule1()</span><br><span class="line">net2 = MyModule2()</span><br><span class="line">print(&#x27;net1:&#x27;)</span><br><span class="line">for parameter in net1.parameters():</span><br><span class="line">    print(parameter)</span><br><span class="line">print(&#x27;net2:&#x27;)</span><br><span class="line">for parameter in net2.parameters():</span><br><span class="line">    print(parameter)</span><br><span class="line">输出:</span><br><span class="line">net1:</span><br><span class="line">net2:</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[0.9031]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([0.8702], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ 0.4465, -0.2073],</span><br><span class="line">        [-0.3850, -0.6185]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([-0.1432, -0.6002], requires_grad=True)</span><br><span class="line"></span><br><span class="line">#### 3. nn.MoudleDict</span><br><span class="line"></span><br><span class="line">类似于Modulist，只是存储形式为Dict</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>AI编程</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>sklearn主要模块理解</title>
    <url>/2021/07/09/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/sklearn%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<p>主要包括六个模块，其中四个模块为<strong>分类，回归，聚类，降维</strong>的算法模块，两个其他模型，模型选择评估模块和预处理模块。</p>
<img src="/2021/07/09/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/sklearn%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97%E7%90%86%E8%A7%A3/image-20210709164825577.png" class="" title="image-20210709164825577">
<span id="more"></span>
<h3 id="一个数据分析流程中涉及的sklearn模块"><a href="#一个数据分析流程中涉及的sklearn模块" class="headerlink" title="一个数据分析流程中涉及的sklearn模块"></a>一个数据分析流程中涉及的sklearn模块</h3><p>机器学习项目的一般流程为 <strong>数据集获取</strong>-》<strong>数据预处理</strong>-》<strong>训练模型</strong>-》<strong>评估模型</strong>-》<strong>应用模型</strong></p>
<ol>
<li><p>获取数据集，使用dataset模块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"><span class="comment"># 返回一个包含data和target的字典</span></span><br><span class="line">iris_data = iris.data</span><br><span class="line">iris_target = iris.target</span><br></pre></td></tr></table></figure>
<p>包含常用的数据集加载</p>
<img src="/2021/07/09/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/sklearn%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97%E7%90%86%E8%A7%A3/image-20210709164843880.png" class="" title="image-20210709164843880">
</li>
<li><p>数据预处理，使用sklearn的Preprocessing模块</p>
<p>包括常用的归一化、one_hot等处理方式</p>
<img src="/2021/07/09/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/sklearn%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97%E7%90%86%E8%A7%A3/image-20210709164855098.png" class="" title="image-20210709164855098">
</li>
<li><p>划分数据集，使用model_selection模块中的划分函数（不同的训练集测试集划分方式)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_target, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>
<img src="/2021/07/09/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/sklearn%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97%E7%90%86%E8%A7%A3/image-20210709164909443.png" class="" title="image-20210709164909443">
</li>
<li><p>模型选择，直接导入特定模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">clf = svm.SVC(gamma=<span class="number">0.001</span>, C=<span class="number">100.</span>)</span><br><span class="line"><span class="comment"># 训练模型fit</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(clf.get_params())</span><br><span class="line"><span class="comment"># 评价模型</span></span><br><span class="line"><span class="built_in">print</span>(clf.score(X_test,y_test))</span><br></pre></td></tr></table></figure>
</li>
<li><p>评价模型，使用metircs模块进行评价</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">accuracy_score(y_test, clf.predict(X_test))</span><br></pre></td></tr></table></figure>
<img src="/2021/07/09/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/sklearn%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97%E7%90%86%E8%A7%A3/image-20210709164919754.png" class="" title="image-20210709164919754">
</li>
</ol>
]]></content>
      <categories>
        <category>AI编程</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>优化算法学习总结</title>
    <url>/2021/07/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="1-动量梯度下降"><a href="#1-动量梯度下降" class="headerlink" title="1. 动量梯度下降"></a>1. 动量梯度下降</h3><p>为了解决随机梯度梯度下降存在的震荡问题，通过添加动量，平滑动量变化。</p>
<h4 id="什么是指数加权平均"><a href="#什么是指数加权平均" class="headerlink" title="什么是指数加权平均"></a>什么是指数加权平均</h4><p>当前步的函数值不仅取决于当前的输入，还取决于之前的函数值，公式如下</p>
<script type="math/tex; mode=display">
v_t = \beta v_{t-1}+(1-\beta)\theta_t</script><p>通过不断带入展开，可得只包含 <script type="math/tex">\theta</script> 的表达式为</p>
<script type="math/tex; mode=display">
v_t = (1-\beta)\theta_t + (1-\beta)\beta\theta_{t-1} + (1-\beta)\beta^2\theta_{t-2}+....+(1-\beta)\beta^n\theta_{t-n}+\beta^nv_{t-n}</script><img src="/2021/07/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/image-20210720092013215.png" class="" title="image-20210720092013215">
<p>由于 <script type="math/tex">(1-\frac{1}{n})^n</script> 在n趋向于无穷时等于 <script type="math/tex">e^{-1}</script>。当 <script type="math/tex">\beta</script> 趋向于1时，<script type="math/tex">\beta^{\frac{1}{1-\beta}} = (1-\frac{1}{n})^n</script> 同样等于 <script type="math/tex">e^{-1}</script>,如果将 <script type="math/tex">e^{-1}</script>看作一个可以忽略的数字，则展开式中含有阶数大于等于 <script type="math/tex">\theta^{\frac{1}{1-\theta}}</script> 的相可以忽略。</p>
<ul>
<li>当 <script type="math/tex">\beta = 0.98</script>​时，<script type="math/tex">\beta^{\frac{1}{1-\beta}} = \beta^{20}</script>​ ,所以所有系数包含<script type="math/tex">\beta^n</script>​ n&gt;=20的均省略，原式子等价于前20个<script type="math/tex">\theta</script>​的加权平均和，<strong>即当前梯度等于前20个梯度的加权平均和</strong><span id="more"></span>
</li>
</ul>
<h4 id="动量梯度下降公式"><a href="#动量梯度下降公式" class="headerlink" title="动量梯度下降公式"></a>动量梯度下降公式</h4><p>假设第 <script type="math/tex">t</script> 次训练的输入为 <script type="math/tex">x_t</script> ,学习率为<script type="math/tex">\eta_t</script>,计算得到梯度为 <script type="math/tex">g_t</script> ,并定义速度向量 <script type="math/tex">v_t</script>,初始化为0；动量系数为m</p>
<script type="math/tex; mode=display">
v_t = m * v_{t-1} + \eta * g_t \\
x_t = x_{t-1} - v_t</script><p>其中 <script type="math/tex">0<m<1</script>，当 <script type="math/tex">m=0</script> 时等价于随机梯度下降</p>
<p><strong>通过对历史梯度进行指数加权平均，实现了梯度的平滑变换，确保了相邻梯度方向的一致性，解决了梯度下降的震荡问题。</strong></p>
<h3 id="2-AdaGrad算法"><a href="#2-AdaGrad算法" class="headerlink" title="2. AdaGrad算法"></a>2. AdaGrad算法</h3><p>为解决不同方向梯度大小变化不同，导致学习率设置的问题，adagrad算法根据每个方向梯度大小，不断更新学习率</p>
<h4 id="AdaGrad公式"><a href="#AdaGrad公式" class="headerlink" title="AdaGrad公式"></a>AdaGrad公式</h4><p>首先定义向量 <script type="math/tex">s_t</script> ,初始化为0，每次迭代累加梯度的内积，递归公式为</p>
<script type="math/tex; mode=display">
s_t = s_{t-1} + g_i * g_i</script><p>更新目标参数是，使用  <script type="math/tex">s_t</script> 更新学习率大小,其中 <script type="math/tex">\epsilon</script> 为为保持数值稳定性添加的常数</p>
<script type="math/tex; mode=display">
x_t = x_{t-1} - \frac{\eta}{\sqrt{s_t+\epsilon}}*g_t</script><p><strong>AdaGrad算法能够根据梯度大小设置学习率，梯度越大，学习率越小，但是由于 $s_t$ 为累加形式，学习率随着训练会不断减小，到后期学习速率可能过慢</strong></p>
<h3 id="3-RMSprop算法"><a href="#3-RMSprop算法" class="headerlink" title="3. RMSprop算法"></a>3. RMSprop算法</h3><p>为了解决AdAGrad算法后期，学习率过小可能无法找到最优解的问题，在计算 $s_t$ 时引入加权平均</p>
<h4 id="RMSprop公式"><a href="#RMSprop公式" class="headerlink" title="RMSprop公式"></a>RMSprop公式</h4><p>引入加权平均后的 $s_t$ 计算公式如下，避免了学习率的不断减小</p>
<script type="math/tex; mode=display">
s_t = \theta s_{t-1} + (1-\theta)g_i * g_i</script><h3 id="4-AdaDelta算法"><a href="#4-AdaDelta算法" class="headerlink" title="4. AdaDelta算法"></a>4. AdaDelta算法</h3><p>舍弃了学习率这一参数，从另一个角度解决AdaGrad后期学习率变小，难以找到最优解的问题</p>
<h4 id="AdaDelta公式"><a href="#AdaDelta公式" class="headerlink" title="AdaDelta公式"></a>AdaDelta公式</h4><p>$s_t$递推公式与RMSprop一致</p>
<script type="math/tex; mode=display">
s_t = \theta s_{t-1} + (1-\theta)g_i * g_i</script><p>增加了状态 $\Delta x_t$,计算梯度  $g’_t$ </p>
<script type="math/tex; mode=display">
g'_t = \sqrt{\frac{\Delta x_{t-1} + \epsilon}{s_t + \epsilon}}*g_t</script><p>使用 $g’_t$  更新参数</p>
<script type="math/tex; mode=display">
x_t = x_{t-1} - g'_t</script><p>其中状态 $\Delta x_t$ 的递推公式为</p>
<script type="math/tex; mode=display">
\Delta x_t = \theta \Delta x_{t-1} + (1-\theta)g'_i * g'_i</script><p><strong>与RMSprop算法区别点在于学习率也通过指数加权平均递归计算</strong></p>
<h3 id="5-Adam算法"><a href="#5-Adam算法" class="headerlink" title="5. Adam算法"></a>5. Adam算法</h3><p>Adam算法结合了动量法和RMSprop法，对梯度和$s_t$ 均进行了指数加权平均</p>
<p>$s_t$递推公式($\theta_1$  建议为0.999）</p>
<script type="math/tex; mode=display">
s_t = \theta_1 s_{t-1} + (1-\theta_1)g_i * g_i</script><p>$v_t$ 递推公式($\theta_2$  建议为0.9）</p>
<script type="math/tex; mode=display">
v_t = \theta_2 * v_{t-1} + \theta_2 * g_t</script><p>由于 $s_t$ 和 $v_t$ 在递推初始化时初始化为0，根据递推公式中 $\theta_1$ 和 $\theta_2$  均为接近于1的数，导致在训练初期， $s_t$ 和 $v_t$ 取值与梯度关系不大，更加趋向于0，因此需要增加 <strong>误差修正(bias correction)</strong> 操作</p>
<script type="math/tex; mode=display">
s'_t = \frac{s_t}{1-\theta_1^t}\\
v'_t = \frac{v_t}{1-\theta_2^t}</script><p>使用修正后的两个参数计算新梯度</p>
<script type="math/tex; mode=display">
g'_t =\frac{\eta v'_t}{\sqrt{s'_t} + \epsilon}</script><p>更新参数公式为</p>
<script type="math/tex; mode=display">
x_t = x_{t-1} - g'_t</script><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><img src="/2021/07/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/image-20210720143613685.png" class="" title="image-20210720143613685">
]]></content>
      <categories>
        <category>AI编程</category>
      </categories>
  </entry>
  <entry>
    <title>sklearn数据预处理一般流程</title>
    <url>/2021/07/09/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>The <code>sklearn.preprocessing</code> package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.</p>
</blockquote>
<p>sklearn的preprocessing模块提供了一系列包括标准化、数据最大最小缩放处理、正则化、特征二值化和数据缺失值处理在内的数据预处理模块。</p>

<p>基本操作流程为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.创建预处理器 transform</span></span><br><span class="line">test_scaler = StandardScaler()</span><br><span class="line"><span class="comment"># 2. 调用fit函数 计算预处理所需要的相关数据(如StandardScaler会计算mean、var等)</span></span><br><span class="line">test_scaler.fit(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># 3. 调用transform函数对数据进行预处理</span></span><br><span class="line">test_scaler.transform(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># 或者直接合并fit和transform两部操作</span></span><br><span class="line">test_scaler.fit_transform(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h2 id="1-标准化"><a href="#1-标准化" class="headerlink" title="1. 标准化"></a>1. 标准化</h2><h3 id="使用StandardScaler（mean-1-std-0）"><a href="#使用StandardScaler（mean-1-std-0）" class="headerlink" title="使用StandardScaler（mean = 1,std = 0）"></a>使用StandardScaler（mean = 1,std = 0）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">&gt;&gt; <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;&gt; test_array = np.arange(<span class="number">0</span>,<span class="number">12</span>).reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">       [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">       [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]])</span><br><span class="line">&gt;&gt; test_scaler = StandardScaler()</span><br><span class="line">&gt;&gt; test_scaler.fit(test_array)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(test_scaler.var_)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(test_scaler.mean_)</span><br><span class="line">&gt;&gt; test_scaler.transform(test_array, copy = <span class="literal">True</span>)</span><br><span class="line">[<span class="number">10.66666667</span> <span class="number">10.66666667</span> <span class="number">10.66666667</span> <span class="number">10.66666667</span>]</span><br><span class="line">[<span class="number">4.</span> <span class="number">5.</span> <span class="number">6.</span> <span class="number">7.</span>]</span><br><span class="line">array([[-<span class="number">1.22474487</span>, -<span class="number">1.22474487</span>, -<span class="number">1.22474487</span>, -<span class="number">1.22474487</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ],</span><br><span class="line">       [ <span class="number">1.22474487</span>,  <span class="number">1.22474487</span>,  <span class="number">1.22474487</span>,  <span class="number">1.22474487</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="使用MaxMinScaler进行区间缩放（默认0-1）"><a href="#使用MaxMinScaler进行区间缩放（默认0-1）" class="headerlink" title="使用MaxMinScaler进行区间缩放（默认0-1）"></a>使用MaxMinScaler进行区间缩放（默认0-1）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">&gt;&gt; test_array = np.random.uniform(low=-<span class="number">1</span>, high=<span class="number">16</span>, size=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(test_array)</span><br><span class="line">[[<span class="number">10.45968472</span>  <span class="number">6.52372993</span> <span class="number">12.08526458</span>]</span><br><span class="line"> [ <span class="number">9.94529398</span>  <span class="number">6.3849395</span>   <span class="number">6.22910917</span>]</span><br><span class="line"> [<span class="number">12.02516025</span> <span class="number">11.50269044</span>  <span class="number">6.380779</span>  ]</span><br><span class="line"> [<span class="number">14.67759022</span>  <span class="number">0.36908024</span>  <span class="number">1.13677392</span>]]</span><br><span class="line">[<span class="number">0.42262781</span> <span class="number">0.17963625</span> <span class="number">0.18267358</span>]</span><br><span class="line"><span class="comment"># 传入要缩放的区间</span></span><br><span class="line">&gt;&gt; test_scaler = MinMaxScaler((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">&gt;&gt; test_scaler.fit(test_array)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(test_scaler.scale_)</span><br><span class="line">&gt;&gt; test_scaler.transform(test_array)</span><br><span class="line">array([[-<span class="number">0.78260417</span>,  <span class="number">0.1055982</span> ,  <span class="number">1.</span>        ],</span><br><span class="line">       [-<span class="number">1.</span>        ,  <span class="number">0.08066641</span>, -<span class="number">0.06976488</span>],</span><br><span class="line">       [-<span class="number">0.12099067</span>,  <span class="number">1.</span>        , -<span class="number">0.04205881</span>],</span><br><span class="line">       [ <span class="number">1.</span>        , -<span class="number">1.</span>        , -<span class="number">1.</span>        ]])</span><br></pre></td></tr></table></figure>
<h3 id="使用MaxAbsScaler稀疏数据标准化"><a href="#使用MaxAbsScaler稀疏数据标准化" class="headerlink" title="使用MaxAbsScaler稀疏数据标准化"></a>使用MaxAbsScaler稀疏数据标准化</h3><p>为了避免标准化过程中破坏稀疏数据的稀疏性质，使用MaxAbsScaler,根据样本数据除以最大绝对值，实现到[-1, 1]的映射</p>
<h3 id="使用RobustScaler带有离群值的数据标准化"><a href="#使用RobustScaler带有离群值的数据标准化" class="headerlink" title="使用RobustScaler带有离群值的数据标准化"></a>使用RobustScaler带有离群值的数据标准化</h3><h2 id="2-非线性转化"><a href="#2-非线性转化" class="headerlink" title="2.非线性转化"></a>2.非线性转化</h2><p>主要包括概率分布转化（Quantile transforms）和正态变换（Power transforms），用来将原特定分布的特征值映射到另一个特征分布。</p>
<h3 id="使用QuantileTransformer进行均匀分布映射转换"><a href="#使用QuantileTransformer进行均匀分布映射转换" class="headerlink" title="使用QuantileTransformer进行均匀分布映射转换"></a>使用QuantileTransformer进行均匀分布映射转换</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> QuantileTransformer</span><br><span class="line">&gt;&gt; data_x, data_y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line">&gt;&gt; x_train, x_test, y_train, y_test = train_test_split(data_x, data_y,  test_size = <span class="number">0.2</span>, random_state = <span class="number">42</span>)</span><br><span class="line">&gt;&gt; quantileTransformer = QuantileTransformer()</span><br><span class="line">&gt;&gt; x_train_trans = quantileTransformer.fit_transform(x_train)</span><br><span class="line">&gt;&gt; x_test_trans = quantileTransformer.fit_transform(x_test)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(np.percentile(x_train[:, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">25</span>, <span class="number">50</span>, <span class="number">75</span>, <span class="number">100</span>]))</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(np.percentile(x_train_trans[:, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">25</span>, <span class="number">50</span>, <span class="number">75</span>, <span class="number">100</span>]))</span><br><span class="line">[<span class="number">4.3</span>  <span class="number">5.1</span>  <span class="number">5.75</span> <span class="number">6.4</span>  <span class="number">7.7</span> ]</span><br><span class="line">[<span class="number">0.</span>         <span class="number">0.24789916</span> <span class="number">0.5</span>        <span class="number">0.7605042</span>  <span class="number">1.</span>        ]</span><br></pre></td></tr></table></figure>
<h3 id="使用PowerTransformer进行正态分布映射转换"><a href="#使用PowerTransformer进行正态分布映射转换" class="headerlink" title="使用PowerTransformer进行正态分布映射转换"></a>使用PowerTransformer进行正态分布映射转换</h3><p>使用Yeo-Johnson transform和 Box-Cox transform两种变换方式（暂时还不太懂，不列举代码）</p>
<h2 id="3-标准化（Normalization）"><a href="#3-标准化（Normalization）" class="headerlink" title="3.标准化（Normalization）"></a>3.标准化（<strong>Normalization</strong>）</h2><p>直接使用normalize函数，有三种归一化方式 <code>&#123;‘l1’, ‘l2’, ‘max’&#125;, default=’l2’</code> ，坑爹的是默认使用行向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">&gt;&gt; test_array = np.arange(<span class="number">3</span>, <span class="number">15</span>).reshape(<span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(test_array)</span><br><span class="line">&gt;&gt; result = preprocessing.normalize(test_array, norm = <span class="string">&#x27;l1&#x27;</span>, axis = <span class="number">0</span>)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(result)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(result.<span class="built_in">sum</span>(axis = <span class="number">0</span>))</span><br><span class="line">[[ <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>  <span class="number">8</span>]</span><br><span class="line"> [ <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span>]]</span><br><span class="line">[[<span class="number">0.25</span>       <span class="number">0.28571429</span> <span class="number">0.3125</span>     <span class="number">0.33333333</span> <span class="number">0.35</span>       <span class="number">0.36363636</span>]</span><br><span class="line"> [<span class="number">0.75</span>       <span class="number">0.71428571</span> <span class="number">0.6875</span>     <span class="number">0.66666667</span> <span class="number">0.65</span>       <span class="number">0.63636364</span>]]</span><br><span class="line">[<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br></pre></td></tr></table></figure>
<p>或者使用Normalizer类</p>
<h2 id="4-类型转化"><a href="#4-类型转化" class="headerlink" title="4.类型转化"></a>4.类型转化</h2><p>主要包括 onehot和数字顺序编码两种形式，主要涉及OneHotEncoder和OrdinalEncoder</p>
<h2 id="5-遇到继续整理。。。。"><a href="#5-遇到继续整理。。。。" class="headerlink" title="5. 遇到继续整理。。。。"></a>5. 遇到继续整理。。。。</h2><h2 id="自定义转化器"><a href="#自定义转化器" class="headerlink" title="自定义转化器"></a>自定义转化器</h2><h3 id="1-使用FunctionTransformer封装函数为转化器"><a href="#1-使用FunctionTransformer封装函数为转化器" class="headerlink" title="1.使用FunctionTransformer封装函数为转化器"></a>1.使用FunctionTransformer封装函数为转化器</h3><p>没想到怎么传入多个参数的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> FunctionTransformer</span><br><span class="line">&gt;&gt; <span class="function"><span class="keyword">def</span> <span class="title">my_power</span>(<span class="params">x, power = <span class="number">2</span></span>):</span></span><br><span class="line">    x = np.power(x, power);</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line">&gt;&gt; my_transformer = FunctionTransformer(my_power)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(my_transformer)</span><br><span class="line">&gt;&gt; test_array = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(my_transformer.fit_transform(test_array))</span><br><span class="line">FunctionTransformer(func=&lt;function my_power at <span class="number">0x000001988F4543A0</span>&gt;)</span><br><span class="line">[ <span class="number">1</span>  <span class="number">4</span>  <span class="number">9</span> <span class="number">16</span>]</span><br></pre></td></tr></table></figure>
<h3 id="3-继承BaseEstimator-TransformerMixin（自动实现fit-transform）"><a href="#3-继承BaseEstimator-TransformerMixin（自动实现fit-transform）" class="headerlink" title="3. 继承BaseEstimator, TransformerMixin（自动实现fit_transform）"></a>3. 继承BaseEstimator, TransformerMixin（自动实现fit_transform）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line">rooms_ix, bedrooms_ix, population_ix, households_ix = <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span></span><br><span class="line"><span class="comment"># 通过原有属性增加一列属性（来自书籍-机器学习实战）</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CombineAttirbutesAdder</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, add_bedrooms_per_room = <span class="literal">True</span></span>):</span></span><br><span class="line">        self.add_bedrooms_per_room = add_bedrooms_per_room</span><br><span class="line">    <span class="comment"># 提取某些特征，例如归一化处理是求平均值和方差</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        rooms_per_house = X[:, rooms_ix] / X[:, households_ix]</span><br><span class="line">        pepoles_per_house = X[:, population_ix] / X[:, households_ix]</span><br><span class="line">        <span class="keyword">if</span> self.add_bedrooms_per_room:</span><br><span class="line">            bedrooms_per_house = X[:,bedrooms_ix] / X[:, households_ix]</span><br><span class="line">            <span class="keyword">return</span> np.c_[X, rooms_per_house, bedrooms_per_house]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> np.c_[X, rooms_per_house]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>AI编程</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>sklearn Pipeline 组合多个预处理和预测模型</title>
    <url>/2021/07/09/AI%E5%AD%A6%E4%B9%A0/DL%E7%BC%96%E7%A8%8B/%E7%BB%84%E5%90%88%E5%A4%9A%E4%B8%AA%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<blockquote>
<p> Transformers are usually combined with classifiers, regressors or other estimators to build a composite estimator</p>
</blockquote>
<p>将一系列的数据预处理和模型封装在一起，固定处理数据的一系列步骤，调用一次fit和predict完成整个流程，并可使用grid search对pipeline内参数进行统一调试。</p>
<p>构成规则如下（前面处理数据，最后一步输入模型或者完全处理数据）：</p>
<ul>
<li>所有流水线中的estimator必须为transformer，除了最后一个</li>
<li>最后一个estimator可以是任何类型（trainsformer，classifier）</li>
</ul>
<p>主要函数：</p>
<ul>
<li>union为只包含transformer的pipeline</li>
</ul>

<span id="more"></span>
<h3 id="1-pipeline的基本使用"><a href="#1-pipeline的基本使用" class="headerlink" title="1. pipeline的基本使用"></a>1. pipeline的基本使用</h3><p>使用类似字典的元组链表初始化pipeline，或者使用make_pipeline()函数直接传入estimator列表</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">&gt;&gt; estimators = [(<span class="string">&#x27;reduce_dim&#x27;</span>, PCA()), (<span class="string">&#x27;model&#x27;</span>, SVC())]</span><br><span class="line">&gt;&gt; test_pipeline = Pipeline(estimators)</span><br><span class="line">&gt;&gt; test_pipeline</span><br><span class="line">Pipeline(steps=[(<span class="string">&#x27;reduce_dim&#x27;</span>, PCA()), (<span class="string">&#x27;model&#x27;</span>, SVC())])</span><br></pre></td></tr></table></figure>
<p>可通过数组、字典以及step属性访问每个estimator</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="built_in">print</span>(test_pipeline[<span class="number">0</span>])</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(test_pipeline.steps)</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(test_pipeline[<span class="string">&#x27;reduce_dim&#x27;</span>])</span><br><span class="line">PCA()</span><br><span class="line">[(<span class="string">&#x27;reduce_dim&#x27;</span>, PCA()), (<span class="string">&#x27;model&#x27;</span>, SVC())]</span><br><span class="line">PCA()</span><br></pre></td></tr></table></figure>
<p>可使用数组的截断形式，获取子pipeline</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="built_in">print</span>(<span class="built_in">len</span>(test_pipeline))</span><br><span class="line">&gt;&gt; sub_pipeline = test_pipeline[<span class="number">1</span>:]</span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(sub_pipeline)</span><br><span class="line"><span class="number">2</span></span><br><span class="line">Pipeline(steps=[(<span class="string">&#x27;model&#x27;</span>, SVC())])</span><br></pre></td></tr></table></figure>
<p>使用<code>&lt;estimator&gt;__&lt;parameter&gt;</code> 即pipeline中estimator名称+参数，设置访问pipeline中某一estimator的参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; test_pipeline.set_params(model__C = <span class="number">10</span>)</span><br><span class="line">&gt;&gt; test_pipeline.get_params()[<span class="string">&#x27;model__C&#x27;</span>]</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="comment"># 与GridSearchCV结合使用</span></span><br><span class="line">&gt;&gt; <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line">&gt;&gt; param_grid = <span class="built_in">dict</span>(reduce_dim__n_components=[<span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>], clf__C=[<span class="number">0.1</span>, <span class="number">10</span>, <span class="number">100</span>])</span><br><span class="line">&gt;&gt; grid_search = GridSearchCV(test_pipeline, param_grid=param_grid)</span><br><span class="line">&gt;&gt; grid_search</span><br><span class="line">GridSearchCV(estimator=Pipeline(steps=[(<span class="string">&#x27;reduce_dim&#x27;</span>, PCA()),</span><br><span class="line">                                       (<span class="string">&#x27;model&#x27;</span>, SVC(C=<span class="number">10</span>))]),</span><br><span class="line">             param_grid=&#123;<span class="string">&#x27;clf__C&#x27;</span>: [<span class="number">0.1</span>, <span class="number">10</span>, <span class="number">100</span>],</span><br><span class="line">                         <span class="string">&#x27;reduce_dim__n_components&#x27;</span>: [<span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>]&#125;)</span><br></pre></td></tr></table></figure>
<h1 id="待补充。。"><a href="#待补充。。" class="headerlink" title="待补充。。"></a>待补充。。</h1>]]></content>
      <categories>
        <category>AI编程</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>Dockerfile理解使用</title>
    <url>/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Dockerfile%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>docker file 包含一系列命令行，docker通过该文件组织生成镜像，一个docker file文件主要包括四部分：</p>
<ol>
<li>基础镜像信息</li>
<li>维护者信息</li>
<li>镜像操作指令</li>
<li>容器启动时执行指令</li>
</ol>
<p>以一个dockerfile为例：<br><img src="/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Dockerfile%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/image-20210618162710697.png" class="" title="image-20210618162710697.png"></p>
<blockquote>
<p>Dockerfile 的指令每执行一次都会在 docker 上新建一层。所以过多无意义的层，会造成镜像膨胀过大</p>
</blockquote>
<span id="more"></span>
<h3 id="基本的命令内容"><a href="#基本的命令内容" class="headerlink" title="基本的命令内容"></a>基本的命令内容</h3><ol>
<li><p>FORM：指定基础镜像，必须为第一个命令</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line">   　　<span class="comment"># 格式：</span></span><br><span class="line"><span class="keyword">FROM</span> &lt;image&gt;</span><br><span class="line"><span class="keyword">FROM</span> &lt;image&gt;:&lt;tag&gt;</span><br><span class="line"><span class="keyword">FROM</span> &lt;image&gt;@&lt;digest&gt;</span><br><span class="line">   　　<span class="comment"># 例如：</span></span><br><span class="line">   　　<span class="keyword">FROM</span> node:alpine</span><br></pre></td></tr></table></figure>
<p>同一个镜像可能有不同大小版本，不同的tag表示基于不同的base image（以python3.9.6 镜像为例）</p>
<ol>
<li><p><strong>完整版</strong> </p>
<img src="/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Dockerfile%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/image-20210727160609885.png" class="" title="image-20210727160609885">
</li>
<li><p><strong>alpine</strong>  基于<strong>alpine linux</strong>的docker镜像，特点就是容量小，适合作为基础镜像</p>
<ul>
<li>缺少<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/GNU_C_Library">GNU C Library</a> (glibc)</li>
<li>缺少的软件过多，<a href="https://pythonspeed.com/articles/alpine-docker-python/">构建时间长</a></li>
</ul>
<blockquote>
<p>Alpine 是众多 Linux 发行版中的一员，和 CentOS、Ubuntu、Archlinux之类一样，只是一个发行版的名字，号称小巧安全，有自己的包管理工具 apk</p>
</blockquote>
<img src="/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Dockerfile%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/image-20210727160550212.png" class="" title="image-20210727160550212">
</li>
<li><p><strong>Debian Buster</strong> 基于<strong>Debian linux</strong>的docker镜像</p>
<img src="/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Dockerfile%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/image-20210727160529738.png" class="" title="image-20210727160529738">
</li>
<li><p><strong>slim</strong> 即瘦身版本，删去了部分通用包支持</p>
<ul>
<li>python:slim-buster是大多数Python用例的良好基础镜像</li>
</ul>
<img src="/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Dockerfile%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/image-20210727160805454.png" class="" title="image-20210727160805454">
<img src="/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Dockerfile%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/image-20210727160814190.png" class="" title="image-20210727160814190">
</li>
</ol>
</li>
<li><p>RUN: 构建镜像是执行的命令(支持两种执行方式)</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="comment"># shell执行</span></span><br><span class="line"><span class="comment"># 格式：</span></span><br><span class="line">    <span class="keyword">RUN</span><span class="bash"> &lt;<span class="built_in">command</span>&gt;</span></span><br><span class="line"><span class="comment"># exec执行</span></span><br><span class="line"><span class="comment"># 格式：</span></span><br><span class="line">    <span class="keyword">RUN</span><span class="bash"> [<span class="string">&quot;command&quot;</span>, <span class="string">&quot;param1&quot;</span>, <span class="string">&quot;param2&quot;</span>]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>ADD/COPY:将本地文件添加到docker镜像中</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ADD</span><span class="bash"> &lt;src&gt; &lt;dest&gt;</span></span><br><span class="line"><span class="comment"># 例如：</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> . /data</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>WORKDIR:切换工作目录,即容器启动后的pwd，当前目录</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 格式：</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> path</span></span><br><span class="line"><span class="comment"># 例如：</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> . /data</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>CMD： 在容器启动后执行相关命令</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 格式：</span></span><br><span class="line">    <span class="keyword">CMD</span><span class="bash"> [<span class="string">&quot;executable&quot;</span>,<span class="string">&quot;param1&quot;</span>,<span class="string">&quot;param2&quot;</span>] </span></span><br><span class="line">    <span class="keyword">CMD</span><span class="bash"> <span class="built_in">command</span> param1 param2 </span></span><br></pre></td></tr></table></figure>
<ul>
<li>如果 Dockerfile 中如果存在多个 CMD 指令，仅最后一个生效。</li>
</ul>
<p>参数名和参数值相连，分别为列表中的两个不同项，以gunicorn为例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gunicorn MainWebApp:app -c gunicorn.conf.py -t 100</span><br></pre></td></tr></table></figure>
<p>等价于</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">&quot;gunicorn&quot;</span>, <span class="string">&quot;MainWebApp:app&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;gunicorn.conf.py&quot;</span>, <span class="string">&quot;&quot;</span>-t<span class="string">&quot;, &quot;</span>100<span class="string">&quot;]</span></span></span><br></pre></td></tr></table></figure>
<p>列表中的参数和命令一定要使用双引号</p>
</li>
</ol>
]]></content>
      <categories>
        <category>技术框架</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker基础概念</title>
    <url>/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<p>主要包括三个基本核心概念：</p>
<ul>
<li><strong>镜像（image)</strong>：相当于一个静态的文件系统，类似于未安装的windows ios文件，相当于是一个未挂载的root文件系统</li>
<li><strong>容器（container)</strong>：容器是镜像运行时的实体，可以启动、创建、停止以及删除。容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间</li>
<li><strong>仓库（Repository）</strong>：镜像中心，类似于maven的依赖包中心，用来保存镜像，最常使用的公共仓库是官方的Docker Hub</li>
</ul>
<p>基本架构（cs架构）:</p>
<blockquote>
<p>守护进程（daemon）是生存期长的一种进程，没有控制终端。它们常常在系统引导装入时启动，仅在系统关闭时才终止</p>
</blockquote>
<ul>
<li><strong>Docker 客户端(Client)</strong>:与docker host中的守护进程进行通信，通过命令执行实际的操作（cs架构中的c）</li>
<li><strong>Docker 主机(Host)</strong>:运行容器，存储镜像的机器</li>
<li><strong>Docker Registry</strong>：一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。</li>
</ul>
<img src="/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210727162745322.png" class="" title="image-20210727162745322">
<span id="more"></span>
<p>从hello-word可以得到docker执行的基本流程</p>
<ol>
<li>client 连接 hosts 的daemon进程</li>
<li>daemon进程查看本地镜像，如果不存在，向远程Registry获取镜像</li>
<li>daemon进程为镜像启动容器</li>
<li>daemon进程将信息发送给client</li>
</ol>
<img src="/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210727162755137.png" class="" title="image-20210727162755137">
<h3 id="几个基本概念"><a href="#几个基本概念" class="headerlink" title="几个基本概念"></a>几个基本概念</h3><h4 id="1-容器与虚拟机的区别"><a href="#1-容器与虚拟机的区别" class="headerlink" title="1. 容器与虚拟机的区别"></a>1. 容器与虚拟机的区别</h4><p><strong>虚拟机</strong>，相当于一套真的操作系统，在os层上增加了一层hypervisor，用来虚拟化硬件，每个虚拟机以层虚拟化的硬件为基础，建立自己的OS层（GuestOa），应用层.</p>
<p><strong>容器</strong>，更偏向于一个进程隔离空间，空间内包含特定进程执行所需要的环境（镜像），容器之间互不干扰，但是共同调用底层的os接口，由daemon进程统一管理</p>
<img src="/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210727162804622.png" class="" title="image-20210727162804622">
<h4 id="2-Dockerfile相关总结"><a href="#2-Dockerfile相关总结" class="headerlink" title="2. Dockerfile相关总结"></a>2. Dockerfile相关总结</h4><h2 id="Docker基本使用（以某个项目部署为例）"><a href="#Docker基本使用（以某个项目部署为例）" class="headerlink" title="Docker基本使用（以某个项目部署为例）"></a>Docker基本使用（以某个项目部署为例）</h2><ol>
<li><p>在项目中添加Dockerfile(该文件用来提供构建镜像文件的必要信息)</p>
<img src="/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210727162814042.png" class="" title="image-20210727162814042">
</li>
<li><p>docker builder 构建镜像文件</p>
<ul>
<li>首先拉取基础docker镜像，之后将应用程序复制到docker容器中，根据应用程序依赖信息加载相关依赖，加载完成后执行初始run命令</li>
<li>-t 指明生成镜像文件的tag</li>
<li>. 指明dockerfile位置</li>
</ul>
</li>
<li><p>docker run 指定并绑定端口后运行项目</p>
<img src="/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210727162829498.png" class="" title="image-20210727162829498">
</li>
</ol>
<h3 id="docker基础镜像构建（搭建基础机器学习-深度学习环境）"><a href="#docker基础镜像构建（搭建基础机器学习-深度学习环境）" class="headerlink" title="docker基础镜像构建（搭建基础机器学习+深度学习环境）"></a>docker基础镜像构建（搭建基础机器学习+深度学习环境）</h3><blockquote>
<p>To stop a container, use <code>CTRL-c</code>. This key sequence sends <code>SIGKILL</code> to the container. If <code>--sig-proxy</code> is true (the default),<code>CTRL-c</code> sends a <code>SIGINT</code> to the container. If the container was run with <code>-i</code> and <code>-t</code>, you can detach from a container and leave it running using the <code>CTRL-p CTRL-q</code> key sequence.</p>
</blockquote>
<ol>
<li><p>使用ubuntu作为基础镜像,安装anaconda</p>
<ul>
<li><p>docker cp 复制文件到镜像中</p>
<img src="/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210726103848754.png" class="" title="image-20210726103848754">
</li>
<li><p>docker attach 切换到前台</p>
<img src="/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210726104207802.png" class="" title="image-20210726104207802">
</li>
<li><p><code>sh Anaconda3-2021.05-Linux-x86_64.sh</code> 执行安装脚本</p>
</li>
</ul>
</li>
<li><p>退出容器，将当前容器导出为镜像 docker commit</p>
<img src="/2021/07/27/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/Docker/Docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/image-20210726105455898.png" class="" title="image-20210726105455898">
</li>
<li><p>推送远程仓库（镜像名称中的用户名一定要与远程仓库用户名一致，否则报错）</p>
</li>
</ol>
]]></content>
      <categories>
        <category>技术框架</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>RabbitMQ基本概念和安装</title>
    <url>/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h2 id="RabbitMQ基本概念"><a href="#RabbitMQ基本概念" class="headerlink" title="RabbitMQ基本概念"></a>RabbitMQ基本概念</h2><p>如下图(来自<a href="https://baike.baidu.com/item/rabbitmq/9372144?fr=aladdin">百度百科</a>)，RabbitMQ是基于高级消息队列协议（AMQP）设计实现的消息中间件，主要包括的概念有交换机(Exchange)、消息队列(queue)、绑定(binding)、通道(channel)等。</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E5%AE%89%E8%A3%85/image-20230109104500878.png" class="" title="image-20230109104500878">
<span id="more"></span>
<p>下面基于AMQP协议的内容对上述概念进行总结，主要内容来自<a href="https://rubydoc.info/github/ruby-amqp/amqp/master/file/docs/AMQP091ModelExplained.textile">AMQP 0.9.1 Model Explained</a></p>
<h3 id="交换机-Exchange"><a href="#交换机-Exchange" class="headerlink" title="交换机(Exchange)"></a>交换机(Exchange)</h3><p>接收消息后根据消息的路由键和队列的绑定（bingding），将消息路由到对应队列中，交换机类型主要包括：</p>
<ol>
<li>直连（direct）：拿到消息后根据路由键是否等于对应队列的绑定键，直接将消息路由到对应队列。</li>
<li>扇形（Fanout）：将消息发送到当前交换机绑定的所有队列（广播路由）。</li>
<li>主题（topic）：通过将消息路由键与队列绑定模式匹配，将消息转发到一个或者多个匹配成功的队列（多播路由）。</li>
<li>头（head）：将路由键从字符串变为了多值属性，多值匹配实现路由。</li>
</ol>
<p>一个消息中间件包含多个交换机，交换机可以为持久的或者临时的。</p>
<h3 id="消息队列-Queue"><a href="#消息队列-Queue" class="headerlink" title="消息队列(Queue)"></a>消息队列(Queue)</h3><p>存储消息的一个FIFO队列，由客户端消费队列中的消息内容，队列包含一系列属性：</p>
<ol>
<li>Name：队列名</li>
<li>Durable：是否持久化</li>
<li>Exclusive：是否只有一个连接使用，连接关闭后即删除</li>
<li>Auto-delete：随后一个消费者退订后即删除</li>
</ol>
<p>客户端对于消息队列的消费方式可以分为主动（pull）和被动（push）两种方式，为了避免消息在成功消费前，在队列中删除，AMQP提供了两种确认建议：</p>
<ol>
<li>消息发送给应用后立刻删除</li>
<li>需要收到应用的消费ACK消息后，才能够删除</li>
</ol>
<p>当消息消费失败，队列会将消息重新添加到队列，投递给另外一个消费者消费，或者当前消费者的重新消费（<del>不是很清晰</del>）</p>
<h3 id="绑定-Binding"><a href="#绑定-Binding" class="headerlink" title="绑定(Binding)"></a>绑定(Binding)</h3><p>绑定是交换机根据消息路由键，将消息路由到的对应队列的”匹配规则“，根据交换机类型的不同、消息路由键的不同以及绑定规则的不同，一条消息可能发布给0-多个队列。</p>
<h3 id="连接和通道（Channel）"><a href="#连接和通道（Channel）" class="headerlink" title="连接和通道（Channel）"></a>连接和通道（Channel）</h3><p>AMQP是基于TCP可靠传输实现的应用层协议，每个客户端与消息中间件通过TCP长连接通信，为了避免一个客户端与中间件建立多个连接，引入了通道（Channel）的概念。</p>
<ul>
<li><p>在一个TCP连接上建立多个相互隔离的通道，每个通道单独的实现消息的发布/消费。</p>
</li>
<li><p>不同通道消息带有不同的通道号，客户端通过通道号区分不同通道消息。</p>
</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在我看来AMQP像是一种”设计模式“，设计和规定了消息中间件的具体实现方式，但不涉及实现细节，RabbitMQ是按照AMQP协议设计实现的消息中间件系统，因此其整体上的架构和逻辑必定满足AMQP概念，只是可能实现细节上有所区别，在进一步学习中总结。</p>
<h2 id="RabbitMQ安装"><a href="#RabbitMQ安装" class="headerlink" title="RabbitMQ安装"></a>RabbitMQ安装</h2><p>了解概念后的下一步就是安装实操，按照官网上<a href="https://www.rabbitmq.com/install-rpm.html">Installing on RPM-based Linux</a>安装教程主要有三种方式</p>
<ol>
<li><p>使用Docker环境运行，快速测试RabbitMQ</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> latest RabbitMQ 3.11</span></span><br><span class="line">docker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:3.11-management</span><br></pre></td></tr></table></figure>
</li>
<li><p>基于 <a href="https://www.rabbitmq.com/install-rpm.html#cloudsmith">Cloudsmith.io</a> or <a href="https://www.rabbitmq.com/install-rpm.html#package-cloud">PackageCloud</a>的yum安装(<strong>官方推荐</strong>)</p>
<ul>
<li>默认仓库的RabbitMq版本往往落后非常多</li>
<li>RabbitMQ官方团队会将最新的RabbitMQ RPM包发布在 Cloudsmith.ior和PackageCloud</li>
</ul>
</li>
<li><p>RPM包安装法</p>
<ul>
<li>逐个下载对应的RPM包，使用<code>RPM</code>命令安装</li>
</ul>
</li>
</ol>
<p>RabbitMQ的依赖包有三个：</p>
<ol>
<li><code>erlang</code></li>
<li><code>socat</code></li>
<li><code>logrotate</code></li>
</ol>
<h4 id="使用方法2安装"><a href="#使用方法2安装" class="headerlink" title="使用方法2安装"></a>使用方法2安装</h4><p>RPM安装较为简单，这里尝试一下方法2</p>
<ol>
<li><p>配置PakageCloud仓库</p>
<ul>
<li>或者可以直接在<code>/etc//etc/yum.repos.d</code>中添加新仓库的配置文件，添加对应配置</li>
</ul>
</li>
</ol>
<ul>
<li><p>脚本执行结果（生成的配置文件命名有问题，需要按照手动修改配置）</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E5%AE%89%E8%A3%85/image-20230109151115538.png" class="" title="image-20230109151115538">
</li>
<li><p>更新本地源仓库，<code>--disablerepo</code> 首先关闭所有仓库，<code>--enablerepo</code> 然后开启两个仓库</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum -q makecache -y --disablerepo=&#x27;*&#x27; --enablerepo=&#x27;rabbitmq_erlang&#x27; --enablerepo=&#x27;rabbitmq_server&#x27;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol>
<li><p>依次安装对应RPM包</p>
<ul>
<li><p>安装<code>socat</code>和<code>logrotate</code>包（使用默认源即可）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install socat logrotate -y</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装<code>erlang</code>和<code>rabbitmq-server</code>（使用替换后的新源）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install --repo rabbitmq_erlang --repo rabbitmq_server erlang rabbitmq-server -y</span><br></pre></td></tr></table></figure>
</li>
<li><p>可以看到已是最新版本的<code>rabbitmq</code></p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E5%AE%89%E8%A3%85/image-20230109162721916.png" class="" title="image-20230109162721916">
</li>
<li><p>下载了一下午，下载总是超时,放弃了该方法</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E5%AE%89%E8%A3%85/image-20230109191011158.png" class="" title="image-20230109191011158">
</li>
</ul>
</li>
<li><p>从Github上查找对应<code>RPM</code>包</p>
<ul>
<li><p>RabbitMQ和erlang的对应关系：<a href="https://www.rabbitmq.com/which-erlang.html">RabbitMQ Erlang Version Requirements</a></p>
</li>
<li><p><a href="https://github.com/rabbitmq/rabbitmq-server/releases?q=erlang&amp;expanded=true">rabbitmq3.11.6</a>，<a href="https://github.com/rabbitmq/erlang-rpm/releases">erlang-25.1.2</a></p>
</li>
<li><p>使用RPM命令安装</p>
</li>
</ul>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E5%AE%89%E8%A3%85/image-20230109191347397.png" class="" title="image-20230109191347397">
</li>
<li><p>前台启动查看是否安装成功</p>
<ul>
<li>成功安装并启动</li>
</ul>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E5%AE%89%E8%A3%85/image-20230109191603500.png" class="" title="image-20230109191603500">
</li>
</ol>
<h4 id="基本管理命令"><a href="#基本管理命令" class="headerlink" title="基本管理命令"></a>基本管理命令</h4><p>设置自动启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chkconfig rabbitmq-server on</span><br></pre></td></tr></table></figure>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E5%AE%89%E8%A3%85/image-20230109191803692.png" class="" title="image-20230109191803692">
<p>以系统服务形式管理RabbitMQ服务(<code>systemctl</code>同样可以)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">service rabbitmq-server start</span><br><span class="line">service rabbitmq-server status</span><br><span class="line">service rabbitmq-server stop</span><br></pre></td></tr></table></figure>
<p>开启web管理页面插件，对应端口号为<code>15672</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rabbitmq-plugins enable rabbitmq_management</span><br></pre></td></tr></table></figure>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E5%AE%89%E8%A3%85/image-20230110090005169.png" class="" title="image-20230110090005169">
<p><code>guest</code>用户只支持本机登录，需要创建用户：</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E5%AE%89%E8%A3%85/image-20230110090725725.png" class="" title="image-20230110090725725">
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E5%AE%89%E8%A3%85/image-20230110090818103.png" class="" title="image-20230110090818103">]]></content>
      <categories>
        <category>技术框架</category>
        <category>RabbitMQ</category>
      </categories>
  </entry>
  <entry>
    <title>RabbitMQ死信队列</title>
    <url>/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97/</url>
    <content><![CDATA[<p>对于队列中存在的部分无法消费的消息（dead-lettered），RabbitMQ提供”死信队列”机制，实现死信的收集处理。死信队列还是普通的队列，只是其于某个队列绑定，当该队列出现私信时会将死信转发到该队列中，提供给该队列消费者处理。</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97/image-20230112161219616.png" class="" title="image-20230112161219616">
<span id="more"></span>
<p>当出现以下三种情况时，消息会变为死信：</p>
<ol>
<li>消息超时未消费（TTL）。</li>
<li>消息被消费者拒绝(<code>nack</code>、<code>reject</code>)，且<code>requeue</code>参数为false，即不重新入队。</li>
<li>消息队列中消息数达到上限，将新消息丢弃。</li>
</ol>
<p>设置一个死信队列的基本流程为：</p>
<ol>
<li><p>声明一个交换机和队列，作为死信交换机和队列</p>
</li>
<li><p>声明普通交换机和对应的队列，并绑定对应的死信交换机以及路由key（设置队列的<code>dead-letter-exchange</code>属性和<code>x-dead-letter-routing-key</code>属性）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">channel.exchangeDeclare(<span class="string">&quot;deadLetterex&quot;</span>, <span class="string">&quot;direct&quot;</span>);</span><br><span class="line">Map&lt;String, Object&gt; args = <span class="keyword">new</span> HashMap&lt;String, Object&gt;();</span><br><span class="line">args.put(<span class="string">&quot;x-dead-letter-exchange&quot;</span>, <span class="string">&quot;deadLetterex&quot;</span>);</span><br><span class="line">args.put(<span class="string">&quot;x-dead-letter-routing-key&quot;</span>, <span class="string">&quot;deadLetterkey&quot;</span>);</span><br><span class="line"><span class="comment">//设置队列属性</span></span><br><span class="line">channel.queueDeclare(<span class="string">&quot;myqueue&quot;</span>, <span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">false</span>, args);</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>与其说死信队列，不如说是队列的一种特殊机制，将”死信”转发到实现设置好的交换机中。</p>
<h3 id="Java实现测试"><a href="#Java实现测试" class="headerlink" title="Java实现测试"></a>Java实现测试</h3><ol>
<li><p>初始化死信队列</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> IOException, TimeoutException </span>&#123;</span><br><span class="line">    Channel c = RabbitMQUtils.getChannel();</span><br><span class="line">    <span class="comment">//首先声明死信交换机和队列</span></span><br><span class="line">    c.exchangeDeclare(DLX, BuiltinExchangeType.DIRECT);</span><br><span class="line">    c.queueDeclare(DLX_QUE, <span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">null</span>);c.queueBind(DLX_QUE, DLX, DLX_KEY);</span><br><span class="line">    <span class="comment">//声明普通交换机和队列</span></span><br><span class="line">    c.exchangeDeclare(NORMAL_X, BuiltinExchangeType.DIRECT);</span><br><span class="line">    <span class="comment">//设置dead-letter-exchange属性</span></span><br><span class="line">    Map&lt;String, Object&gt; properties = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    properties.put(<span class="string">&quot;x-dead-letter-exchange&quot;</span>, DLX);</span><br><span class="line">    properties.put(<span class="string">&quot;x-dead-letter-routing-key&quot;</span>, DLX_KEY);</span><br><span class="line">    <span class="comment">//声明普通队列</span></span><br><span class="line">    c.queueDeclare(NORMAL_QUE, <span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">false</span>, properties);</span><br><span class="line">    c.queueBind(NORMAL_QUE, NORMAL_X, NORMAL_KEY);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>生产者和消费者代码较为简单，只展示消费者对于消息的处理逻辑</p>
<ul>
<li><p>为了展示死信队列的效果，普通队列消费者会根据消息内容，进行拒绝和休眠</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DeliverCallback deliverCallback = <span class="keyword">new</span> DeliverCallback() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(String consumerTag, Delivery message)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        String[] allMsg = <span class="keyword">new</span> String(message.getBody()).split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="keyword">long</span> tag = message.getEnvelope().getDeliveryTag();</span><br><span class="line">        <span class="keyword">if</span>(Integer.parseInt(allMsg[<span class="number">0</span>]) == <span class="number">1</span>)&#123;</span><br><span class="line">            c.basicReject(tag, <span class="keyword">false</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(Long.parseLong(allMsg[<span class="number">1</span>]) * <span class="number">1000</span>);</span><br><span class="line">                System.out.println(<span class="string">&quot;[normal] consume:&quot;</span> + tag</span><br><span class="line">                                   + <span class="string">&quot;after &quot;</span> + Long.parseLong(allMsg[<span class="number">1</span>]) + <span class="string">&quot;s get:&quot;</span> + allMsg[<span class="number">2</span>]);</span><br><span class="line">                c.basicAck(tag, <span class="keyword">false</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>死信队列消费者输出消息头中的<code>death_reason</code>属性</p>
</li>
</ul>
</li>
</ol>
<p>展示效果：</p>
<ol>
<li><p>由于消息被拒绝，导致进入死信队列</p>
<ul>
<li>输出原因为<code>rejected</code></li>
</ul>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97/image-20230112184257699.png" class="" title="image-20230112184257699">
</li>
<li><p>超时导致进入死信队列（ttl设置为1s）</p>
<ul>
<li><p>关闭消费者，输出原因为<code>expired</code></p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97/image-20230112190007610.png" class="" title="image-20230112190007610">
</li>
<li><p>诡异的是如果一条消息被消费者接收了但未<code>ack</code>，队列中的消息并不会超时（第一条消息让消费者休眠20秒，后两条消息未超时）</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97/image-20230112190521916.png" class="" title="image-20230112190521916">
</li>
<li><p>查看对应的消息状态为<code>Unacked</code></p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97/image-20230112191503040.png" class="" title="image-20230112191503040">
</li>
<li><p>添加<code>c.basicQos(1)</code>代码，避免提前投递</p>
<ul>
<li><p>此时消息状态为<code>ready</code></p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97/image-20230112191833449.png" class="" title="image-20230112191833449">
</li>
<li><p>正常超时</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97/image-20230112191851947.png" class="" title="image-20230112191851947">
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="基于插件的延时队列实现"><a href="#基于插件的延时队列实现" class="headerlink" title="基于插件的延时队列实现"></a>基于插件的延时队列实现</h3><p>由于基于死信队列实现延时队列的队尾处理与超时处理的逻辑不一致，因此存在另一种通过插件实现延时队列的方式，通过安装<code>rabbitmq_delayed_message_exchange</code>插件，在Rabbitmq中增加一种支持消息延迟的交换机，在消息超时后才将消息投递到对应的队列中。</p>
<p>下载<a href="https://github.com/rabbitmq/rabbitmq-delayed-message-exchange/releases">对应版本插件</a>到rabbitmq的<code>plugins</code>目录中，安装插件并重启</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97/image-20230207090241669.png" class="" title="image-20230207090241669">
<p>通过web页面可以看到新增加了<code>x-delayed-message</code>类型的交换机，需要设置消息投递方式</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97/image-20230207091014278.png" class="" title="image-20230207091014278">
<h3 id="结论以及补充"><a href="#结论以及补充" class="headerlink" title="结论以及补充"></a>结论以及补充</h3><ol>
<li><p>TTL控制的是指消息<code>Ready</code>状态的时间，一旦被投送到消费者，转换为<code>Unacked</code>状态</p>
</li>
<li><p>由于队列排队消费的性质，队列中的消息即使超时，也要<strong>等到其到达队尾，才会进行处理</strong>（丢弃，进入死信队列等）</p>
</li>
<li><p>默认同时向消费者投送不限量的消息，即使最新的消息未<code>ack</code></p>
</li>
<li><p>可以对队列设置超时时间，也可以为每条消息单独设置超时时间，两者较小的作为最终的超时时间</p>
<blockquote>
<p> When both a per-queue and a per-message TTL are specified, the lower value between the two will be chosen.</p>
</blockquote>
</li>
</ol>
<p>​        </p>
]]></content>
      <categories>
        <category>技术框架</category>
        <category>RabbitMQ</category>
      </categories>
  </entry>
  <entry>
    <title>RabbitMQ集群相关</title>
    <url>/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E9%9B%86%E7%BE%A4%E7%9B%B8%E5%85%B3/</url>
    <content><![CDATA[<h2 id="RabbitMQ集群相关"><a href="#RabbitMQ集群相关" class="headerlink" title="RabbitMQ集群相关"></a>RabbitMQ集群相关</h2><blockquote>
<p>Clustering is meant to be used across LAN. It is not recommended to run clusters that span WAN.</p>
</blockquote>
<p>似乎所有的中间件框架最后都要走到集群配置这一步，RabbitMQ通过集群配置实现数据和服务的可要行，官网推荐在局域网中使用RabbitMQ集群，跨网络间的同步建议通过 <a href="https://www.rabbitmq.com/shovel.html">Shovel</a> or <a href="https://www.rabbitmq.com/federation.html">Federation</a>等插件实现。</p>
<p>RabbitMQ集群主要解决服务器崩溃导致的服务不可用以及数据丢失问题，集群中节点同步备份所有的元数据，由于集群本身并不执行队列的备份，通过特殊类型的队列（镜像和quorum）实现队列备份。</p>
<ol>
<li>队列元数据：并不同步队列本身以及队列中的消息</li>
<li>交换机元数据</li>
<li>绑定元数据</li>
<li>vhost元数据</li>
</ol>
<span id="more"></span>
<h3 id="配置RabbitMQ集群"><a href="#配置RabbitMQ集群" class="headerlink" title="配置RabbitMQ集群"></a>配置RabbitMQ集群</h3><p>采用docker镜像的形式，在一台机器上运行三个RabbitMQ，实现模拟集群配置效果。</p>
<ol>
<li><p>首先下载带有管理界面的rabbitmq镜像</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E9%9B%86%E7%BE%A4%E7%9B%B8%E5%85%B3/image-20230208145008994.png" class="" title="image-20230208145008994">
</li>
<li><p>运行起来三个rabbitmq容器(<code>rabbit1</code>、<code>rabbit2</code>、<code>rabbit3</code>)</p>
<ul>
<li><code>RABBITMQ_ERLANG_COOKIE=&#39;cluster_cookie&#39;</code>：设置相同的erlang cookie</li>
<li><code>--link 容器名</code>：解决同一台主机上不同容器通信问题</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run -d --hostname rabbit3 --name instance3 -p 15674:15672 -p 5674:5672 --link instance1 --link instance2 -e RABBITMQ_ERLANG_COOKIE=&#x27;cluster_cookie&#x27; rabbitmq:management</span><br></pre></td></tr></table></figure>
</li>
<li><p>进入<code>rabbit2</code>、<code>rabbit3</code>依次加入集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入容器内部</span></span><br><span class="line">docker exec -it instance2 /bin/bash</span><br><span class="line"><span class="meta">#</span><span class="bash"> on rabbit2</span></span><br><span class="line">rabbitmqctl stop_app</span><br><span class="line"><span class="meta">#</span><span class="bash"> =&gt; Stopping node rabbit@rabbit2 ...<span class="keyword">done</span>.</span></span><br><span class="line">rabbitmqctl reset</span><br><span class="line"><span class="meta">#</span><span class="bash"> =&gt; Resetting node rabbit@rabbit2 ...</span></span><br><span class="line">rabbitmqctl join_cluster rabbit@rabbit1</span><br><span class="line"><span class="meta">#</span><span class="bash"> =&gt; Clustering node rabbit@rabbit2 with [rabbit@rabbit1] ...<span class="keyword">done</span>.</span></span><br><span class="line">rabbitmqctl start_app</span><br><span class="line"><span class="meta">#</span><span class="bash"> =&gt; Starting node rabbit@rabbit2 ...<span class="keyword">done</span>.</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>进入任意管理页面，可以看到集群配置成功</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E9%9B%86%E7%BE%A4%E7%9B%B8%E5%85%B3/image-20230208151815168.png" class="" title="image-20230208151815168">
</li>
</ol>
<p>测试在集群上进行操作:</p>
<ol>
<li><p>生产者在<code>rabbit2</code>上创建队列，消费者在<code>rabbit1</code>上消费对应队列</p>
<ul>
<li><p>接收者同样能够收到消息</p>
</li>
<li><p>符合官方文档中的对于队列消息同步的描述，即消息在所有节点上均可达</p>
<blockquote>
<p>An exception to this are message queues, which by default reside on one node, though they are visible and reachable from all nodes.</p>
</blockquote>
</li>
</ul>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E9%9B%86%E7%BE%A4%E7%9B%B8%E5%85%B3/image-20230208152710962.png" class="" title="image-20230208152710962">
</li>
</ol>
<h3 id="队列备份"><a href="#队列备份" class="headerlink" title="队列备份"></a>队列备份</h3><p>从上述集群操作中，可以看出虽然引入了集群，队列还是单机存储，其他节点也需要从当前队列节点上获取消息返回给消费者，因此RabbitMQ引入了两种特殊的队列结构</p>
<ol>
<li><p>镜像队列 </p>
<blockquote>
<p><strong>Important</strong>: mirroring of classic queues will be <strong>removed in a future version</strong> of RabbitMQ. Consider using <a href="https://www.rabbitmq.com/quorum-queues.html">quorum queues</a> or a non-replicated classic queue instead.</p>
</blockquote>
</li>
<li><p>quorum 队列（3.8版本引入，替代镜像队列的版本）</p>
</li>
<li><p>RabbitMQ自3.9.0版本开始引入的Stream队列类型（未深入了解）：消息以append-only的方式持久化到日志文件中，然后通过调整每个消费者的消费进度offset，来实现消息的多次分发</p>
</li>
</ol>
<h4 id="镜像队列"><a href="#镜像队列" class="headerlink" title="镜像队列"></a>镜像队列</h4><p>镜像队列类似于Mysql的主从同步，由一个leader replica(主副本)和多个mirrors(从副本)组成，对于发布消息，传递消息到消费者以及跟踪消息ACK等队列操作，由leader replica首先应用后，再将操作同步到mirrors。</p>
<ul>
<li><p>只能提供队列备份功能，只能提升数据的可靠性，无法实现性能的横向扩展。</p>
<blockquote>
<p> Consumers are connected to the leader regardless of which node they connect to, with mirrors dropping messages that have been acknowledged at the leader. </p>
</blockquote>
</li>
<li><p>当leader replica失效时，同步时间最长的mirrors会成为新的leader replica，无论其是否已经与leader replica同步。</p>
</li>
</ul>
<p>下面演示镜像队列的使用</p>
<ol>
<li><p>通过<code>policy</code>设置以<code>mirror</code>开头的队列的镜像规则</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E9%9B%86%E7%BE%A4%E7%9B%B8%E5%85%B3/image-20230208160019330.png" class="" title="image-20230208160019330">
</li>
<li><p>创建<code>mirror_test</code>队列，发现备份成功</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E9%9B%86%E7%BE%A4%E7%9B%B8%E5%85%B3/image-20230208160054338.png" class="" title="image-20230208160054338">
</li>
<li><p>关闭rabbit2后，查看队列状态</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E9%9B%86%E7%BE%A4%E7%9B%B8%E5%85%B3/image-20230208160202697.png" class="" title="image-20230208160202697">
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E9%9B%86%E7%BE%A4%E7%9B%B8%E5%85%B3/image-20230208160428279.png" class="" title="image-20230208160428279">
</li>
</ol>
<p>​    4. 连接rabbit1，消费消息</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E9%9B%86%E7%BE%A4%E7%9B%B8%E5%85%B3/image-20230208160357089.png" class="" title="image-20230208160357089">
<h4 id="Quorum-队列"><a href="#Quorum-队列" class="headerlink" title="Quorum 队列"></a>Quorum 队列</h4><p>官方文档对于Quorum队列特性总结的比较好,不再进行赘述。对应中文版本为：<a href="https://www.cnblogs.com/oyld/p/16783108.html">RabbitMQ 3.8 Feature Focus - Quorum Queues</a></p>
]]></content>
      <categories>
        <category>技术框架</category>
        <category>RabbitMQ</category>
      </categories>
  </entry>
  <entry>
    <title>Redis分布式相关</title>
    <url>/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/</url>
    <content><![CDATA[<p>Redis支持多机多实例部署，通过多实例部署，可以保证服务不受到单个Redis示例宕机的影响，实现服务的数据的可靠性和高可用，下面结合代码和底层原理，加深对于Redis多机部署的理解，主要内容来自<em>Redis设计与实现</em></p>
<h3 id="主从复制"><a href="#主从复制" class="headerlink" title="主从复制"></a>主从复制</h3><p>Redis的主从复制在我看来较为轻量级，由从节点向主节点申请成为从节点，主节点不断向从节点发送数据，实现同步，与Mysql主从复制不同的是，即使在同步前主从状态不一致，同步后也会变为一致。</p>
<p>通过<code>replicaof</code>命令可制定当前Redis实例为对应实例的从机关系，重启即失效：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> replicaof 主机IP 端口</span></span><br><span class="line">replicaof 192.168.1.1 6379</span><br><span class="line"><span class="meta">#</span><span class="bash"> 取消作为从机</span></span><br><span class="line">replicate no one</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p>或者在配置文件中永久开启：</p>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/image-20221221185800217.png" class="" title="image-20221221185800217">
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/image-20221221185745857.png" class="" title="image-20221221185745857">
<p>开启主从备份后，可以在任意Redis实例上通过<code>info replication</code>命令查看同步信息</p>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/image-20221221155936348.png" class="" title="image-20221221155936348">
<p>若主机配置了<code>requirepass</code>，需要在配置文件中添加对应连接密码</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 临时设置</span></span><br><span class="line">config set masterauth &lt;password&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置文件中添加</span></span><br><span class="line">masterauth &lt;password&gt;</span><br></pre></td></tr></table></figure>
<p>其他的相关注意事项有：</p>
<ol>
<li>参与主从复制的任意Redis实例宕机，其他Redis实例仍保持原来的身份，导致如果主机宕机丢失数据，在空数据的条件下启动，会导致所有从机同步为空（<del>这一点有悖于复制的初衷</del>）。</li>
<li>Redis主从复制采用异步复制策略，主机向从机发送操作后并不会等待操作返回。</li>
<li>对于带有过期时间(TTL)的键值对，从库不会主动删除，等待主机删除后向从机发送同步删除命令，只是根据本地逻辑时钟判断是否向客户端返回空。</li>
</ol>
<h4 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h4><p>主从同步的主要操作按照连接状况的不同分为以下几种情况：</p>
<ol>
<li>主机与从机正常连接。主机不断地将写操作发送到从机，从机执行操作实现状态同步，这个过程为异步的，主机并不等待从机返回。</li>
<li>从机与主机完全不同步状态。主机接收到请求后，启动后台线程持久化当前状态到RDB文件中，写入完毕后，发送给从机读取，实现状态同步。</li>
<li>从机与主机处于部分同步状态。主机根据两者数据之间的差距，发送从机缺失的操作，从机读取执行，实现状态同步，例如从机断线，网络分区等场景导致。</li>
</ol>
<p>老版本Redis支支持情况2对应的<strong>完全同步(sync)</strong>操作，对于情况3同样采用完全同步，这导致即使从机只由于网络波动缺失了主机的部分操作，而主机还是需要重新生成RDB文件，造成了效率的下降。为了解决上述问题，新版本Redis引入了<strong>部分同步操作(psync)</strong>，在上述场景中主机能够识别从机缺失部分，直接发送缺失部分操作。</p>
<p>为了支持部分同步操作，每个主机上添加三个数据结构：</p>
<ol>
<li>复制ID（replication ID）：主机启动时初始化一个独一无二的ID（单机自己为主机），用来标识本机数据，同时每个实例记录自己的上一个复制ID，原因在<a href="https://redis.io/docs/management/replication/">官方文档</a>中描述了。</li>
<li>主机复制偏移量（replication offset）：每当主机需要从机发送写操作时，在复制偏移量上加上操作对应发送的数据字节长度，可以理解为逻辑上的操作时间戳</li>
<li>主机复制积压缓冲区（replication backlog）：固定长度的先进先出缓冲区，缓存一定数量的主机向从机发送的最新复制数据，主机在向从机发送复制数据时，也会写入backlog。</li>
</ol>
<p>从上面三个数据结构，不难想到Redis是如何实现部分同步操作(psync)：主机复制ID用来比较从机的数据是否与本机来自一个”数据”，复制偏移量用来判断从机相较于主机缺失多少数据，复制挤压缓冲区存历史数据代表主机能够恢复的数据。</p>
<ol>
<li>backlog 长度 &gt; 主机 replication offset - 从机 replication offset，主机携带了所有从机缺失的数据，进行<strong>部分同步(psync)</strong></li>
<li>backlog 长度 &lt; 主机 replication offset - 从机 replication offset，主机无能为例，只能<strong>完全同步(sync)</strong></li>
</ol>
<p>replication ID在区别“数据”版本上具有重要作用，其变化时机为：</p>
<ol>
<li>实例重新启动时</li>
<li>当一个从实例被设置为主实例时</li>
</ol>
<p>从实例会主动向主服务器发送心跳包，实现检测主服务器网络状态，告知主服务器自身的复制偏移量。</p>
<h3 id="哨兵模式"><a href="#哨兵模式" class="headerlink" title="哨兵模式"></a>哨兵模式</h3><p>在读写分离的主从复制模式下，如果主节点失效，从节点并不会自己重新选出新的主节点，导致系统陷入只读状态，因此Redis引入了<strong>哨兵模式</strong>：一种特殊的Redis服务器，用来监控主节点的状态，当主节点失效时，该服务器会选择特定的从服务器成为新的主服务器，其他服务器（包括恢复后的老主服务器）修改为新主的从服务器。</p>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/%E7%BB%98%E5%9B%BE1.png" class="" title="绘图1">
<p>Sentinel模式实现的基本原理如下（简单概括）：</p>
<ol>
<li>Sentinel是运行在特殊模式下的Redis服务器，通过读取指定的配置文件，获悉监视的主服务器IP以及端口。</li>
<li>通过向主服务器发送请求，获取所有的从服务器信息，保存完整的主从服务器信息。</li>
<li>Sentinel周期性的向所有主从服务器发送INFO命令，当主服务超过一定时间未有效响应时，即认为主服务器失效，当足够多的Sentinel认为主服务下线时，进入故障切换阶段（基于Raft选主多的Sentinel）。</li>
<li>主Sentinel选择一个从服务器发送<code>replica no one</code>命令,其他发送<code>replicaof</code>命令，完成新的主服务器切换。</li>
<li>失效的旧主服务器上限后，主Sentinel也会发送<code>replicaof</code>命令</li>
</ol>
<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><p>启用Sentinel相关配置项如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 监控的主Redis服务器，最后一个</span></span><br><span class="line">sentinel monitor mymaster 127.0.0.1 6379 2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 判断下线的时间阈值</span></span><br><span class="line">sentinel down-after-milliseconds mymaster 60000</span><br><span class="line"><span class="meta">#</span><span class="bash"> 故障转移阶段超时时间阈值</span></span><br><span class="line">sentinel failover-timeout mymaster 180000</span><br></pre></td></tr></table></figure>
<p>指定配置文件的方式进行启动：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-sentinel /path/to/sentinel.conf</span><br><span class="line">redis-server /path/to/sentinel.conf --sentinel</span><br></pre></td></tr></table></figure>
<p>启动成功后，输出监视信息（一主一从）：</p>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/image-20221221200153194.png" class="" title="image-20221221200153194">
<p>手动关闭Salve，查看输出：</p>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/image-20221221195316557.png" class="" title="image-20221221195316557">
<p>启动Slave后，关闭Master查看输出：</p>
<ul>
<li>显示完成切换</li>
</ul>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/image-20221221201219757.png" class="" title="image-20221221201219757">
<p>在Slave查看主从备份信息，发现切换成功：</p>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/image-20221221201301948.png" class="" title="image-20221221201301948">
<h3 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h3><p>Redis集群是Redis的分布式数据库方案，支持数据分片和复制，实现数据库的水平扩展，一个典型的Redis集群组成如下：</p>
<ol>
<li>一系列主服务器，存储部分键值对，提供读写服务。</li>
<li>一系列从服务器，复制主服务器数据，在主服务器失效时替代主服务器提供服务。</li>
</ol>
<p>键空间划分为共16384个槽(slot)，将槽分别存储在不同的主服务器上，实现所谓的<strong>分片</strong>（Sharding），采用CRC16算法实现key到slot的映射，其转化计算方式为：</p>
<ul>
<li>Node A contains hash slots from 0 to 5500.</li>
<li>Node B contains hash slots from 5501 to 11000.</li>
<li>Node C contains hash slots from 11001 to 16383.</li>
</ul>
<script type="math/tex; mode=display">
slot = CRC16(key)\%16384</script><p>Redis集群采用完全去中心化的实现方式，客户端可访问任意集群服务器，当访问操作的键值对不在当前服务器（槽不在当前服务器或者正处于槽迁移阶段）上时，当前服务器返回对应key所在服务器位置，并指导客户端重定向到对应服务器。</p>
<p>其他包括故障检测、故障恢复等均通过节点间的消息传递实现，当主服务器宕机，基于Raft算法从多个从服务器中选择成为新的主服务器</p>
<h4 id="使用-1"><a href="#使用-1" class="headerlink" title="使用"></a>使用</h4><p>按照官网教程的方式，采用一台虚拟机多个Redis进程的方式，学习Redis集群的配置和使用。</p>
<p>首先构建六个配置文件，下面为最小配置内容，不同配置文件需要修改端口号，集群配置文件名</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">port 7000 # 不同节点不同</span><br><span class="line">cluster-enabled yes</span><br><span class="line">cluster-config-file nodes.conf # 不同节点不同</span><br><span class="line">cluster-node-timeout 5000</span><br><span class="line">appendonly yes</span><br></pre></td></tr></table></figure>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/image-20221222172140192.png" class="" title="image-20221222172140192">
<p>加载配置文件，启动对应Redis实例</p>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/image-20221222180459540.png" class="" title="image-20221222180459540">
<p>初始化集群，完成集群搭建（六个，三个主，三个从）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-cli --cluster create 127.0.0.1:7001 127.0.0.1:7002 \</span><br><span class="line">127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 127.0.0.1:7006 \</span><br><span class="line">--cluster-replicas 1</span><br></pre></td></tr></table></figure>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/image-20221222180631919.png" class="" title="image-20221222180631919">
<p>操作集群</p>
<ol>
<li><p>访问不在当前节点上的Key</p>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/image-20221222181004371.png" class="" title="image-20221222181004371">
</li>
<li><p>查看指定键所属的slot</p>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/image-20221222181338047.png" class="" title="image-20221222181338047">
</li>
<li><p>查看集群信息</p>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/image-20221222181523186.png" class="" title="image-20221222181523186">
</li>
</ol>
<p>关闭节点7003后，查看集群状态</p>
<ul>
<li><p>可以看出，此时的7003状态为Failed，其从节点7005成为新的主节点</p>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/image-20221222181945418.png" class="" title="image-20221222181945418"></li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
        <category>redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis持久化</title>
    <url>/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E6%8C%81%E4%B9%85%E5%8C%96/</url>
    <content><![CDATA[<p>Redis支持两种持久化手段，下面分别进行总结</p>
<ol>
<li>RDB（Redis DataBase）：将当前Redis数据库中的所有数据以二进制的形式存储在.rbd文件中，也就是<strong>状态备份</strong>。</li>
<li>AOF（Append Only File）：将写操作持久化在AOF文件中，也就是<strong>状态机备份</strong>。</li>
</ol>
<h3 id="RDB"><a href="#RDB" class="headerlink" title="RDB"></a>RDB</h3><p>RDB机制将当前Redis存储的所有KV对存储在二进制文件中，持久化的触发时机有两个：</p>
<ol>
<li><p>显式调用<code>save</code>或<code>bgsave</code></p>
</li>
<li><p>满足配置文件中的持久化条件</p>
<ul>
<li>在过去的xxx秒内，若写入操作超过xx个，则进行RDB持久化</li>
</ul>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E6%8C%81%E4%B9%85%E5%8C%96/image-20221210101328909.png" class="" title="image-20221210101328909">
</li>
</ol>
<span id="more"></span>
<p>为了避免写入RDB阻塞Redis响应客户端的写入请求，Redis通过<code>fork()</code>生成子进程负责执行RDB生成操作。</p>
<p>Linux中的<code>fork()</code>操作基于<code>copy-on-write(COW)</code>机制，将子进程的内存复制推迟到了父进程执行写入操作。</p>
<ul>
<li>无<code>COW</code>机制的<code>fork()</code>：创建子进程时，子进程与父进程共享正文段，开辟新空间复制父进程的数据段，队，栈。</li>
<li>Linux中的<code>fork()</code>：创建子进程时，只为子进程创建虚拟空间，与父进程共享物理空间，当父进程修改对应物理空间数据库，再为子进程开辟空间复制数据。</li>
</ul>
<p>Redis的后台RDB机制基于Linux中的<code>fork()</code>的<code>copy-on-write(COW)</code>，因此其生成的RDB文件保存的是<strong>发起RDB持久化时刻的Rdis数据库状态</strong>。</p>
<h3 id="AOF"><a href="#AOF" class="headerlink" title="AOF"></a>AOF</h3><p>AOF(Apeend Only File)通过记录<code>变更</code>持久化数据库状态，<code>变更</code>包括<code>set</code>，<code>hset</code>，<code>del</code>等数据库写入操作，其实实现机制类似于Mysql中的<code>redo log</code></p>
<ul>
<li><p>每次写入操作执行后，写入到位于内存的AOF缓冲区<code>aof_buf</code>中</p>
</li>
<li><p>刷新时机由配置文件参数<code>appendfsync</code>控制（<del>和Mysql一毛一样</del>）</p>
<ol>
<li><code>no</code>：只写入文件，但是不调用文件系统刷新接口<code>fsync()</code></li>
<li><code>everysec</code>：写入文件，若距离上次调用<code>fsync()</code>超过一秒，则调用</li>
<li><code>always</code>：每次写入后调用<code>fsync()</code></li>
</ol>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E6%8C%81%E4%B9%85%E5%8C%96/image-20221210102825441.png" class="" title="image-20221210102825441">
</li>
</ul>
<p>由于AOF采用了记录操作的思路实现持久化，相较于RDB，带来了两个新问题</p>
<ol>
<li>重启无法像RDB文件一样，直接加载到内容中即可<ul>
<li>Redis采取“伪客户端”逐个执行操作的方式，实现启动时的状态还原</li>
</ul>
</li>
<li>随着Redis执行的操作越来越多，AOF文件大小也会越来越大，重启重新执行的时间也会过长<ul>
<li>Redis使用异步AOF重写机制，实现AOF文件的缩小</li>
<li>AOF重写机制：基于当前状态构建生成当前数据库状态的操作，生成新的AOF文件（根据结果反推过程替代真正的过程），例如<code>sadd a,sadd b,s add c-&gt;sadd a b c</code></li>
<li>对应命令为<code>BGREWRITEAOF</code><br>除主动调用外，当<code>AOF</code>文件大于配置文件中设置大小时，后台自动启动AOF重写操作。</li>
</ul>
</li>
</ol>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E6%8C%81%E4%B9%85%E5%8C%96/image-20221211085143603.png" class="" title="image-20221211085143603">
<h3 id="RDB-vs-AOF"><a href="#RDB-vs-AOF" class="headerlink" title="RDB vs AOF"></a>RDB vs AOF</h3><p>RDB和AOF都是Redis提供的实现持久化的手段，两者的区别主要在于：</p>
<ol>
<li>RDB是对Redis状态的持久化，AOF是对Redis操作的持久化，因此AOF的粒度更细</li>
<li>RDB以二进制形式存储，AOF存储的是一个一个修改操作，因为RDB文件更小</li>
<li>在Redis重新启动时，RDB形式的备份只需要加载，AOF则需要逐个操作读取执行</li>
</ol>
<p>上述区别导致了AOF更适合数据持久性要求更高的应用场景，RDB更适合做数据备份以及持久性要求较低的应用场景。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://redis.io/docs/management/persistence/">Redis persistence</a></li>
<li>Redis设计与实现</li>
<li>[详细讲解Linux内核写时复制技术COW机制](</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis数据结构</title>
    <url>/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<h2 id="Redis数据结构"><a href="#Redis数据结构" class="headerlink" title="Redis数据结构"></a>Redis数据结构</h2><p>redis有六种主要的数据类型，下面从调用API以及底层实现原理对于Redis中主要数据类型进行总结和整理，主要内容来自<a href="https://redis.io/docs/data-types/">redis文档</a>和《Redis设计与实现》。</p>
<h3 id="底层数据结构"><a href="#底层数据结构" class="headerlink" title="底层数据结构"></a>底层数据结构</h3><h4 id="字符串-simple-dynamic-string"><a href="#字符串-simple-dynamic-string" class="headerlink" title="字符串(simple dynamic string)"></a>字符串(simple dynamic string)</h4><p>字符串是Redis中最基本的数据类型，键和值的类型均为字符串类型，底层基于支持动态扩容的简单动态字符串（simple dynamic string,SDS）实现，其数据结构示例如下：</p>
<ol>
<li><code>len</code>：buf数组中已经使用的长度，遵循C语言字符串末尾添加空字符<code>\0</code>的习惯</li>
<li><code>free</code>：buf数组中还剩余的长度</li>
</ol>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sdshdr</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> len;</span><br><span class="line">    <span class="keyword">int</span> <span class="built_in">free</span>;</span><br><span class="line">    <span class="keyword">char</span> buf[];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p>SDS的动态扩容机制类似于java中的ArrayList，当剩余空间不足以装下追加字符串时，SDS回进行扩容操作，多余的空间在有需要时，通过对应API可进行释放：</p>
<ol>
<li>若追加后的SDS长度（<code>len</code>）小于1MB，分配和<code>len</code>大小相同的空间</li>
<li>若追加后的SDS长度（<code>len</code>）大于1MB，只分配1MB空间</li>
</ol>
<blockquote>
<p>A <strong>binary-safe</strong> function is one that treats its input as a raw stream of bytes and ignores every textual aspect it may have. The term is mainly used in the <a href="https://en.wikipedia.org/wiki/PHP">PHP</a> programming language to describe expected behaviour when passing binary data into <a href="https://en.wikipedia.org/wiki/Subroutines">functions</a> whose main responsibility is text and <a href="https://en.wikipedia.org/wiki/String_(computer_science">string</a>) manipulating, and is used widely in the official PHP documentation</p>
</blockquote>
<p>区别于C字符串，操作SDS的API将其视为二进制数据进行存储和处理，因此Redis字符串可以保存任意大小不超过512MB的二进制数据。</p>
<h5 id="实现：键值"><a href="#实现：键值" class="headerlink" title="实现：键值"></a>实现：键值</h5><p>除去基本的get、set外，字符串类型还支持获取以及覆盖子串操作：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; get strKey</span><br><span class="line">&quot;newValue&quot;</span><br><span class="line">127.0.0.1:6379&gt; getrange strKey 1 3</span><br><span class="line">&quot;ewV&quot;</span><br><span class="line">127.0.0.1:6379&gt; setrange strKey 4 wwwc</span><br><span class="line">(integer) 8</span><br><span class="line">127.0.0.1:6379&gt; get strKey</span><br><span class="line">&quot;newVwwwc&quot;</span><br></pre></td></tr></table></figure>
<h4 id="链表-linkedlist"><a href="#链表-linkedlist" class="headerlink" title="链表(linkedlist)"></a>链表(linkedlist)</h4><p>Redis实现了双向无环链表，链表的每个节点存储一个字符串类型的值，最大长度 2^32 - 1 (4,294,967,295)，其数据结构实现示例如下：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">listNode</span>&#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">listNode</span> *<span class="title">pre</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">listNode</span> *<span class="title">next</span>;</span></span><br><span class="line">    <span class="keyword">void</span> *value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Reedis列表键、发布订阅监视器等功能都用到了链表实现。</p>
<h5 id="实现：List-列表"><a href="#实现：List-列表" class="headerlink" title="实现：List(列表)"></a>实现：List(列表)</h5><p>与其说是列表，不如说时双端队列，并不支持随机访问，添加删除API包括<code>l/rpush</code>,<code>l/rpop</code>,<code>lrange</code>,<code>ltrim</code>：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; lpush listKey left1 left2 left 3</span><br><span class="line">(integer) 4</span><br><span class="line">127.0.0.1:6379&gt; rpush listKey right1 right2 right3</span><br><span class="line">(integer) 7</span><br><span class="line">127.0.0.1:6379&gt; llen listKey</span><br><span class="line">(integer) 7</span><br><span class="line">127.0.0.1:6379&gt; lrange listKey 1 3</span><br><span class="line">1) &quot;left&quot;</span><br><span class="line">2) &quot;left2&quot;</span><br><span class="line">3) &quot;left1&quot;</span><br><span class="line">127.0.0.1:6379&gt; lpop listKey</span><br><span class="line">&quot;3&quot;</span><br><span class="line">127.0.0.1:6379&gt; rpop listKey</span><br><span class="line">&quot;right3&quot;</span><br></pre></td></tr></table></figure>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20221208084937227.png" class="" title="image-20221208084937227">
<p>支持阻塞删除操作，即等到列表中存在元素能够删除返回元素</p>
<ol>
<li><code>blpop</code></li>
<li><code>brpop</code></li>
</ol>
<h4 id="字典-hashtable"><a href="#字典-hashtable" class="headerlink" title="字典(hashtable)"></a>字典(hashtable)</h4><p>redis字典即哈希表，基于链地址法解决冲突的哈希表实现，当哈希表的负载因子小/大到一定返回，Redis回进行<code>rehash</code>操作，重新开辟合适大小的哈希表，讲原哈希表上的键值对转移到新hash表上，其<code>rehash</code>的时机为：</p>
<ol>
<li>当前服务器没有执行<code>BGSAVE</code>命令或者<code>BGREWRITEAOF</code>命令，且哈希表负载因子大于等于1</li>
<li>正在执行对应命令，且负载因子大于等于5</li>
<li>负载因子小于0.1</li>
</ol>
<p>负载因子$\alpha$（load factor）的定义为:</p>
<ul>
<li>n为哈希表总元素个数，k为哈希表长度</li>
</ul>
<script type="math/tex; mode=display">
load factor(\alpha) = \frac{n}{k}</script><p>由于哈希表中可能存储成千上万个键值对，一次性完成<code>rehash</code>操作成本过高，Redis采用了渐进式的方式实现</p>
<ol>
<li>记录当前rehashindex，指向当前即将rehash的位置，不进行rehashindex时设置为-1</li>
<li>每次对字典执行增删改查操作时，将rehashindex对应位置键值链表转移到新哈希表上</li>
<li>在rehash过程中，一次键值对访问可能需要访问两个哈希表（不在旧，但是在新）</li>
</ol>
<p>rehash开辟新的hash表空间规则如下：</p>
<ol>
<li>若是扩展操作，则新哈希表的大小为大于$n * 2$的最小2的幂次，其中n为原哈希表中元素个数</li>
<li>若是缩小操作，则i性能哈希表大小为大于<code>n</code>的最小2的幂次</li>
</ol>
<h5 id="实现：hashes-哈希表"><a href="#实现：hashes-哈希表" class="headerlink" title="实现：hashes(哈希表)"></a>实现：hashes(哈希表)</h5><p>当哈希键包含的键值比较多，或者键值对中的元素都是比较长的字符串时，hashes底层实现基于字典</p>
<ul>
<li>hashes对于key的数量没有约束，取决于当前是否存在剩余空间</li>
</ul>
<p>常用API有：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; hset hashKey a 1 b 2 c 3</span><br><span class="line">(integer) 3</span><br><span class="line">127.0.0.1:6379&gt; hget hashKey a</span><br><span class="line">&quot;1&quot;</span><br><span class="line">127.0.0.1:6379&gt; hmget hashKey a b c</span><br><span class="line">1) &quot;1&quot;</span><br><span class="line">2) &quot;2&quot;</span><br><span class="line">3) &quot;3&quot;</span><br><span class="line">127.0.0.1:6379&gt; hexists hashKey a</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; hkeys hashKey</span><br><span class="line">1) &quot;a&quot;</span><br><span class="line">2) &quot;b&quot;</span><br><span class="line">3) &quot;c&quot;</span><br><span class="line">127.0.0.1:6379&gt; hvals hashKey</span><br><span class="line">1) &quot;1&quot;</span><br><span class="line">2) &quot;2&quot;</span><br><span class="line">3) &quot;3&quot;</span><br></pre></td></tr></table></figure>
<h4 id="跳跃表-skiplist"><a href="#跳跃表-skiplist" class="headerlink" title="跳跃表(skiplist)"></a>跳跃表(skiplist)</h4><p>跳跃表是Redis有序集合的底层实现之一，数据结构原理不再总结，列几条Redis实现细节</p>
<ol>
<li>Redis中每个跳表节点层高范围为1-32，在节点创建时随机确定</li>
<li>跳跃表中的节点大小按照score后key的方式确定</li>
</ol>
<h5 id="实现：sorted-sets-有序集合"><a href="#实现：sorted-sets-有序集合" class="headerlink" title="实现：sorted sets(有序集合)"></a>实现：sorted sets(有序集合)</h5><p>当有序集合中的元素数量比较多，或者元素成员为较长的字符串时，有序集合就会采用跳表实现（总结就是“大”），支持的操作和集合基本一致：</p>
<ol>
<li>增加元素：<code>zadd</code></li>
<li>集合操作：<code>zinter</code>,<code>zdiffer</code>,<code>zunion</code></li>
<li>优先队列操作：<code>zpopmin</code>，<code>zpopmax</code></li>
</ol>
<h4 id="整数集合-intset"><a href="#整数集合-intset" class="headerlink" title="整数集合(intset)"></a>整数集合(intset)</h4><p>整数集合是Redis Set的底层实现之一，其存储结构示意如下：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">intset</span>&#123;</span></span><br><span class="line">	<span class="keyword">unit32_t</span> encoding;</span><br><span class="line">	uint32_rt length;</span><br><span class="line">	<span class="keyword">int8_t</span> contents[];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同时该数据结构支持集合的类型的自动升级和扩容：</p>
<ul>
<li><code>encoding</code>属性存储当前集合中的数据类型，当插入大于当前数据类型元素时，集合自动升级</li>
<li>自动升级方式为：首先将原有数据升级到对应类型（开辟空间，位置右移），再将新插入的值放在开头或者末尾。</li>
</ul>
<p>书里没讲清楚的几点是：</p>
<ol>
<li>扩容大小是扩容到刚好可以存储所有元素，还是类似于字典的扩容机制？</li>
<li>是否能向集合中插入字符串，由于set可以插入，如果可以插入，对应的升级机制是什么？</li>
</ol>
<h5 id="实现：Set（集合）"><a href="#实现：Set（集合）" class="headerlink" title="实现：Set（集合）"></a>实现：Set（集合）</h5><p>当Set中只包含整数且元素数量较少时，Redis使用整数集合作为Set的底层数据结构</p>
<img src="/2023/01/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Redis/Redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20221208085158085.png" class="" title="image-20221208085158085">
<p>相关操作API包括</p>
<ol>
<li>添加删除元素：<code>SADD</code>，<code>SREM</code></li>
<li>集合运算：<code>SINTER</code>，<code>SDIFF</code>，<code>SUNION</code></li>
<li>集合长度：<code>SCARD</code></li>
</ol>
<h4 id="压缩列表（ziplist）"><a href="#压缩列表（ziplist）" class="headerlink" title="压缩列表（ziplist）"></a>压缩列表（ziplist）</h4><p>压缩列表(ziplist)是列表和哈希表键值对类型的底层实现之一，其类似于一个内存上连续存储的链表：</p>
<ul>
<li>每个元素不仅存储当前元素的数据，还存储前置元素的字节长度，元素与元素之间不存在间隔，通过字节长度确定元素位置。</li>
<li>列表头有三个字段：<code>zlbytes</code>,<code>zltail</code>,<code>zllen</code>分别记录压缩列表字节长度、压缩列表尾节点偏移量、压缩列表节点个数。</li>
</ul>
<p>上述实现机制导致了压缩链表自能从尾向头遍历，首先访问<code>ztail</code>确定尾部元素位置，之后再不断的根据每个元素存储的前置元素长度，确定前置元素位置向前遍历。</p>
<p>记录前置元素长度的字段<code>previous_entry_length</code>的长度会基于前置元素长度改变：</p>
<ol>
<li>当前置元素长度小于254byte时，使用1byte存储。</li>
<li>当前置元素长度大于等于254byte时，使用5byte存储，其中第一个byte存储254作为标识符，方便与情况1区别。</li>
</ol>
<p>由于列表中的元素长度可修改，导致需要修改对应后置元素<code>previous_entry_length</code>长度变化，可能导致<code>连锁更新</code>问题</p>
<h3 id="底层与实现之间的关系"><a href="#底层与实现之间的关系" class="headerlink" title="底层与实现之间的关系"></a>底层与实现之间的关系</h3><p>Redis中共有5种常用数据类型，6种底层实现，两种之间通过抽象对象关联起来，定义结构如下：</p>
<ul>
<li>k-v键值对类型为对象，对象内<code>ptr</code>指针指向具体的底层实现</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">redisObject</span>&#123;</span></span><br><span class="line">    <span class="keyword">unsigned</span> type;</span><br><span class="line">    <span class="keyword">unsigned</span> encoding;</span><br><span class="line">    vord *ptr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过对象这一层实现底层数据结构的抽象隔离，使得Redis能够数据结构以及内存管理更加灵活：</p>
<ol>
<li>同一种kv值类型根据存储需求不同，可以对应不同的底层实现（数据量小使用小类型，数据量大使用正常类型，中间涉及到转换问题）<ul>
<li><code>string</code>类型：<code>int</code>，<code>emdstr</code>，<code>raw</code></li>
<li><code>list</code>类型：<code>ziplist</code>，<code>quicklist</code>(双向链表)</li>
<li><code>set</code>类型：<code>intset</code>，<code>hashtable</code></li>
<li><code>zset</code>类型：<code>ziplist</code>，<code>hashtable</code>+<code>skiplist</code></li>
<li><code>hash</code>类型：<code>ziplist</code>，<code>hashtable</code></li>
</ul>
</li>
<li>基于引用计数（refcount）的内存回收和常量共享机制<ul>
<li>类型对象设置<code>refcount</code>属性，初始化为1，当程序使用/不再使用时对应加一减一，当<code>refcount = 0</code>时，对象占用内存会被释放</li>
<li>基于上述机制，Redis对包含整数值的字符串对象进行共享，即不创建新的，只是对<code>refcount+1</code></li>
</ul>
</li>
</ol>
<h3 id="其他数据类型"><a href="#其他数据类型" class="headerlink" title="其他数据类型"></a>其他数据类型</h3><h4 id="Bitmaps"><a href="#Bitmaps" class="headerlink" title="Bitmaps"></a>Bitmaps</h4><p>string类型的一种拓展，将string当作一个bit数组，执行位操作，类似于动态规划中的状态压缩，通过bit代替原来占用空间更大的状态，实现空间上的压缩，支持的常用API包括</p>
<ol>
<li>基本操作：<code>setbit</code>，<code>getbit</code>,<code>bitcount</code></li>
<li>二进制的逻辑操作：<code>bitop and/or/xor/not</code></li>
</ol>
<p>bitmap存储格式为<code>raw</code>即动态字符串，长度似乎是动态分配的，初始化只有1byte，分配基本单位为byte</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; setbit bitKey 1 1</span><br><span class="line">(integer) 0</span><br><span class="line">127.0.0.1:6379&gt; bitcount bitKey</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; object encoding bitKey</span><br><span class="line">&quot;raw&quot;</span><br><span class="line">127.0.0.1:6379&gt; setbit bitKey 100 1</span><br><span class="line">(integer) 0</span><br><span class="line">127.0.0.1:6379&gt; bitop not newBit bitKey</span><br><span class="line">(integer) 13</span><br><span class="line">127.0.0.1:6379&gt; bitcount newBit</span><br><span class="line">(integer) 102</span><br></pre></td></tr></table></figure>
<h3 id="HyperLogLog"><a href="#HyperLogLog" class="headerlink" title="HyperLogLog"></a>HyperLogLog</h3><p>为了解决涉及到计数场景下的内存占用与元素个数呈线性关系的基数计数问题（<strong>cardinality counting</strong>），提出的一种基于统计的计数结构。</p>
<ul>
<li>例如：统计一个页面在一段时间内不同IP数量，如果使用set，hashtable等类型，如果访问IP数量过多，会导致去重集合占用巨大的空间。</li>
<li>HyperLogLog是一种基于概率的近似算法，首先对计数元素进行hash，选取二进制首位1作为对应标志位置，判断是否重复（更深入理解见<a href="https://www.yuque.com/abser/aboutme/nfx0a4">博客</a>）</li>
</ul>
<p>底层实现数据结构为<code>raw</code>即动态字符串，基本的操作包括:<code>PFADD</code>,<code>PFCOUNT</code>,<code>PFMERGE</code></p>
<h3 id="Geospatial"><a href="#Geospatial" class="headerlink" title="Geospatial"></a>Geospatial</h3><p>以经纬度方式存储和查询数据，类似于一个二维的sortedset，支持的结构包括</p>
<ol>
<li>插入和查询：<code>GEOADD</code>,<code>GEODIST</code>,<code>GEOPOS</code>,<code>GEOHASH</code></li>
<li>范围查询：<code>GEOSEARCH</code>,<code>GEORADIUS</code></li>
</ol>
<p>存储类型为<code>ziplist</code>,使用示例：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; geoadd henan:city 113.62 34.75 zhengzhou 114.78 34.55 kaifeng 112.47 34.15 luoyang 113.18 33.77 pingdingshan</span><br><span class="line">(integer) 4</span><br><span class="line">127.0.0.1:6379&gt; object encoding henan:city</span><br><span class="line">&quot;ziplist&quot;</span><br><span class="line">127.0.0.1:6379&gt; geopos henan:city zhengzhou</span><br><span class="line">1) 1) &quot;113.62000197172164917&quot;</span><br><span class="line">   2) &quot;34.74999926510690784&quot;</span><br><span class="line">127.0.0.1:6379&gt; georadius henan:city 113.62 34.75 100 km</span><br><span class="line">1) &quot;zhengzhou&quot;</span><br><span class="line">127.0.0.1:6379&gt; georadius henan:city 113.62 34.75 1000 km</span><br><span class="line">1) &quot;pingdingshan&quot;</span><br><span class="line">2) &quot;kaifeng&quot;</span><br><span class="line">3) &quot;zhengzhou&quot;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据库</category>
        <category>redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title>InnoDB存储引擎</title>
    <url>/2022/11/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/InnoDB%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/</url>
    <content><![CDATA[<h2 id="InnoDB架构及物理存储"><a href="#InnoDB架构及物理存储" class="headerlink" title="InnoDB架构及物理存储"></a>InnoDB架构及物理存储</h2><blockquote>
<p><code>InnoDB</code> is a general-purpose storage engine that balances high reliability and high performance. </p>
</blockquote>
<p>InnoDB作为MySQL默认引擎，深入了解InnoDB有助于加深对于数据库系统的理解，下面首先从InnoDB的架构出发，介绍InnoDB的组成以及表数据的物理存储方式，其他例如事务、并发控制等相关内容在本文中不展开。</p>
<h3 id="InnoDB架构"><a href="#InnoDB架构" class="headerlink" title="InnoDB架构"></a>InnoDB架构</h3><blockquote>
<p>这部分内容是对<a href="https://dev.mysql.com/doc/refman/8.0/en/">MySQL 8.0 Reference Manual</a>学习自我总结</p>
</blockquote>
<p>InnoDB分为内存结构和磁盘结构两部分，其中内存结构主要为了减少磁盘访问，从而让提升访问性能，磁盘结构部分负责InnoDB中数据的实际物理存储。</p>
<img src="/2022/11/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/InnoDB%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/image-20221108183750949.png" class="" title="image-20221108183750949">
<span id="more"></span>
<h4 id="In-Memory-Structrues"><a href="#In-Memory-Structrues" class="headerlink" title="In-Memory Structrues"></a>In-Memory Structrues</h4><p>内存结构中的主要结构有四个：</p>
<ol>
<li><code>Buffer Pool</code>：缓存历史访问数据和索引，用来减少磁盘访问</li>
<li><code>Change Buffer</code>：缓存对非<code>Buffer pool</code>中数据的修改，当页面加载到<code>Buffer pool</code>中时，会将修改合并到对应页面上</li>
<li><code>Adaptive hash index(AHI)</code>：自适应哈希索引，对于某些访问”频繁“数据的数据，在内存中建立索引到数据页的直接映射<ul>
<li>”频繁“定义为：数据页寻路时间长，或者多个SQL命中相同页面</li>
<li>与<code>Buffer Pool</code>共用空间</li>
</ul>
</li>
<li><code>Log Buffer</code>：缓存<code>redo log</code>数据，默认大小为16MB，周期性刷出到磁盘中</li>
</ol>
<p><code>Buffer Pool</code>采用LRU机制进行缓存数据页的汰换，将内存空间划分为了<code>New Sublist</code>（5/8）和<code>Old Sublist</code>（3/8），从上到下数据页的新旧程度变旧，页面汰换策略如下：</p>
<ul>
<li><code>Midpoint Insertion</code>新页面插入策略，即每次插入新页面时，插入到<code>New Sublist</code>尾部，<code>Old Sublist</code>头部。为了解决一次查询的结果只使用一次，但是涉及到大量数据页导致缓存大量刷出的情况</li>
<li>当访问<code>Buffer pool</code>中的旧页时，会将旧页面移动到<code>New Sublist</code>的头节点</li>
<li>当缓冲区内存够空间不足时，会从<code>Old Sublist</code>尾部踢出页面</li>
</ul>
<img src="/2022/11/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/InnoDB%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/image-20221108185356088.png" class="" title="image-20221108185356088">
<p><code>Change Buffer</code> 是为了减少磁盘访问的另一手段，当修改数据不在内存中时，将修改缓存在该缓存中，当下次需要访问修改涉及页面是，将数据页读入内存后执行merge操作</p>
<ul>
<li>周期性将缓存中的修改写入到对应磁盘中，将大量随机访问转化成了成批次的集中访问，一定程度降低修改的成本。</li>
<li>当修改操作涉及到读取数据页时，不会写入<code>Change buffer</code>，例如<code>delete***id = 1</code>，因为执行修改操作前数据页面一定已经加载到内存中。</li>
<li>系统表空间中存有<code>Change Buffer</code> 的备份</li>
</ul>
<h4 id="On-Disk-Structures"><a href="#On-Disk-Structures" class="headerlink" title="On-Disk Structures"></a>On-Disk Structures</h4><p>磁盘中的结构主要包括各种表空间和其他文件</p>
<ul>
<li>表空间：数据的逻辑存储方式，包括系统表空间、用户表空间、临时表空间等等</li>
<li>其他文件：二次写文件、redo log文件等</li>
</ul>
<p>InnoDB中表的物理存储概念从大到小为：表空间-&gt;段-&gt;区-&gt;页</p>
<ol>
<li>表空间（Table space）：物理存储中的一个表。</li>
<li>段（Segement）：表的逻辑组成部分，一个表一般包括数据段、索引段等，一个段以区为单位进行空间分配。</li>
<li>区（Extent）：64个数据页组成一个区，由于数据页过小，以页进行分配和访问较为离散，成本较高，因此多个页构建区的概念，方便连续存储和访问。</li>
<li>页（Page）：磁盘管理的最小数据单位，一个页包含多个数据项，页大小一般为16KB。</li>
</ol>
<p>根据上述存储逻辑，一个表空间的空间分配逻辑可以如下理解：</p>
<ol>
<li>创建表时首先默认创建两个段，叶子节点段和非叶子节点段</li>
<li>插入数据时，首先在<strong>碎片区</strong>以页为单位分配存储空间，当某个段占用32个碎片区页面后，再以完整的区为单位，分配存储空间</li>
<li>此时的段概念等价于：区+碎片区页</li>
</ol>
<p>二次写文件对应InnoDB的二次写机制（double write）:</p>
<ul>
<li><strong>double write</strong>：将脏页真正写入对应磁盘页之前，首先分两次每次写入1MB到<code>Double Write Buffer</code>也就是二次写文件中，完成该操作后再写入真正的磁盘页中。</li>
<li>主要为了解决页面写入中途崩溃，导致页面受损无法恢复即<strong>页断裂(partial write)</strong>问题</li>
<li>为什么不能用redo log解决？本质上是由于不同层IO的粒度不同，其中DB block (8-16K)&gt; OS block (4K)&gt;= IO block &gt; 磁盘 sector(512byte)，redo log针对的整个页面的状态变更，此时页面的一部分损坏，redo log无法处理该程度的损坏</li>
</ul>
<p>redo log file在后续事务学习中再进行总结</p>
<h3 id="物理存储格式"><a href="#物理存储格式" class="headerlink" title="物理存储格式"></a>物理存储格式</h3><p>上文中表存储表空间、段、区并不是真正的物理存储结构，而是一个逻辑上的表和空间分配的单位，InnoDB实际上的物理存储分为页格式和行格式，其中页中存储行，下面分别进行总结</p>
<h4 id="页格式"><a href="#页格式" class="headerlink" title="页格式"></a>页格式</h4><p>一个InnoDB数据页被划分为7部分：</p>
<ul>
<li>File Header（38 Bytes）：文件头，包括页号、到其他页链接、页面类型等的页面信息</li>
<li>Page Header（56 Bytes）：页头，记录页面内数据信息，例如最新插入位置、事务ID等</li>
<li>Infimum + Supremum Records（26 Bytes）：最小记录和最大记录 两个虚拟的行记录</li>
<li>User Records ：存储数据行</li>
<li>Free Space：空闲空间</li>
<li>Page Directory：页中的”槽”的相对位置，InnoDB对数据行进行均匀划分称为”槽“</li>
<li>File Trailer（8 Bytes）：文件尾，和头各有一个校验和和LSN，验证页面的数据完整性</li>
</ul>
<img src="/2022/11/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/InnoDB%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/image-20221108195120063.png" class="" title="image-20221108195120063">
<p>每个表对应一个表空间，对应<code>data_dir</code>中的一个.ibd文件</p>
<img src="/2022/11/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/InnoDB%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/image-20221108195932481.png" class="" title="image-20221108195932481">
<h4 id="行格式"><a href="#行格式" class="headerlink" title="行格式"></a>行格式</h4><p>表中的每一行对应数据页中<code>User Records</code>区域中的一个数据行，InnoDB中有四种行格式，分别为</p>
<ol>
<li>Redundant：最早的默认行格式</li>
<li>Compact：对Redundant行格式的概念，当前版本的默认行格式</li>
<li>Compressed和Dynamic：InnoDB1.0.x引入的两种行格式，合称为Barracuda，并将前两种格式合称为Antelope<ul>
<li>3两种格式与1，2格式的不同点在于其对于<strong>行溢出</strong>的处理。1，2格式在数据大于768byte时，存储前768byte的数据+指针，其余溢出数据存储到<code>off page</code>中；3两种格式对于行溢出只存储指针不存储数据</li>
<li>Compressed和Dynamic的区别在于前者可以使用zlib库对行数据进行压缩</li>
</ul>
</li>
</ol>
<p>Compact格式如下图所示（两张格式示意图均来自<a href="https://zhuanlan.zhihu.com/p/266060894">知乎博客</a>）</p>
<ol>
<li>变长字段长度列表：按照反向字段顺序放置，当字段长度不超过255byte时，使用1个字节表示；当字段长度大于255byte时，使用2个字节表示</li>
<li>Null值列表：1bite，01表示对应字段为空，不包括定义非空的字段</li>
<li>记录头信息：包括如delete_flag、record_type、next_record等记录信息，同一个页中的记录通过next_record连接称为单向链表</li>
</ol>
<p><img src="https://pic4.zhimg.com/80/v2-439295784a38d93a17f0d7db590644e7_720w.webp" alt="img"></p>
<p>Redundant格式与Compact格式基本相同</p>
<ol>
<li>字段长度偏移列表：记录每个字段的偏移量（反向字段顺序）</li>
<li>没有null值列表，在便宜列表中相邻偏移量差为0，代表null</li>
</ol>
<p><img src="https://pic2.zhimg.com/v2-de40067a916b9c8055f125eb6e012aad_r.jpg" alt="img"></p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-storage-engine.html">MySQL 8.0 Reference Manual The InnoDB Storage Engine</a></li>
<li><a href="https://www.modb.pro/db/113573">博客：MySQL各种“Buffer”之Adaptive Hash Index</a></li>
<li><a href="https://blog.jcole.us/2013/01/07/the-physical-structure-of-innodb-index-pages/">博客：The physical structure of InnoDB index pages</a></li>
<li><a href="https://ioutime.github.io/2021/11/07/InnoDB-Page-Formats/">博客：InnoDB Page Formats</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/266060894">知乎：MySQL之InnoDB存储引擎：Row Format行格式</a></li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>InnoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL索引总结</title>
    <url>/2022/11/06/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/MySQL%E7%B4%A2%E5%BC%95%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h2 id="索引相关知识总结"><a href="#索引相关知识总结" class="headerlink" title="索引相关知识总结"></a>索引相关知识总结</h2><p>索引作为数据库的重要知识，在本科的数据库学习中基本没有接触过，在大四找工作准备面试笔试时才了解到这个常考的概念，本博客是对于以往索引相关知识缺失的补足和系统整理。</p>
<h3 id="索引是什么？"><a href="#索引是什么？" class="headerlink" title="索引是什么？"></a>索引是什么？</h3><blockquote>
<p>A <strong>database index</strong> is a <a href="https://en.wikipedia.org/wiki/Data_structure">data structure</a> that improves the speed of data retrieval operations on a <a href="https://en.wikipedia.org/wiki/Table_(database">database table</a>) at the cost of additional writes and storage space to maintain the index data structure.  -Wikipedia</p>
</blockquote>
<p>索引(index)是用来加速数据库表数据检索的一种特殊数据结构，其作用类似于目录，通过减少检索过程中的磁盘IO，提升检索效率，索引的优点可以概括为以下三点</p>
<ol>
<li>减少服务器需要扫描的数据量</li>
<li>避免因检索导致的排序和临时表</li>
<li>将随机I/O变为顺序I/O</li>
</ol>
<span id="more"></span>
<p>索引缺点总结如下：</p>
<ol>
<li>索引来源于数据表中的数据，创建和维护索引需要消耗时间（由数据量决定）</li>
<li>索引会占用额外的存储空间（InnoDB中每个B+树节点为一个数据页，每个数据页占16KB空间）</li>
</ol>
<p>正因为索引以上的缺点，在设计索引时需要在检索性能、索引大小、索引更新性能几个因素之间进行权衡。</p>
<p>索引按照和数据项的关系可以分为两种：</p>
<ol>
<li><strong>聚簇索引</strong>（Clustered）：表数据物理上按照索引的逻辑大小排列存储，即按照索引从小到大存储，具有相邻索引值的表项存储在相邻的位置。<ul>
<li>由于聚簇索引决定了数据的物理存储方式，<strong>一个表上只能定义一个聚簇索引</strong>。</li>
<li>聚簇索引对于数据的顺序检索性能有较大的提升，例如排序查找、范围查找等</li>
<li><strong>缺点</strong>：由于索引即数据，数据的增删改涉及到数据页的合并和拆分，对性能影响较大；非聚簇索引由于实际存储不需要满足索引定义的顺序，相应的修改影响相对较小</li>
</ul>
</li>
<li><strong>非聚簇索引</strong>（not clustered，二级索引、辅助索引）：区别于聚簇索引，表数据的存储方式和索引的逻辑顺序没有关系，可以以任意顺序存储<ul>
<li>这类索引通常应用于非主键列，用来加速<code>JOIN, WHERE, and ORDER BY</code>等操作</li>
<li>联合索引（composite index）是一类特殊的非聚簇索引，即索引的逻辑顺序由多个属性共同决定。例如在<code>(fisrt_name, second_name)</code>上创建的联合索引，会首先比较 <code>fisrt_name</code> 然后比较<code>second_name</code>确定索引的顺序</li>
<li>InnoDB中非聚簇索引叶子节点存储的主键值，使用非聚簇索引需要<code>回表操作</code>，即首先在当前索引检索到对应主键值，再通过主键上的聚簇索引检索到对应的数据项。与之不同的是，MyISAM中非聚簇索引叶子节点存储的是对应数据项的物理存储地址。</li>
<li>缺点：InnoDB中由于<code>回表</code>操作，检索效率相较于非聚簇索引更差；</li>
</ul>
</li>
</ol>
<p>按照索引的性质可以分为四类：</p>
<ol>
<li><p>普通索引</p>
</li>
<li><p>唯一性索引：unique约束会自动建立唯一性索引</p>
</li>
<li><p>主键索引：主键上的索引，在InnoDB中主键索引等价于聚簇索引，决定了表数据的存储方式，InnoDB对于主键的处理如下：</p>
<ul>
<li><strong>有主键</strong>，聚簇索引即为主键索引</li>
<li><strong>无主键</strong>，InnoDB会使用第一个唯一非空索引作为聚簇索引（unique not null）</li>
<li><strong>全无</strong>，InnoDB 会生成一个名为 GEN_CLUST_INDEX（6个字节的长整数） 的隐藏聚簇索引。MyISAM没有聚簇索引。</li>
</ul>
</li>
<li><p>全文索引：用于全文搜索，只能用在<code>CHAR</code>、<code>VARCHAR</code>、<code>TEXT</code>属性列上</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> articles <span class="keyword">WHERE</span> <span class="keyword">MATCH</span> (属性列) AGAINST (<span class="string">&#x27;查询字符串&#x27;</span>);</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>按照排列顺序可以分为：</p>
<ol>
<li><p>升序索引</p>
</li>
<li><p>降序索引：在MySQL8.0以后开始支持，如果一个查询，需要对多个列进行排序，且顺序要求不一致。在这种场景下，要想避免数据库额外的排序-“filesort”，只能使用降序索引</p>
<ul>
<li><p>与索引方向相反，MySQL使用<code>Backward index scan</code></p>
<blockquote>
<p> <a href="https://dev.mysql.com/blog-archive/mysql-8-0-labs-descending-indexes-in-mysql/">we can see forward index scans are ~15% better than backward index scans</a></p>
</blockquote>
</li>
<li><p>与索引方向不一致，例如 <code>a asc b dsc</code> 对应检索<code>a dsc b dsc</code> ,需要<code>file sort</code></p>
</li>
</ul>
</li>
</ol>
<h3 id="索引的数据结构"><a href="#索引的数据结构" class="headerlink" title="索引的数据结构"></a>索引的数据结构</h3><p>为了加速访问，数据库采用B+Tree作为索引的数据结构，下面分析BTree 和B+Tree的结构和插入删除等操作细节</p>
<h4 id="B-Tree"><a href="#B-Tree" class="headerlink" title="B-Tree"></a>B-Tree</h4><p>B-Tree是一种自平衡树形数据结构，B-Tree允许节点拥有两个以上的孩子节点，可以理解为多叉平衡搜索树，B-Tree的结构性质使其适用于存储读写大量数据的存储系统，其查找/删除/插入的(最坏)时间复杂度均为<code>log(n)</code></p>
<p>m叉B-Tree的定义如下：</p>
<ol>
<li>每个节点最多有 $m$ 个孩子节点，即最多$m-1$个key</li>
<li>每个内部节点（非根和叶子节点）至少有$⌈m/2⌉$个孩子节点，即至少有$⌈m/2⌉ - 1$个key</li>
<li>每个非叶子节点至少有2个孩子节点</li>
<li>一个包含k-1个key的非叶子节点有k个孩子节点</li>
</ol>
<p>B-Tree同样满足搜索树性质，即当前节点key序列中的key大小大于其左边子树元素，小于右边子树元素</p>
<img src="/2022/11/06/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/MySQL%E7%B4%A2%E5%BC%95%E6%80%BB%E7%BB%93/image-20221101201644340.png" class="" title="image-20221101201644340">
<p>B+Tree的插入类似于AVL树，只是在树高平衡操作上有一定的区别：</p>
<ol>
<li>首先找到插入元素位置的叶子节点，插入到叶子节点key序列的对应位置中</li>
<li>若此时叶子节点的key个数达到$m$，则需要进行叶子节点分裂操作<ul>
<li>将叶子节点key序列分为三部分<code>[0...m/2 - 1, m/ 2, m/2 + 1...m]</code>,左右两边差分为两个叶子节点，中间一个key上推到父亲节点</li>
<li>若上推之后父亲节点key个数达到$m$，同样需要分裂操作</li>
<li>不断重复分裂-上推的循环， 直到结束分裂</li>
</ul>
</li>
<li>完成插入</li>
</ol>
<p>B+Tree的删除类似于插入时的反推，需要当前节点从孩子节点借：</p>
<ol>
<li>首先找到插入元素位置，删除对应key，若为非叶子节点需要从下‘借’节点</li>
<li>从删除key位置的左边子树，或者右边子树上推一个最大/最小key，替代对应被删除的key</li>
<li>相当于将父节点的删除转化为了子节点删除，不断重复上述过程，直到删除转移到叶子节点</li>
<li>若叶子节点删除后key为空，则需要进行合并操作，类似于上推的逆操作<ul>
<li>与相邻叶子节点以及对应的父亲节点合并成为新的叶子节点</li>
<li>若父亲节点不满足性质2，重复上述过程</li>
<li>直到向上传递到对应节点满足性质2</li>
</ul>
</li>
</ol>
<p>B-Tree应用到数据库中作为索引加速访问</p>
<ul>
<li>每个key对应一条数据项，每个节点相当于邻近的一系列表项</li>
<li>假设key之间的比较复杂度为为<code>C</code>，相较于普通的二分查找的复杂度<code>log2(N) * C</code>，m叉B-Tree遍历比较key进行查找的复杂度为<code>logm-1(N) * (m-1) * C</code></li>
<li>由于内存查找比较的速度远远快于从磁盘中读取数据的数据，两者的查找成本区别在于访问磁盘次数，m叉B-Tree的<code>logm-1(N)</code>次远小于二分查找的<code>log2(N)</code>次</li>
</ul>
<img src="/2022/11/06/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/MySQL%E7%B4%A2%E5%BC%95%E6%80%BB%E7%BB%93/image-20221101205630650.png" class="" title="image-20221101205630650">
<p><strong>B+Tree</strong>是B-Tree的变种，相当于只在叶子节点存储数据的B-Tree，两者的主要区别有</p>
<ol>
<li>B+Tree只在叶子节点存储数据，叶子节点按顺序连接成为双向链表</li>
<li>m阶B+Tree，每个节点最多有 $m$ 个孩子节点，即最多$m$个key</li>
<li>每个key对应一个孩子节点，孩子节点中的key范围为：<code>[左边key+1,key]</code></li>
</ol>
<h4 id="几个概念性问题"><a href="#几个概念性问题" class="headerlink" title="几个概念性问题"></a>几个概念性问题</h4><ol>
<li>为什么不使用查询插入速度为O(1)的hash索引?<ul>
<li>hash索引支支持等值比较查询，不支持任何的范围查询，例如<code>where age &gt; 10</code></li>
<li>若数据分布不均匀，或选取的hash函数不合适，出现大量哈希冲突影响检索和索引维护的成本</li>
<li>InnoDB不支持hash索引，但是支持自适应哈希索引（类似于缓存），Memory引擎显式支持哈希索引</li>
</ul>
</li>
<li><p>为什么不使用B-Tree而使用B+Tree？</p>
<ul>
<li>B-Tree由于非叶子节点同样存储数据，查询可能在叶子节点或者非叶子节点命中，导致查询性能波动较大，无法预测（Mysql8.0删除缓存的原因）</li>
<li>B-Tree的非叶子节点存储数据占用更多的空间，B+Tree在相同节点大小下，能够存储更多key，也就意味着B+Tree的深度更小，查询成本更低</li>
<li>B+Tree叶子节点以双向链表形式组织，对于范围查询支持更好</li>
</ul>
<h3 id="索引使用"><a href="#索引使用" class="headerlink" title="索引使用"></a>索引使用</h3></li>
</ol>
<p>索引的创建分为两种方式</p>
<ol>
<li><p>在创建表时声明约束(主键、唯一、外键)会创建对应的索引</p>
</li>
<li><p>主动通过<code>CREATE</code>语句在现有的表上添加索引</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">UNIQUE</span><span class="operator">|</span>FULLTEXT<span class="operator">|</span>SPATIAL] INDEX indexName <span class="keyword">ON</span> mytable(<span class="keyword">column</span>(length)); </span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>索引操作示例：</p>
<ol>
<li><p>创建两个表，查看对应约束</p>
<ul>
<li><p>创建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> department(</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> dept_id <span class="type">int</span> <span class="keyword">PRIMARY</span> KEY <span class="keyword">NOT</span> <span class="keyword">NULL</span> AUTO_INCREMENT,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> dept_name <span class="type">varchar</span>(<span class="number">100</span>) <span class="keyword">default</span> &quot;待定&quot;</span><br><span class="line">    );</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.06</span> sec)</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> empolyee(</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> emp_id <span class="type">int</span> <span class="keyword">PRIMARY</span> KEY,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> dept_id <span class="type">int</span>, </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> phone_num <span class="type">int</span> <span class="keyword">UNIQUE</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">CONSTRAINT</span> <span class="keyword">FOREIGN</span> KEY(dept_id) <span class="keyword">REFERENCES</span> department(dept_id)</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> );</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.02</span> sec)</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看约束：<code>SHOW INDEX FROM 表名</code></p>
<img src="/2022/11/06/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/MySQL%E7%B4%A2%E5%BC%95%E6%80%BB%E7%BB%93/image-20221104165652781.png" class="" title="image-20221104165652781">
</li>
</ul>
</li>
<li><p>手动创建索引</p>
<ul>
<li>同样可使用<code>ALTER ADD</code></li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> index test_index <span class="keyword">on</span> employee(phone_num);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除索引</p>
<ul>
<li>同样可使用<code>DROP INDEX index_name ON table_name</code></li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> alter table employee drop index test_index;</span></span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>当检索涉及到指定资源的顺序(order by)、比较(where on等)时，可以考虑使用索引。</p>
<h3 id="索引失效场景"><a href="#索引失效场景" class="headerlink" title="索引失效场景"></a>索引失效场景</h3><p>并不是在属性列上设置索引后，所有用到对应属性列的查询都能够用到索引，个人总结索引生效的条件为：<strong>检索条件必须为常量</strong>，即能够让数据库根据某个值在B+树上逐层深入，而是需要遍历所有数据项确定是否满足<code>where</code>条件。</p>
<p>常见的索引失效场景有：</p>
<ol>
<li><p>查询条件中存在计算函数、类型转换等</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">left</span>(name, <span class="number">4</span>) <span class="operator">=</span> <span class="string">&#x27;wang&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>不等于(<code>&lt;&gt;</code> ，<code>!=</code>，<code>not like</code>，<code>not null</code>)等条件</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> name <span class="operator">!=</span> <span class="string">&#x27;zhangsan&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>or</code>前后存在非索引列</p>
<ul>
<li>因为非索引列需要遍历所有数据项</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> name <span class="operator">=</span> <span class="string">&#x27;zhangsan&#x27;</span> <span class="keyword">or</span> age <span class="operator">=</span> <span class="number">100</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>联合索引范围条件右边索引失效</p>
<ul>
<li>联合索引<code>index(name, age, home)</code></li>
<li><code>age</code>范围条件，导致该联合索引只能用到前两个属性，相当于<code>index(name, age)</code></li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> name <span class="operator">=</span> <span class="string">&#x27;100&#x27;</span> <span class="keyword">and</span> age <span class="operator">&gt;</span> <span class="number">100</span> <span class="keyword">and</span> home <span class="keyword">like</span> &quot;cn%&quot;</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang并发控制总结</title>
    <url>/2022/09/06/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/Golang%E7%BC%96%E7%A8%8B/Golang%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h2 id="Golang并发控制总结"><a href="#Golang并发控制总结" class="headerlink" title="Golang并发控制总结"></a>Golang并发控制总结</h2><blockquote>
<p>写在开头：最近用go做MIT6.824课程作业时，涉及到大量基于golang的并发控制，但是由于不熟悉golang语言以及相关并发控制手段，导致出现了大量的bug，影响了实现进程，因此产生了总结学习golang并发控制的想法</p>
</blockquote>
<p>目前大厂的后端开发大量的从java转向go，很大一部分原因是由于go所具备的高并发、高性能、容易开发等性质，可以说go并发控制是学习go内容中最为重要的一部分（<del>java并发我都没学会，直接学go，看出我的诚意了吧</del>），下面的总结学习主要基于golang官网的Effective Go的<a href="https://go.dev/doc/effective_go#concurrency">cocurrency章节</a>。</p>
<span id="more"></span>
<h3 id="何为并发控制？"><a href="#何为并发控制？" class="headerlink" title="何为并发控制？"></a>何为并发控制？</h3><blockquote>
<p>In <a href="https://en.wikipedia.org/wiki/Information_technology">information technology</a> and <a href="https://en.wikipedia.org/wiki/Computer_science">computer science</a>, especially in the fields of <a href="https://en.wikipedia.org/wiki/Computer_programming">computer programming</a>, <a href="https://en.wikipedia.org/wiki/Operating_systems">operating systems</a>, <a href="https://en.wikipedia.org/wiki/Multiprocessor">multiprocessors</a>, and <a href="https://en.wikipedia.org/wiki/Database">databases</a>, <strong>concurrency control</strong> ensures that correct results for <a href="https://en.wikipedia.org/wiki/Concurrent_computing">concurrent</a> operations are generated, while getting those results as quickly as possible. -wikipedia</p>
</blockquote>
<p>并发控制本质上是通过一定的手段保证多个并发进行操作最终产生正确的结果，并且尽可能的保证性能，所以学习并发控制主要从两个角度入手：</p>
<ul>
<li><p>如何实现并发？</p>
</li>
<li><p>有哪些控制并发的手段？</p>
</li>
</ul>
<p>编程语言层面的并发控制可以理解为通过一定手段保证多个操作共享变量的线程正确执行，并且保证性能，这里的两个角度为</p>
<ul>
<li>多线程实现并发：go中的Goroutines</li>
<li>多种控制并发的手段：mutex，channel，waitGroup等</li>
</ul>
<h3 id="golang并发控制"><a href="#golang并发控制" class="headerlink" title="golang并发控制"></a>golang并发控制</h3><p>golang并发控制基于：”Do not communicate by sharing memory; instead, share memory by communicating.” 的思想，即并发线程之间不通过共享内存进行通信，而是通过通信实现共享内存，对此我的理解是：</p>
<ul>
<li>传统的并发控制手段是共享资源+锁的形式，实现互斥访问</li>
<li>golang舍弃了上述思想，采用通信的方式，将共享资源的访问变为序列化处理的通信传递</li>
</ul>
<p>引用 <a href="https://go.dev/talks/2012/waza.slide#19">演讲：Concurrency is not parallelism</a> ppt中的例子(两个取书烧书线程，分别从书籍堆中取书运输到火堆烧毁)，第一种思路如下图：</p>
<ul>
<li><p>属于互斥资源，上锁两个线程互斥取书、</p>
</li>
<li><p>“火堆”属于互斥资源，双锁两个线程互斥烧书</p>
</li>
<li><p>共享资源（“书堆“）+锁实现并发控制</p>
<img src="/2022/09/06/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/Golang%E7%BC%96%E7%A8%8B/Golang%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%E6%80%BB%E7%BB%93/image-20220815104551093.png" class="" title="image-20220815104551093">
</li>
</ul>
<p>第二种并发控制思路如下图：</p>
<ul>
<li>三个线程：取书线程、运书线程、烧书线程</li>
<li>三个线程之间通过通信，实现并发控制</li>
</ul>
<img src="/2022/09/06/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/Golang%E7%BC%96%E7%A8%8B/Golang%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%E6%80%BB%E7%BB%93/image-20220815104637154.png" class="" title="image-20220815104637154">
<p>上述两种思路的区别可以总结为：</p>
<ol>
<li>传统思路属于并行的思路，一个活多个人干，人多效率就高，但问题是互斥问题会导致性能下降（纵向）</li>
<li>golang思路属于将一个任务拆解，分成不同阶段，多个人各司其职，避免了互斥资源访问的性能下降,问题是等待通信的延迟（横向）</li>
</ol>
<p>整合上述两种思想的细粒度并发+并行（双cpu）：</p>
<img src="/2022/09/06/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/Golang%E7%BC%96%E7%A8%8B/Golang%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%E6%80%BB%E7%BB%93/image-20220815105310603.png" class="" title="image-20220815105310603">
<h4 id="Goroutines"><a href="#Goroutines" class="headerlink" title="Goroutines"></a>Goroutines</h4><p>Go中类似于”线程“的概念并发调度单位定义为Goroutine：与其它 Go 协程并发运行在同一地址空间的函数。Goroutine于线程的区别点在于：</p>
<ol>
<li>Goroutine相当于”轻量级“线程，启动时只占用少量的栈空间（java线程在启动时会分配固定大小的占空间）</li>
<li>Goroutine与内核线程的数量对应关系是一对一或者多对一，Goroutine的调度由Go管理，避免了内核线程调度上下文切换的成本</li>
<li>Goroutine更像一个独立运行的函数、操作等，线程更像是一个单独运行的轻量级”进程“</li>
</ol>
<p>在调用前添加关键词 <code>go</code>即可启动一个Goroutine：</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="keyword">go</span> list.Sort() </span><br><span class="line"><span class="comment">//匿名函数</span></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    time.Sleep(delay)\</span><br><span class="line">    fmt.Println(message)</span><br><span class="line">&#125;()</span><br></pre></td></tr></table></figure>
<p>使用goroutine经常出现的一个错误如下：</p>
<ol>
<li>由于i为共享变量，Goroutine执行之前主程序可能已经进入下一轮循环，导致输出错误</li>
<li>解决方案为：传参 或者 赋值局部变量</li>
</ol>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">//错误代码</span></span><br><span class="line"><span class="keyword">for</span> i:=<span class="number">1</span>;i &lt; <span class="number">10</span>;i++ &#123;</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        fmt.Println(i)</span><br><span class="line">    &#125;()</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//正确代码</span></span><br><span class="line"><span class="keyword">for</span> i:=<span class="number">1</span>;i &lt; <span class="number">10</span>;i++ &#123;</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(i)</span></span> &#123;</span><br><span class="line">        fmt.Println(number)</span><br><span class="line">    &#125;(number <span class="keyword">int</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> i:=<span class="number">1</span>;i &lt; <span class="number">10</span>;i++ &#123;</span><br><span class="line">    number := i</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        fmt.Println(number)</span><br><span class="line">    &#125;()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h4><p>Channel类似于Unix中的管道概念，提供不同Goroutine之间的通信，通过make函数初始化，其中Channel在初始化时执行</p>
<ul>
<li>在访问无缓冲管道和缓冲写入已满的管道时，访问Goroutine会阻塞</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line">ci := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)            <span class="comment">// unbuffered channel of integers</span></span><br><span class="line">cj := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>, <span class="number">0</span>)         <span class="comment">// unbuffered channel of integers</span></span><br><span class="line">cs := <span class="built_in">make</span>(<span class="keyword">chan</span> *os.File, <span class="number">100</span>)  <span class="comment">// buffered channel of pointers to Files</span></span><br><span class="line"><span class="comment">//写入管道</span></span><br><span class="line">ci &lt;- <span class="number">1</span></span><br><span class="line"><span class="comment">//读取管道</span></span><br><span class="line">i &lt;- ci</span><br></pre></td></tr></table></figure>
<p>通过Channel实现的最简单的同步例子如下：</p>
<ul>
<li>主线程调用排序Goroutine后等待读取管道</li>
<li>排序Goroutine排序完成后，写入管道</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">c := make(chan <span class="keyword">int</span>)  <span class="comment">// Allocate a channel.</span></span><br><span class="line"><span class="comment">// Start the sort in a goroutine; when it completes, signal on the channel.</span></span><br><span class="line"><span class="function">go <span class="title">func</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    list.Sort()</span><br><span class="line">    c &lt;- <span class="number">1</span>  <span class="comment">// Send a signal; value does not matter.</span></span><br><span class="line">&#125;()</span><br><span class="line">doSomethingForAWhile()</span><br><span class="line">&lt;-c   <span class="comment">// Wait for sort to finish; discard sent value.</span></span><br></pre></td></tr></table></figure>
<p>结合select可实现异步阻塞通信同步功能：</p>
<ul>
<li>监听case管道，读取到内容，则执行相关操作</li>
<li>无default时，阻塞直到读到内容；有default，执行default，继续向下执行</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">fibonacci</span><span class="params">(c, quit <span class="keyword">chan</span> <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">	x, y := <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		<span class="keyword">select</span> &#123;</span><br><span class="line">		<span class="keyword">case</span> c &lt;- x:</span><br><span class="line">			x, y = y, x+y</span><br><span class="line">		<span class="keyword">case</span> &lt;-quit:</span><br><span class="line">			fmt.Println(<span class="string">&quot;quit&quot;</span>)</span><br><span class="line">			<span class="keyword">return</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line">	quit := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">10</span>; i++ &#123;</span><br><span class="line">			fmt.Println(&lt;-c)</span><br><span class="line">		&#125;</span><br><span class="line">		quit &lt;- <span class="number">0</span></span><br><span class="line">	&#125;()</span><br><span class="line">	fibonacci(c, quit)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>发送者可以主动调用<code>close()</code>关闭管道，接收端for循环读取会终止：</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">fibonacci</span><span class="params">(n <span class="keyword">int</span>, c <span class="keyword">chan</span> <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">	x, y := <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; n; i++ &#123;</span><br><span class="line">		c &lt;- x</span><br><span class="line">		x, y = y, x+y</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">close</span>(c)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>, <span class="number">10</span>)</span><br><span class="line">	<span class="keyword">go</span> fibonacci(<span class="built_in">cap</span>(c), c)</span><br><span class="line">	<span class="keyword">for</span> i := <span class="keyword">range</span> c &#123;</span><br><span class="line">		fmt.Println(i)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="sync"><a href="#sync" class="headerlink" title="sync"></a>sync</h4><p>sync包提供了包含锁在内的一系列并发控制手段，包括sync.Mutex、sync.RWMutex、sync.Cond、sync.WaitGroup等</p>
<h5 id="sync-Mutex"><a href="#sync-Mutex" class="headerlink" title="sync.Mutex"></a>sync.Mutex</h5><p>golang中的锁，提供上锁、解锁等操作方法</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *Mutex)</span> <span class="title">Lock</span><span class="params">()</span> 			//上锁</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *Mutex)</span> <span class="title">TryLock</span><span class="params">()</span> <span class="title">bool</span>  //尝试上锁</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *Mutex)</span> <span class="title">Unlock</span><span class="params">()</span>		//解锁</span></span><br></pre></td></tr></table></figure>
<h5 id="sync-RWMutex"><a href="#sync-RWMutex" class="headerlink" title="sync.RWMutex"></a>sync.RWMutex</h5><p>读写互斥锁，在普通锁接口上提供了“读”锁上锁解锁操作</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rw *RWMutex)</span> <span class="title">RLock</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rw *RWMutex)</span> <span class="title">RUnlock</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<h5 id="sync-Once"><a href="#sync-Once" class="headerlink" title="sync.Once"></a>sync.Once</h5><p>确保函数只执行一次的接口（例如初始化），调用对应Do方法，传入调用的函数</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> once Once					<span class="comment">//初始化</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(o *Once)</span> <span class="title">Do</span><span class="params">(f <span class="keyword">func</span>()</span>)		//执行<span class="title">f</span>函数（保证只执行一次）</span></span><br></pre></td></tr></table></figure>
<h5 id="sync-WaitGroup"><a href="#sync-WaitGroup" class="headerlink" title="sync.WaitGroup"></a>sync.WaitGroup</h5><p>该方法实现等待一系列的Goroutines的退出，基本接口如下:</p>
<ul>
<li>通过add()方法增加WaitGroup counter数量，done()方法表明Goroutine之一完成，减少WaitGroup counter数量</li>
<li>wait()方法等待 WaitGroup counter为0</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(wg *WaitGroup)</span> <span class="title">Add</span><span class="params">(delta <span class="keyword">int</span>)</span></span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(wg *WaitGroup)</span> <span class="title">Done</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(wg *WaitGroup)</span> <span class="title">Wait</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<p>具体的使用例子：</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line">	<span class="keyword">var</span> urls = []<span class="keyword">string</span>&#123;</span><br><span class="line">		<span class="string">&quot;http://www.golang.org/&quot;</span>,</span><br><span class="line">		<span class="string">&quot;http://www.google.com/&quot;</span>,</span><br><span class="line">		<span class="string">&quot;http://www.example.com/&quot;</span>,</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">for</span> _, url := <span class="keyword">range</span> urls &#123;</span><br><span class="line">		<span class="comment">// Increment the WaitGroup counter.</span></span><br><span class="line">		wg.Add(<span class="number">1</span>)</span><br><span class="line">		<span class="comment">// Launch a goroutine to fetch the URL.</span></span><br><span class="line">		<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(url <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">			<span class="comment">// Decrement the counter when the goroutine completes.</span></span><br><span class="line">			<span class="keyword">defer</span> wg.Done()</span><br><span class="line">			<span class="comment">// Fetch the URL.</span></span><br><span class="line">			http.Get(url)</span><br><span class="line">		&#125;(url)</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// Wait for all HTTP fetches to complete.</span></span><br><span class="line">	wg.Wait()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="sync-Cond"><a href="#sync-Cond" class="headerlink" title="sync.Cond"></a>sync.Cond</h5><p>条件变量实现（golang建议能够通过Channel实现同步，尽量避免使用条件变量），需要与Locker配合使用：</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewCond</span><span class="params">(l Locker)</span> *<span class="title">Cond</span>	//返回一个使用锁<span class="title">l</span>的条件变量</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Cond)</span> <span class="title">Broadcast</span><span class="params">()</span>		//唤醒等待条件变量的所有<span class="title">Goroutine</span></span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Cond)</span> <span class="title">Signal</span><span class="params">()</span>			//唤醒一个等待条件变量的<span class="title">Goroutine</span></span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Cond)</span> <span class="title">Wait</span><span class="params">()</span>			//等待条件变量</span></span><br></pre></td></tr></table></figure>
<p>基本的使用方法为：</p>
<ul>
<li>在修改条件时需要上锁，调用Broadcast()或Signal()不必须上锁</li>
<li>调用wait()时首先上锁，wait()方法会将当前Goroutine添加到唤醒列表，释放锁阻塞当前Goroutine等待条件变量（Broadcast或Signal），上锁退出wait()方法</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Cond)</span> <span class="title">Wait</span><span class="params">()</span></span> &#123;</span><br><span class="line">	c.checker.check()</span><br><span class="line">	t := runtime_notifyListAdd(&amp;c.notify)</span><br><span class="line">	c.L.Unlock() 							<span class="comment">//上面上锁的原因是并发添加等待Goroutine需要互斥执行</span></span><br><span class="line">	runtime_notifyListWait(&amp;c.notify, t)	<span class="comment">//释放锁后，等待条件变量</span></span><br><span class="line">	c.L.Lock() 								<span class="comment">//重新上锁，目的是与修改条件操作互斥</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第一次学的时候没搞明白为什么要有锁，深入思考下发现，本质是为了<strong>保证条件变更和根据条件进行操作的Goroutine的互斥</strong>，避免操作时条件发生变更，不满足操作条件，此方法带来两个问题：</p>
<ol>
<li>等待线程被唤醒后，条件可能发生该改变（因为等待时释放了锁），所以需要循环判断</li>
<li>同时只有一个等待Goroutine能在Wait()唤醒后进行执行</li>
</ol>
<p>所以wait()方法的使用方式为：</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line">c.L.Lock()</span><br><span class="line"><span class="keyword">for</span> !condition() &#123;</span><br><span class="line">    c.Wait()</span><br><span class="line">&#125;</span><br><span class="line">... <span class="built_in">make</span> use of condition ...</span><br><span class="line">c.L.Unlock()</span><br></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>golang并发控制学起来并不复杂，主要是理解其并发控制思路+学习常用的并发控制接口，结合官方文档以及官网提供的一些学习资料学下来并不难（<del>别看csdn</del>）</p>
]]></content>
      <categories>
        <category>编程相关</category>
        <category>Golang</category>
      </categories>
  </entry>
  <entry>
    <title>AmazonAurora云数据库</title>
    <url>/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/AmazonAurora%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
    <content><![CDATA[<h2 id="Amazon-Aurora-云端分布式数据库"><a href="#Amazon-Aurora-云端分布式数据库" class="headerlink" title="Amazon Aurora 云端分布式数据库"></a>Amazon Aurora 云端分布式数据库</h2><blockquote>
<p>论文: <a href="https://dl.acm.org/doi/epdf/10.1145/3035918.3056101">Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Database</a></p>
</blockquote>
<p>Amazon Aurora是亚马逊从2014年开始提供的一种云上关系型数据库架构，基于mysql的基础上改进而来，在实现传统关系型数据库特性的基础上，实现了事务吞吐量、错误恢复等性能巨大提升。</p>
<h3 id="Amazon-Aurora到来之前"><a href="#Amazon-Aurora到来之前" class="headerlink" title="Amazon Aurora到来之前"></a>Amazon Aurora到来之前</h3><p>要理解Amazon Aurora的设计原理，首先要了解一般数据库的事务执行流程；传统单机事务型数据库数据一般以B-Tree形式组织存储在硬盘上，并在内存中存储数据的缓存以加速访问/修改过程。以写事务流程为例，<strong>一般的事务执行过程</strong>如下：</p>
<ol>
<li>首先锁定要想修改的数据，防止其他事务修改</li>
<li>在WAL（Write-ahead-log）中写入当前事务日志项</li>
<li>根据操作修改缓存中的数据项<ul>
<li>修改前镜像+修改日志+修改后进行</li>
</ul>
</li>
<li>提交事务，等待特定的时机(缓存区满等)，再将缓存持久化到磁盘中</li>
</ol>
<span id="more"></span>
<p>之所以采用WAL的形式，是因为写入磁盘的成本过高，通过追加形式写入WAL降低了提交事务的成本，再合适的时机再将修改持久化到磁盘中。</p>
<ul>
<li>另外通过多次修改合并，再一次将缓存持久化到磁盘，降低了磁盘读取写入的速度</li>
<li>MySQL通过redo、undo log实现了WAL的机制，最终实现了事务的原子性和持久化</li>
</ul>
<h4 id="为什么会有Aurora"><a href="#为什么会有Aurora" class="headerlink" title="为什么会有Aurora"></a>为什么会有Aurora</h4><p>论文中首先分析了在使用Aurora之前，Amazon云服务提供的基于MySQL的云上关系数据库服务存在的问题，基本架构如下图所示：</p>
<ul>
<li>每个数据库实例采用主从备份形式，分为主实例和从实例。负责存储数据的EBS（Amazon弹性块存储单元，我就理解为一个逻辑上的存储服务器），都带有一个镜像EBS，备份主EBS上的数据。</li>
<li>每当数据修改，首先在主服务上进行提交即执行1操作后，再被备份到从服务上即执行3操作</li>
</ul>
<img src="/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/AmazonAurora%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93/image-20220710151104545.png" class="" title="image-20220710151104545">
<p>上述架构存在的问题，导致了Aurora的出现</p>
<ol>
<li><strong>写放大问题</strong>。写入涉及到大量数据，包括redo log、binaray log、修改的数据页、临时的double-write以及FRM元数据等，另外由于操作1-&gt;3-&gt;5必须顺序执行，增加了写入的延时。</li>
<li><strong>备份需要通过网络传输修改涉及到的数据，数据传输量过大</strong></li>
</ol>
<h3 id="相关概念和名词定义"><a href="#相关概念和名词定义" class="headerlink" title="相关概念和名词定义"></a>相关概念和名词定义</h3><p><strong>Availability Zone（AZ）</strong></p>
<p>服务器可用区。定义为一系列具有“区域”临近关系的服务器节点集合（可以理解为一个数据中心内的服务器节点）</p>
<ul>
<li>一个AZ内的节点被认为是既有错误相关性的，存在AZ内所有服务器因为某种原因宕机的情况（例如：网络中断、洪水、网络升级、软件部署等）</li>
<li>不同AZ之间通过低延迟网络连接，对于上述类型错误具有隔离性（例如:东部的服务器中心停电了，西部的没问题正常工作）</li>
<li>如磁盘错误、服务器过热宕机等等这里不具有相关性的导致服务器宕机的错误，在不同AZ内部普遍存在</li>
</ul>
<p><strong>Data Segment</strong></p>
<p>将数据库中的数据划分为固定大小（10G）的数据段，数据段是错误和恢复的最小单元</p>
<p><strong>Protection Groups (PGs)</strong></p>
<p>每个Data Segment在Amazon Aurora中共存储6份，这6份数据组成当前数据段的Protection Groups，存储在三个AZ上</p>
<p><strong>storage volume</strong></p>
<p>存储卷是一系列Protection Groups的组合，存储在大量的存储节点上，对外接口表现为一个使用带有存储的EC2的虚拟主机（我理解的对外看起来就是一个磁盘“卷”，屏蔽了底层多个节点分布式存储的细节），通过不断地增加PG，可以增加存储卷的大小（目前支持到64 TB）</p>
<h3 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h3><p>如果要在经常出错的分布式环境中构建一套可靠的关系型数据库服务，持久性和数据一致性是我们首先要满足的基本需求。另一方面之舍弃单机/主从架构转向分布式，也是从性能的角度出发，期望兼具分布式系统的高性能和高可用特征。针对上述问题，Amazon Aurora提出了主要三个方面的设计思想</p>
<ol>
<li>Offloading Redo Processing to Storage：只传输log，不传输数据本身，存储节点在接收到log后，执行操作实现数据变更，从而降低了网络IO负担</li>
<li>Replication and Correlated Failures：使用多副本+quorum机制，保证持久性+多副本下的数据一致性</li>
<li>Segmented Storage：通过对数据划分，实现快速错误恢复</li>
</ol>
<img src="/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/AmazonAurora%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93/image-20220710155140778.png" class="" title="image-20220710155140778">
<h4 id="传输日志"><a href="#传输日志" class="headerlink" title="传输日志"></a>传输日志</h4><p>Aurora中不同副本存储节点之间数据同步并不直接传输数据，只通过网络传输redo log，由存储节点在接收到日志后，在内存中按照日志操作进行变更，基本流程如下：</p>
<ol>
<li>主实例收到写操作，将redo持久化到本地，向6个副本节点发送redo log</li>
<li>存储节点接收到redo log持久化到本地，在内存中按照日志操作进行变更</li>
<li>主实例接收到多数派应答后，认为日志被持久化</li>
<li>通过链式复制（chain replication），两外两个AZ中的从实例进行日志备份同步</li>
</ol>
<p>存储节点内存的持久化（写入磁盘），会在特定的时机（缓存满等）进行，同样会将多个修改合并为一次磁盘写入。</p>
<ul>
<li>Aurora设计原则上保证后台处理与前台处理负相关（优先满足前台请求，与传统数据库不同），写入磁盘实际上就是后台处理</li>
</ul>
<p>通过传输日志，相较于上文中的mysql主从复制架构中传输数据，实现了性能35倍的提升（事务处理速度）</p>
<blockquote>
<p>The results of our experiment are summarized in Table 1. Over the 30-minute period, Aurora was able to sustain 35 times more transactions than mirrored MySQL.</p>
</blockquote>
<h4 id="多副本-quorum"><a href="#多副本-quorum" class="headerlink" title="多副本+quorum"></a>多副本+quorum</h4><p>如上图所示，Aurora中每个数据段存储6个副本，每两个副本存储在一个AZ中，多副本在读取和的写入时就涉及到了共识问题</p>
<ul>
<li>确保写入的数据在下次读取中能够读到；假设写入需要W个节点的确认，读取需要R个节点的确认，当 W + R &gt; N(副本节点数量)时，保证读取一定能够读到之前的写入。</li>
<li>Aurora中有6个副本，设定W=4、R=3，即W + R = 7 &gt; 6</li>
</ul>
<p>通过以上设置，Aurora实现了读和写不同程度的容错</p>
<ol>
<li><strong>写操作</strong>：最低可保证在一个AZ失效，或者两个不同AZ节点失效时可正常写入</li>
<li><strong>读操作</strong>：保证AZ+1，即一个AZ失效+一个节点失效的情况下可以读到所有写入的数据</li>
</ol>
<h4 id="数据分块"><a href="#数据分块" class="headerlink" title="数据分块"></a>数据分块</h4><p>数据分块主要从错误恢复的角度出发，其中论文定义列两个概念</p>
<ol>
<li>MTTR（Mean Time to Repair）平均错误修复时间。当一部分数据失效时，我们需要花费一定时间恢复数据，使得副本恢复到失效前状态</li>
<li>MTTF（Mean Time to Failure ）平均错误时间。即系统错误出现的时间间隔</li>
</ol>
<p>论文中认为MTTF时难以改变的，只有通过减少MTTR，使得在错误修复过程中尽量的避免另外的错误发生</p>
<ul>
<li>数据块一旦失效，需要从其他副本传输获得失效的数据，此时数据大小和网络IO是决定MTTR的主要因素</li>
<li>因此Aurora通过将数据分块，降低数据块的大小，从而降低MTTR</li>
</ul>
<p>论文中列举的数据是10G的数据块通过10Gbps的网络需要花费十秒完成错误恢复</p>
<blockquote>
<p>A 10GB segment can be repaired in 10 seconds on a 10Gbps network link.</p>
</blockquote>
<h3 id="如何保证日志持久性和一致性"><a href="#如何保证日志持久性和一致性" class="headerlink" title="如何保证日志持久性和一致性"></a>如何保证日志持久性和一致性</h3><p>按照上文中描述qurorum模型，由于不同副本节点log接收的情况不同，可能存在不同存储节点缺失不同数量的log情况，当单个存储节点失效或者数据库系统失效时(即主实例)，如何保证日志持久性和一致性，也就相当于保证了数据库的持久性和一致性，不难想到肯定是<strong>给log进行编号</strong>。</p>
<ul>
<li>数据库系统主实例会为每个事务的log添加一个原子递增的编号：<strong>Log Sequence Number (LSN)</strong></li>
<li>主实例维护一个特殊的编号 <strong>VDL or the Volume Durable LSN</strong>，我理解时当前数据库提交的（已经认为持久化的）最大的LSN</li>
<li>不同副本节点会定时与同一个PG中的其他节点进行通信，根据LSN同步自己缺失的log</li>
</ul>
<p>当系统<strong>重启</strong>时，主实例会与存储节点进行通信（qurorum 读），确定当前PG的持久化点（VCL），然后发送命令通知所有的存储节点，截断LSN大于VCL的日志。</p>
<p>当<strong>单个节点失效重新上线</strong>时，需要进行状态同步，即将未执行的log在数据副本的基础上进行重放</p>
<ul>
<li>Aurora则将重放的过程放到了数据存储节点，完全后台化操作。即使故障发生时正在处理100K TPS，也能在10秒内恢复</li>
</ul>
<p>另外Aurora将事务进行更细粒度的划分-mini transection:</p>
<ol>
<li>每个数据库层的事务会被划分为多个 <em>mini</em> 事务，这些事务是有序的，并且被原子地执行</li>
<li>每个 <em>mini</em> 事务由多个连续的日志记录组成</li>
<li><em>mini</em> 事务的最后一个日志记录就是一个 <em>CPL</em> （不是所有的mini事务都能提交，只有CPL才能提交）</li>
</ol>
<p>mini事务+CPL使得每条日志提交，变成了每个事务最后一个mini事务的日志提交（细中带粗）</p>
<h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><blockquote>
<p>In Aurora, background processing has negative correlation with foreground processing.</p>
</blockquote>
<p>正如上文中所描述的，Aurora的设计出发点之一就是要保证前台操作的高吞吐，所有的后台操作的优先级都是低于前台操作，在这个基础上去理解实现细节，稍微有条理一些</p>
<h4 id="写操作"><a href="#写操作" class="headerlink" title="写操作"></a>写操作</h4><p>在遵循quorum机制的基础上，添加了以下实现细节</p>
<ol>
<li>每当主实例写入log收到存储节点的多数派响应后，增加VDL（相当于记录当前PG的log提交进度）</li>
<li>LSN Allocation Limit (LAL)：限定当前分配的LSN不能超过VDL一定的数量（接收的请求不能超过提交过多），以避免写入请求接受速度远远大于数据库写入速度。通过LSN约束，反向降低了接收请求的数量</li>
<li>Segment Complete LSN（SCL）：每个数据段只能看到影响自己的log,每个log中包含一个backlink，用来标记当前log在PG中的前一个log。这个反向连接在节点之间相互通信时用来确定自己缺失的log。</li>
</ol>
<p>在日志写入提交时，并不时满足提交条件即立即提交（asynchronously）</p>
<ul>
<li>处理当前事务的线程将待提交记录添加到在一个单独的事务队列（COMMIT LSN）中等待被确认提交。</li>
<li>当 <em>VDL</em> 不断的增加，数据库找到哪些事务等待被确认，用一个单独的线程给等待的客户端返回事务完成的确认。</li>
<li>属于是 事务处理线程只管先扔到队列里然后继续处理其他事务，由另一个线程专精队列中事务提交+响应 （<del>~没看明白这么做有什么意义</del>）</li>
</ul>
<h4 id="读操作-副本"><a href="#读操作-副本" class="headerlink" title="读操作 + 副本"></a>读操作 + 副本</h4><p>首先介绍一下传统数据库中的读操作：</p>
<ol>
<li>首先寻找缓存中是否存在当前读操作命中的缓存页</li>
<li>如果没有，需要从磁盘加载到内存中</li>
<li>如果缓存满了，需要将缓存中的某些页换出，如果换出的页是脏页（dirty page）,则需要持久化到磁盘上</li>
</ol>
<p>上述机制就导致了读操作可能会引发数据库的持久化写操作，这与Aurora的<strong>前台操作（读操作）与后台操作（持久化）操作负相关</strong>的原则是不符的，理想的方式是：前台读操作不引发缓存持久化，由系统在后台根据前台请求负载周期性（特定条件触发）进行持久化操作。</p>
<p>针对以上问题，Aurora修改了缓存页踢出机制</p>
<ul>
<li>Aurora在缓存踢出是不会进行缓存持久化，而是简单的踢出</li>
<li>只有LSN版本号大于等于VDL的缓存数据页才会被踢出，该性质保证<ol>
<li>所有页面的修改均已持久化在log中</li>
<li>如果缓存失效，可以通过获取最新页来构造当前<em>VDL</em> 所对应的页面。（难道说是：磁盘加载+日志重放）</li>
</ol>
</li>
</ul>
<p>Aurora的读取不需要通过quorum机制实现</p>
<ol>
<li>当从盘里面读一个页的时候，数据库建一个读取点，代表请求发生时的 <em>VDL</em></li>
<li>数据库选择一个带有当前VDL最新修改的数据页节点，返回给读取请求</li>
</ol>
<h4 id="持久化时机"><a href="#持久化时机" class="headerlink" title="持久化时机"></a>持久化时机</h4><p>Aurora不通过缓存踢出持久化，如何实现修改的持久化？实际上还是基于日志的持久化操作</p>
<ol>
<li>数据库系统首先从所有未完成的读取记录中，获得读取数据版本的最小LSN，即之后的所有读取都是在LSN操作发生后进行读取的</li>
<li>通过合并最小LSN之前的日志项并将对应修改的数据页持久化到数据库中，从而实现日志的垃圾回收和修改的持久化</li>
</ol>
<h3 id="实现架构"><a href="#实现架构" class="headerlink" title="实现架构"></a>实现架构</h3><ol>
<li><p>数据引擎是魔改MySQL得到的，论文中说法是：<em>fork of “community” MySQL/InnoDB</em>，支持与MySQL相同级别的写隔离</p>
</li>
<li><p>每个数据库集群包括一个主读写实例和多个只读实例，不同实例之间通过RDS VPC(Amazon Virtual Private Cloud)通信，通过RDS(Amazon Relational Database Service)管理主从实例</p>
</li>
<li><p>存储节点部署在 <em>EC2</em> 虚拟机上，通过SSD存储日志和持久化数据，不同存储节点之间通过Storage VPC通信</p>
</li>
<li><p>并使用<em>Amazon DynamoDB</em>在S3备份存储节点元数据，在节点失效时进行恢复操作</p>
<img src="/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/AmazonAurora%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93/image-20220711104512699.png" class="" title="image-20220711104512699">
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>基本上算是理解Aurora的设计思路和实现原理，但是由于数据库知识缺失的比较多，所以很多知识点和设计出发点理解的不是很深入，不是很理解为什么要用某些设计、某个设计为什么能起到效果，等进一步学习数据库知识后，再回看可能会由理解上的进步。</p>
<p>从Aurora的设计思路和设计出发点，我觉得可以总结几条经验（就是说点废话）</p>
<ol>
<li>问题来自生产实际，Aurora许多设计思想实际上都是来自于具体的业务需求，有需求才有改进的方向，闭门造车哪来的问题？</li>
<li>要做分布式还得是再真正的云环境下。每天学学课本，在一两台机器上跑跑实验，是真的很难切实感受到分布式理论中所要解决的内些问题，因为在小吞吐量下，类似问题根本就不会出现，而从未遇到并解决这些问题，又怎么能说自己精通分布式原理？（想表达的意思：工作还是要找大厂，遇到实际问题并解决，才是真的能力提升，小厂连业务量都不够，哪能遇到问题？）</li>
<li>大的progress往往是修修补补的积累。可以参考Aurora的实现，无非是在amazon已有的技术的基础上，进行拼装修改组合+新思路，才最终实现了一个成熟的系统。研究生阶段很多项目、很多研究上来就是做个什么高大上的东西出来，然而并没有先前的积累我能做出来什么？要么是借学长东风，要么是找现成的方案拼凑，这样能做出来有价值的工作嘛？我觉得并不行。简而言之，就是<strong>培养积累的过程缺失，很难做出有价值的工作</strong></li>
</ol>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>分布式理论</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>分布式数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Frangipani分布式文件系统</title>
    <url>/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Frangipani%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h2 id="Frangipani-分布式文件系统"><a href="#Frangipani-分布式文件系统" class="headerlink" title="Frangipani 分布式文件系统"></a>Frangipani 分布式文件系统</h2><blockquote>
<p><a href="https://dl.acm.org/doi/10.1145/269005.266694">Frangipani: A Scalable Distributed File System</a></p>
</blockquote>
<p>Frangipani是20多年前发表论文中描述的一个分布式文件系统，之所以现在还要深度了解一篇这个“古老”的论文，是因为其中的一些设计思路值得我们学习和研究（<del>不还是因为MIT6.824里要讲吗</del>），Frangipani的设计出发点是要实现一个简单、方便扩展和管理的共享文件系统，在读论文时主要关注的点如下：</p>
<ol>
<li>如何在分布式环境下实现缓存一致性？</li>
<li>如何实现分布式事务？</li>
<li>如何在经常出错的分布式环境中，通过错误恢复实现容错？、</li>
</ol>
<span id="more"></span>
<h3 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h3><p>如下图所示，Frangipani的架构组成分为两层：</p>
<ol>
<li>Frangipani文件系统层（Frangipani file server module）：运行在操作系统kernel中，向kernel注册为一个文件系统实现，为上层的用于应用程序提供提供文件系统的操作接口</li>
<li>Petal 虚拟磁盘层（Petal virutal disk）：为了方便扩展，Frangipani底层基于虚拟磁盘Petal，简化了磁盘管理、扩容等操作</li>
</ol>
<img src="/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Frangipani%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/image-20220712154753633.png" class="" title="image-20220712154753633">
<p>整个系统中共有一下几种不同身份职责的运行进程：</p>
<ol>
<li>Petal server：Petal底层采用多个server+多块磁盘实现大容量分布式容错的虚拟磁盘，Petal server可以理解为接收磁盘操作请求、执行持久化操作的存储服务器</li>
<li>Lock server：文件系统肯定离不开锁，其中lock server用来管理文件系统中的锁（multiple reader/singlewriter lock）。锁由多个lock server分别进行管理，每个lock server管理整个文件系统中部分锁。</li>
<li>Clerk module：每个file server上运行的锁管理进程，用来维护和管理分配给当前file server的锁信息（申请、释放、锁降级等）</li>
<li>Frangipani file server：提供文件系统操作接口，与Petal server交互实现数据操作。</li>
<li>Petal device driver：隐藏petal的细节，使上层file server看起来就是一个大容量磁盘</li>
</ol>
<p>不同身份职责进程的实际部署情况：</p>
<ol>
<li>Pertal server: 运行于实际的存储节点上，多个server共同提供一个 <strong>large, scalable, fault tolerant</strong>的虚拟磁盘</li>
<li>Lock server：可以运行于Pertal server上，也可以运行于Frangipani file server，数量并不是与Pertal server一一对应的</li>
<li>Frangipani file server：运行于任意服务器上，不同的Frangipani file server是等价的，均为用户提供整个文件系统的读取和修改权限</li>
</ol>
<h3 id="文件系统结构"><a href="#文件系统结构" class="headerlink" title="文件系统结构"></a>文件系统结构</h3><p>Frangipani的文件系统结构如下图所示，和Unix的文件系统结构类似，均采用Inode+文件块的形式管理文件存储空间，由于Frangipani底层基于虚拟磁盘（优点是具备更大的可分配空间，缺点是虚拟磁盘上相邻的空间段可能分布于不同server上的磁盘），其对文件系统进行了更细致化的定义和管理。</p>
<img src="/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Frangipani%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/image-20220712161534127.png" class="" title="image-20220712161534127">
<p>Frangipani将虚拟磁盘空间划分为了6个逻辑块</p>
<ol>
<li>参数块（parameters 0-1T）：存储共享配置信息以及文件系统管理信息，实际上只能用到几kb的空间</li>
<li>存储日志块（logs 1-2T）：将1T的空间等大小划分为256块，其中每一块（$2^{40}$bytes）属于一个file server的日志存储区（<u>ps:日志是什么在下一部分进行介绍</u>），每个日志存储区可以存储256个日志，即每一条日志最大空间为（$2^{16}$bytes）。</li>
<li>分配bitmap块（Allocation bitmaps 2-5T）：用来表示剩余的空间块是否为被占用</li>
<li>Inodes块（Inodes 5-6T）：和unix文件系统中的inode功能相同，都是用来记录文件的元数据，每个inode大小与磁盘块大小保持一致512B（避免争抢情况），inode块与bitmap块的对应关系固定，即特定的bitmap永远标识指定的inode。(一方面释放文件是只需要释放bitmap，另一方面方便锁管理)</li>
<li>小块区（Small blocks 6-134T）：每块大小4 KB ($2^12$ bytes)，用来存储大小小于64kb的文件，当文件大小大于64KB时，剩余文件记录存储在大块区中</li>
<li>大块区（Large bolcks 136-$2^{34}$T）：每块大小为1T</li>
</ol>
<p>Frangipani在虚拟磁盘划分时，采用了大量的固定大小分块思想，这样必然会导致内存碎片问题（fragmentation），论文中表示将小文件数据直接存储在inode上，能够缓解这种问题（<del>存疑</del>）</p>
<p>另外Frangipani文件系统还有几个小的trick</p>
<ol>
<li>只有真正的写入虚拟空间时，才会真正的分配实际的物理存储空间</li>
<li>论文中说块的大小都是可调整的，Logs中日志存储区数量也是可调整的（<del>为了掩盖设计不完善，画的饼吧</del>）</li>
</ol>
<h3 id="基于log的持久化保证"><a href="#基于log的持久化保证" class="headerlink" title="基于log的持久化保证"></a>基于log的持久化保证</h3><p>Frangipani采用类似于WAL（write ahead log）日志机制，实现元数据（metadata）的容错，具体实现如下</p>
<ol>
<li>每个对于元数据的操作，首先创建一个相关日志写入到file server本地缓存中</li>
<li>日志周期性的将日志缓存持久化到petal磁盘上（即上面的logs），每个file server有自己单独的log存储区（论文中描述同样支持实时同步，即生成日志立刻持久化到petal磁盘上）</li>
<li>只有在日志持久化以后，才会真正的修改对应的metadata（<del>论文里没有说什么时候向客户端响应修改成功或者说修改对客户端可见？我觉得是修改缓存成功后，因为论文中说持久化的周期时30S，如果在持久化以后延迟太高</del>）</li>
<li>对于用户数据（修改文件内容等），Frangipani不通过日志管理，即<strong>不保证持久性</strong></li>
</ol>
<p>随着时间的的增长，log的数量会越来越多，如何解决log的空间占用/不足问题？</p>
<ul>
<li>Frangipani将Petal磁盘上的log存储区管理为环状存储（circular buffer），当log空间不足，删除最老的25%的日志（老日志确保大部分均已提交，并且在踢出日志时，Frangipani会检查未提交日志，执行完毕再踢出存储）</li>
<li>在两次从内存写入Petal磁盘上的刷新间隔中，最多允许1000-1600个对于元数据的操作</li>
</ul>
<h3 id="基于锁的一致性保证"><a href="#基于锁的一致性保证" class="headerlink" title="基于锁的一致性保证"></a>基于锁的一致性保证</h3><p>既然是文件系统就离不开锁，Frangipani提供了写锁和读写锁（multiplereader/singlewriterlocks）分布式锁实现，以实现文件系统的中的并发控制，从而保证数据一致性，基本实现机制如下：</p>
<ol>
<li>多个lock server+lock clerk模式：每个lock server管理一部分的锁，每个file server对应一个lock clerk，负责管理当前server的锁以及与lock server进行交互</li>
<li>其中锁以64位整数命名，每个lock server上的锁组织在一个以ASCII字符命名的table中</li>
<li>为了避免file server进程意外退出导致<strong>无限期占有锁</strong>，lock server分配锁时带有<strong>lease identifier</strong>（超时时间为30S），client必须在超时之前向lock server更新lease</li>
<li>为了避免死锁，Frangipani为整个文件系统中的锁分配了<strong>全局编号</strong>，client获取锁之前首先要确定需要的所有锁，之后<strong>按照编号顺序从小到大获取</strong>，失败则释放已经占有的锁，从头开始从新获取。（<strong>分布式事务</strong>）</li>
</ol>
<p>补充一条：Frangipani的锁是<strong>sticky</strong>的，即client会一直持有锁，直到另外一个client申请锁</p>
<p>lock server和lock clerk相关的锁操作包括：</p>
<ul>
<li>request：clerk向server发送的申请锁的请求</li>
<li>grant：server向clerk发送的授予锁的响应</li>
<li>revoke：server向clerk发送的撤销锁的请求</li>
<li>release：clerk向server发送的释放锁</li>
<li>另外以上四个操作，能够实现锁的升级和降级操作</li>
</ul>
<p><strong>如何保证一致性</strong></p>
<p>锁通过控制并发实现一致性控制，实际上就是<strong>dirty cache刷新时机和锁重分配之间的协调关系</strong>，当某个client在持有锁的过程中对数据进行了修改，此时要发生锁权限的变更，根据不同情况需要进行不同处理：</p>
<ol>
<li>当前client持有读/读写锁，因为争抢要释放：首先要将dirty cache持久化到petal磁盘中，并将cache标记为无效（等待垃圾回收），最后再释放锁</li>
<li>当前client持有写锁，要降级为读写锁共享锁（只读不写）：首先要将dirty cache持久化到petal磁盘中，锁降级。此时并不需要invalidate cache，因为另外的client也是读，并不会修改数据</li>
</ol>
<p>另个一角度理解（<strong>锁在缓存在，锁亡缓存亡</strong>-锁代表独占权，独占权保证了）：</p>
<ul>
<li>拥有数据的缓存，同时肯定拥有锁（如果是dirty cache，则拥有写锁）</li>
<li>当server释放锁，同时要释放锁对应的缓存段</li>
</ul>
<p>通过上述机制，即可保证Fragipani元数据的持久性和一致性</p>
<h3 id="容错和恢复（fault-tolerant-amp-recovery）"><a href="#容错和恢复（fault-tolerant-amp-recovery）" class="headerlink" title="容错和恢复（fault-tolerant &amp; recovery）"></a>容错和恢复（fault-tolerant &amp; recovery）</h3><p>容错和恢复是拆不开的两个话题，一般分布式系统中通过多server实现容错，当一个server崩溃其他server能够继续正常运行，恢复纸崩溃server恢复重新提供服务的过程，两者共同保证了分布式系统的高可用性质。由于Frangipani存在多种不同身份职责的进程，因此存在多种情况下的容错和恢复。</p>
<h4 id="Lock-server"><a href="#Lock-server" class="headerlink" title="Lock server"></a>Lock server</h4><p>lock server可以看作一系列地位对等的p2p节点，相互直接之间通hearteart消息，监控状态。通过公式算法（Paxos）实现对于元数据的管理和一致性保证。</p>
<ul>
<li>元数据包括locker server列表，每个server负责管理的锁</li>
<li>当lock server失效或者新的lock server加入时，lock会在不同节点间进行均匀分配，保证负载均衡</li>
</ul>
<p>当一个lock server失效时，他所持有的锁以及对映锁状态会重新分配到其他lock server上。lock server恢复即直接作为新的lock server加入server群即可</p>
<h4 id="File-server"><a href="#File-server" class="headerlink" title="File server"></a>File server</h4><p>虽然Frangipani中存在多个file server，然而每个server只为其挂载在unix系统提供服务，当file server崩溃时，唯一的容错措施即在重启file server(<del>看起来不是那么的分布式，或者可以理解为文件系统接口层实际上还是单机</del>)。</p>
<p>虽然只能通过重启进行file server的容错和恢复，在file server崩溃时，需要执行一系列操作以保证数据的一致性，因为当前file server可能存在未持久化的修改、占有锁资源等，具体机制如下：</p>
<ol>
<li><p>当client/lock server长期收不到file server响应时，即认为file server失效</p>
</li>
<li><p>当认定file server失效后，启动 recovery demon，获得失效server的log和lock,根据log重新执行未执行的更新，完成之后释放锁资源</p>
<blockquote>
<p>the changes to a block are applied only,if the block version number is less than the record version number.</p>
<p>通过数据的version number判断是否需要重新执行log记录的操作</p>
</blockquote>
</li>
</ol>
<p>上述机制中，即使是由于网络分区问题导致的无法联系到file server，file server由于无法更新相关锁的lease，即没有权限操作数据，<strong>不会出现“脑裂”问题</strong></p>
<h4 id="Petal-server"><a href="#Petal-server" class="headerlink" title="Petal server"></a>Petal server</h4><p>Frangipani实际上是偷懒了，由于Petal本身支持容错和恢复，所以存储层可以认识是可靠的</p>
<h3 id="备份（Backup）"><a href="#备份（Backup）" class="headerlink" title="备份（Backup）"></a>备份（Backup）</h3><blockquote>
<p><strong>Copy-on-write</strong>：sometimes referred to as <strong>implicit sharing</strong><a href="https://en.wikipedia.org/wiki/Copy-on-write#cite_note-1">[1]</a> or <strong>shadowing</strong>,<a href="https://en.wikipedia.org/wiki/Copy-on-write#cite_note-2">[2]</a> is a resource-management technique used in <a href="https://en.wikipedia.org/wiki/Computer_programming">computer programming</a> to efficiently implement a “duplicate” or “copy” operation on modifiable resources. - wikipedia</p>
<p>对于Copy-on-write，实际上可以简单理解为为了避免写阻塞读，写在副本上写，读此时在原数据上读，写完将副本更新到原数据上</p>
</blockquote>
<p>在Petal的Copy-on-wirte机制上加了一点微调，实现了自己的备份机制（备份启动不需要recovery过程）</p>
<ul>
<li>采用了barrier同步机制，使用一个全局锁作为barrier</li>
<li>file server执行任何修改之前，必须获得这个共享全局锁</li>
<li>当备份进程要执行时，向所有持有全局锁的file server申请释放，file server释放之前首先将dirty cache持久化，并释放其他锁后进入barrier</li>
<li>当所有file server进入barrier后，备份进程获得全局锁，开始备份，备份完成其他file server退出barrier</li>
</ul>
<p>显而易见的缺点是当系统备份时，整个系统只读不能写</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>读完这篇论文发现其中很多设计还是很落后的（file server假分布式等），也存在很多未解决的问题（写请求发送过程中锁超时等），然而其中比较值得学习的就是其锁的管理和缓存一致性的实现。</p>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>分布式理论</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>分布式文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title>GFS论文总结</title>
    <url>/2022/04/17/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/GFS%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="GFS"><a href="#GFS" class="headerlink" title="GFS"></a>GFS</h3><blockquote>
<p>论文地址：<a href="https://research.google/pubs/pub51/">The Google File System</a></p>
</blockquote>
<p>google file system，入门大数据必读三篇文章之一，最近懒得看视频，小研究一波这篇论文，读完发现这种系统论文要比人工智能论文难读一些，涉及的技术比较多，粗浅的总结一下。</p>
<h4 id="论文结构"><a href="#论文结构" class="headerlink" title="论文结构"></a>论文结构</h4><p><del>论文结构有点像我的毕业论文</del>，首先从系统的结构出发，描述系统架构、组成等,从静态角度了解系统组成，然后从动态系统交互出发，描述系统交互，主要数据和操作流，最后单独两章描述mater节点的主要职责以及系统如何实现fault tolerance.</p>
<ol>
<li><p>intro 介绍</p>
<p>没讲很多背景，直接讲GFS这套解决方案与以往的GFS不同点，解决不同问题：</p>
<ol>
<li>分布式系统中组件经常失效（norm rather than exception）</li>
<li>按照以往的标准，目前文件大小都很大</li>
<li>目前对文件操作主要是添加和顺序读</li>
<li>面向应用设计GFS，增加了整个系统的灵活性</li>
</ol>
</li>
<li><p>DESIGN OVERVIEW </p>
<p>系统的组成+一致性模型，基本讲明白了系统怎么实现的文件系统功能</p>
</li>
<li><p>SYSTEM INTERACTIONS</p>
<p>描述了系统与client读写文件的交互流程，在这过程中如何保证GFS特性（从使用的角度，描述系统）</p>
</li>
<li><p>MASTER OPERATION</p>
<p>阐述master节点的职责，client与GFS交互中不直接相关的操作（从管理的角度）</p>
</li>
<li><p>FAULT TOLERANCE AND DIAGNOSIS</p>
<p>阐述容错机制（从策略角度）</p>
</li>
<li><p>MEASUREMENTS + EXPERIENCES + RELATED WORK + CONCLUSIONS</p>
<p>实验验证+经验总结+相关工作+总结</p>
</li>
</ol>
<span id="more"></span>
<h4 id="前提假设"><a href="#前提假设" class="headerlink" title="前提假设"></a>前提假设</h4><p>论文中说了GFS的设计是<strong>文件系统API</strong>和<strong>服务于应用</strong>的co-designing（需求来源于生活）， 对其解决的问题做了一定的假设和限定：</p>
<ol>
<li>分布式系统构建于便宜、容易出错的硬件上</li>
<li>系统存储一定数量的大文件（GB级别）</li>
<li>系统将要面临的读操作主要为大量的顺序读和少量随机读</li>
<li>系统面临的写操作主要为大量顺序添加操作，但也支持随机写</li>
<li>系统必须高效支持多客户端并发append写操作</li>
<li>重要性：高带宽&gt;低延时</li>
</ol>
<p>从前提假设中，我们就能得到GFS设计目标中的重点</p>
<ol>
<li><strong>重点</strong>：为<strong>多客户端</strong>的<strong>大文件存储</strong>以及<strong>顺序读+追加写</strong>提供<strong>高稳定高带宽</strong>服务</li>
<li><strong>相对而言不重要的</strong>（系统支持，但不一定效率高）：随机读+随机写+低延时</li>
</ol>
<h4 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h4><p>单管理节点（master）+多存储节点（chunk server）+多客户端（client）架构</p>
<ol>
<li>master节点职责<ul>
<li><strong>管理文件系统的元数据</strong>，包括访问控制信息，文件命名空间，文件到存储块的映射，存储块到存储节点的映射</li>
<li><strong>进行系统管理活动</strong>，包括存储块释放，垃圾回收，不同存储节点上存储块的迁移、复制，定时获取chunk server状态</li>
</ul>
</li>
<li>chunk server节点职责<ul>
<li>存储文件存储块（chunk）</li>
<li>通过HeartBeat消息，告知master节点自身状态</li>
<li>与client进行文件操作交互</li>
</ul>
</li>
<li>client主要操作<ul>
<li>与master节点进行元数据操作（获取文件存储的chunkserver等）</li>
<li>与chunk server进行文件操作（文件实际读写）</li>
</ul>
</li>
</ol>
<img src="/2022/04/17/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/GFS%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220413194306905.png" class="" title="image-20220413194306905">
<h4 id="几个主要概念"><a href="#几个主要概念" class="headerlink" title="几个主要概念"></a>几个主要概念</h4><h5 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h5><blockquote>
<p>The master stores three major types of metadata: the file and chunk namespaces, the mapping from files to chunks, and the locations of each chunk’s replicas</p>
</blockquote>
<p>文件系统的元数据，存储在master节点的<strong>内存中</strong>（论文里不断强调这一点，为了方便master扫描，执行一些列管理操作，主要包括三种类型</p>
<ol>
<li>文件和文件块的命名空间信息（前缀压缩减少空间占用）</li>
<li>文件到文件块的映射</li>
<li>文件块的存储位置<ul>
<li>master节点在每次启动时，向chunk server请求其拥有的chunk信息，初始化文件块存储位置信息</li>
<li>并通过不断的Heatbeat Message 保证chunk信息不过时。（某种程度的低耦合）</li>
</ul>
</li>
<li>operation Log 操作记录</li>
</ol>
<h5 id="Operation-Log"><a href="#Operation-Log" class="headerlink" title="Operation Log"></a>Operation Log</h5><p>记录系统操作、作为系统逻辑时间的最重要的元数据</p>
<ul>
<li>operation log持久存储，多处备份，每次操作只有真正的记录到Operation Log中时，才会对客户端可见</li>
<li>operation log大小超过一定程度时，系统创建checkpoint，将当前operation log存储到本地</li>
<li>以compact B-tree的形式存储checkpoint，方便快速读取和加载</li>
<li>checkpoint的创建与切换新 operation log file并行进行</li>
</ul>
<h5 id="atomically-at-least-once"><a href="#atomically-at-least-once" class="headerlink" title="atomically at least once"></a>atomically at least once</h5><p>追加写操作保证”原子性“，我理解的是：真实追加写入的offset相较于client发出请求offset不一定一致，但是我保证所有副本最后在追加的offeset一定相同，并将这个offset返回给客户端。</p>
<p>举个例子，例如向A,B,C三个chunk server的同一文件的副本追加文件，<strong>A，B写入成功，C写入失败</strong>，GFS并不会单独重新在C上追加，而是在C上补充空白（insert padding or record duplicates in between.），使得三个文件的偏移量相同，重新写入ABC。</p>
<p>这样做会导致文件中出现<strong>无效数据</strong>，但是论文中说这些数据和用户数据相比<strong>微不足道</strong>（are typically dwarfed by the amount of user data）</p>
<h5 id="一致性模型中的consistent-和-defined"><a href="#一致性模型中的consistent-和-defined" class="headerlink" title="一致性模型中的consistent 和 defined"></a>一致性模型中的consistent 和 defined</h5><p>两者定义</p>
<ul>
<li><p>consistent 指所有的client看到相同的数据，即所有副本均相同</p>
</li>
<li><p>defined 指看到自己操作对于数据的改变 = 期望中的改变</p>
</li>
</ul>
<p>成功和失败的操作定义为：</p>
<ul>
<li>成功：可能会导致操作后<strong>undefined</strong>(无法预期操作的结果)，但数据仍为<strong>consistent</strong></li>
<li>失败：导致unconsistent，即不同数据备份不一致</li>
</ul>
<h5 id="Leases-and-Mutation-Order"><a href="#Leases-and-Mutation-Order" class="headerlink" title="Leases and Mutation Order"></a>Leases and Mutation Order</h5><p>lease用来记录对于一份文件多个并发操作的执行顺序，由master节点选取其中chunk server一个作为primary lease决定执行顺序，所有文件副本均按照primary lease操作顺序执行</p>
<ul>
<li>每个lease 60秒失效</li>
<li>在与master节点的Heartbeat中primary的授权信息（These extension requests and grants are piggybacked on the HeartBeat messages regularly exchanged between the master and all chunk server）</li>
</ul>
<h4 id="系统交互"><a href="#系统交互" class="headerlink" title="系统交互"></a>系统交互</h4><img src="/2022/04/17/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/GFS%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220415142928379.png" class="" title="image-20220415142928379">
<p>以写流程流程为例，涉及client、Master、Chunk server之间的交互</p>
<ol>
<li>Client 向 Master请求写入文件的Chunk Server地址（包括备份的Chunk Server）</li>
<li>Master 返回给Client请求的Chunk Server地址，Client拿到地址后，将写入文件发送到所有目标Chunk Server中（不是<strong>分发</strong>，而是<strong>链式传输</strong>，论文中称“decoupling the data flow from the control flow”，提高了系统performance）</li>
<li>所有接收文件的Chunk Server确认接收完成后，client向被选为primary lease的Chunk Server发送写请求，由改Chunk Server确定包括该请求在内的其他并发请求的执行顺序，通知其他Replica Chunk Server，包括自在内按照该顺序执行操作（forwards the write request to all secondary replicas）</li>
<li>Replica Chunk Server向Primary Chunk Server返回执行完成确认，Primary Chunk Server向Client返回执行完成确认。</li>
</ol>
<p>一旦其中有一步失败，即向Client汇报失败，由Client自己进行处理（重试操作）</p>
<h4 id="几个主要操作（策略）"><a href="#几个主要操作（策略）" class="headerlink" title="几个主要操作（策略）"></a>几个主要操作（策略）</h4><h5 id="Data-Flow-数据传输"><a href="#Data-Flow-数据传输" class="headerlink" title="Data Flow-数据传输"></a>Data Flow-数据传输</h5><p>在Client向多个备份Chunk Server传输文件时，并不是采取传统的一个Client对多个Server的集中发送的方式，而是采取<strong>链式传输</strong></p>
<ol>
<li>Client首先选取最近(<strong>IP地址上的近</strong>)的Chunk Server传输文件</li>
<li>该Chunk Server再选取离他最近的未传输过文件的ChunkServer传输文件</li>
<li>重复传输，直到所有待传输Chunk Server获得文件</li>
</ol>
<h5 id="Atomic-Record-Appends"><a href="#Atomic-Record-Appends" class="headerlink" title="Atomic Record Appends"></a>Atomic Record Appends</h5><p>传统基于offset的写入，同一个区域的并发写入操作是无法序列化的（需要严格同步机制），针对record append操作，GFS舍弃了由应用输入offset的机制，应用只需要写入数据，由GFS写入后，向应用返回offset，其中一致性保证 <strong>atomically at least once</strong> </p>
<ul>
<li>若添加的文件使得chunk大小超出了最大限制，Primary Chunk Server会在填充当前Chunk剩余空间，创建新Chunk存储添加的文件内容</li>
<li>GFS限制写入文件大小小于Chunk块大小的四分之一，避免padding导致的碎片问题过于严重</li>
</ul>
<h5 id="Namespace-Management-and-Locking"><a href="#Namespace-Management-and-Locking" class="headerlink" title="Namespace Management and Locking"></a>Namespace Management and Locking</h5><p><del>说实话没理解并不是很深刻</del>， GFS为系统中的每个路径前缀均设置了读写锁</p>
<ol>
<li><p>读操作需要获取叶子节点所有前缀read-lock</p>
</li>
<li><p>写操作只需要叶子节点路径write-lock(因为目录并不是实际的数据结构，不需要修改目录，只需修改文件)</p>
<img src="/2022/04/17/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/GFS%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220415152805791.png" class="" title="image-20220415152805791">
</li>
</ol>
<h5 id="Snapshot"><a href="#Snapshot" class="headerlink" title="Snapshot"></a>Snapshot</h5><p>系统快照，快速保存文件系统状态，其主要步骤为</p>
<ol>
<li>master首先撤下快照涉及文件的primary lease(避免在快照过程中文件上发生操作)</li>
<li>master将log record 操作记录存储到磁盘中</li>
<li>快照完成后，master接收到来自client对快照文件访问的新请求（通过操作记录数大小判断）时，延迟返回结果，创建一个与原Chunk相同的新Chunk供Client访问（懒备份）</li>
</ol>
<p>这一过程对于Cilent来说是无感的</p>
<h5 id="Garbage-Collection"><a href="#Garbage-Collection" class="headerlink" title="Garbage Collection"></a>Garbage Collection</h5><p>当删除一个文件时，GFS并直接回收文件占用资源，而是将文件名修改为hidden状态，待日后删除，基本流程</p>
<ol>
<li>删除文件，master结点记录删除操作，在chunk中将文件修改为hidden状态</li>
<li>master扫描到hidden状态文件，删除meta数据中关于hidden文件信息</li>
<li>chunk server与master 的Heatbeat message交换中，发现master已没有了这部分信息，<strong>chunk server回收空间</strong></li>
</ol>
<p>同样通过Garbage Collection回收stale Replica</p>
<h5 id="Data-Integrity"><a href="#Data-Integrity" class="headerlink" title="Data Integrity"></a>Data Integrity</h5><p>checksum+chunk version number机制保证数据正确性和及时性（up-to-date）</p>
<ol>
<li>每次读操作时，cunkserver验证本机数据check sum是否正确，不正确返回失败</li>
<li>为每个副本维护一个版本号，操作递增，当版本号不同时，由master在Garbage Collection回收过期副本</li>
</ol>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>按照自己的理解对GFS论文简单的理解了一下，感觉有些东西理解的还不是很透彻，慢慢深入了解</p>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>分布式理论</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>分布式文件系统</tag>
        <tag>GFS</tag>
      </tags>
  </entry>
  <entry>
    <title>Mapreduce论文总结</title>
    <url>/2022/04/24/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Mapreduce%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="Mapreduce"><a href="#Mapreduce" class="headerlink" title="Mapreduce"></a>Mapreduce</h3><blockquote>
<p>论文地址: <a href="https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf">MapReduce: Simplified Data Processing on Large Clusters</a></p>
</blockquote>
<p>Mapreduce分布式编程模型，理解起来比较简单，主要总结一下模型+实现细节</p>
<h4 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h4><p>将任务划分为Map和Reduce两个阶段由用户实现，每个阶段输入输出key-value对（形象理解可以看论文中的例子）</p>
<img src="/2022/04/24/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Mapreduce%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220420082702737.png" class="" title="image-20220420082702737">
<ol>
<li>Map 输入key-value，经过处理输出新的中间 key-value对，由MapReduce执行程序，将相同<strong>中间key</strong>聚集发送给某一个reduce执行程序</li>
<li>Reduce 输入一个中间key和key对应的value列表，reduce执行具体聚集操作后，获得最终的输出key-value</li>
</ol>
<span id="more"></span>
<h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>基本执行流程包括</p>
<ol>
<li>将输入文件划分为m份（16-64mb），对应m个map任务。在集群上启动多份应用程序</li>
<li>其中一个master程序，将map任务和reduce任务分配给空闲的worker(不同的worker可能是一台机器)</li>
<li>map任务程序首先执行，读取对应的文件块，将输出中间键值对缓存在内存中</li>
<li>每隔一段时间，map worker将缓存中的中间键值对存储到本地磁盘（根据key-&gt;reduce的映射进行partition）。这些中间键值对的地址，将传给master worker,供reduce worker远程读取</li>
<li>reduce worker由master唤醒后，通过RPC读取映射到本reduce worker的中间键值对输出</li>
<li>当属于某个reduce worker的中间键值对读取完成后，按照中间键值排序，对不同的key分组处理，也就是所谓的reduce输入key-value list</li>
<li>所有reduce程序完成后，结束mapreduce过程</li>
</ol>
<p>总结一下，就是map-&gt;partition-&gt;sort-&gt;reduce</p>
<img src="/2022/04/24/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Mapreduce%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220420083217188.png" class="" title="image-20220420083217188">
<h4 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h4><p>为map和reduce任务定义了执行状态存储在master结点中，通过状态实现容错，每个任务的状态为</p>
<ol>
<li>idle 等待处理</li>
<li>in-progress 处理中</li>
<li>completed 处理完成</li>
</ol>
<p>master定期ping所有执行或者执行过任务的worker，若worker失效，将worker上执行过的所有任务（in-progress或者completed）状态设置为idle，等待分配给其他worker处理。</p>
<ul>
<li>若mapreduce任务执行在类似GFS的文件系统上，则complete类型任务不需要重新执行，因为输出文件不仅仅存储在失效的worker上</li>
<li>每当一个map任务重新执行后，需要通知所有的reduce任务该map任务的新执行</li>
</ul>
<h4 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h4><ol>
<li>map任务分配任务时遵循“靠近输入文件的原则”，首先考虑分配在包含输入文件的机器上，其次考虑靠近存储文件的机器上（移动计算比移动数据更有效）</li>
<li>用一些backup worker，替代拖后腿的worker，提升系统效率下限</li>
<li>map和reduce任务数量M和R要比机器数量大得多，以更好地负载均衡（每个workerp平均一个任务都分不到，谈何负载均衡?）和恢复错误（没太理解？）</li>
</ol>
<h4 id="问题思考"><a href="#问题思考" class="headerlink" title="问题思考"></a>问题思考</h4><ol>
<li><p>如何把map任务的输出分配给R个reduce worker（<strong>partition</strong>）</p>
<ul>
<li><p>原则：相同key必须分配到同一个reduce worker</p>
</li>
<li><p>最简单方式就是使用key的哈希值进行映射</p>
<img src="/2022/04/24/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Mapreduce%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220420092210098.png" class="" title="image-20220420092210098">
</li>
</ul>
</li>
<li><p>既然有了partition，为什么要combiner?</p>
<ul>
<li>相同key值的key-value对可能很多(例如wordcount)，通过网络传播对带宽压力较大，</li>
<li>在map端先进行<strong>一部分的reduce操作</strong>，合并重复key，也能一定程度减轻reduce的计算压力</li>
</ul>
</li>
<li><p>直觉上为什么mapreduce能解决分布式计算问题？</p>
<p>从程序角度看，不管的单机还是分布式，其本质都是<strong>程序读取输入-&gt;程序计算-&gt;程序输出结果</strong>，我要实现分布式程序，无非要实现 <strong>分布计算 = 单机计算</strong></p>
<ol>
<li><strong>程序输入：</strong> 分布式文件存储在不同机器上，自然而然能够想到<strong>多个map程序读取文件</strong>的操作</li>
<li><strong>程序计算：</strong>难点在于分布式读入文件，我如何实现等价于单机计算的效果？我觉得这就reduce设计巧妙地地方，<strong>文件的分块不等于计算的逻辑分块</strong>，通过map—&gt;reduce程序的计算，实际上将<strong>文件的分块映射到计算的逻辑分块</strong></li>
<li><strong>程序输出：</strong> reduce输出实际逻辑子问题的输出-&gt;实际问题的输出（这一点我还没想明白，类似于归并排序 reudce制作到了归没有做并）</li>
</ol>
<p>以wordcount为例，讲一下我的理解</p>
<ol>
<li><strong>文件分块：</strong>整个文本文件-&gt;子文本文件块 （<strong>问题数据规模上分布</strong>）</li>
<li><strong>计算逻辑分块</strong>：统计每个不同字的字数-&gt;单独统计每个字的字数（<strong>问题逻辑规模上分布</strong>）</li>
</ol>
<p>map解决每个文本文件内字数的统计，reduce解决每个字字数统计</p>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>mapreduce只看理论理解还是太表面，还是需要写代码实战，学到了继续深化吧</p>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>分布式理论</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title>共识算法-Paxos</title>
    <url>/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Paxos/</url>
    <content><![CDATA[<h2 id="分布式共识算法-Paxos"><a href="#分布式共识算法-Paxos" class="headerlink" title="分布式共识算法-Paxos"></a>分布式共识算法-Paxos</h2><p>共识算法目的是通过一系列通信约束，使得依靠不可靠通信网络上的不同结点之间能够通过算法得到一致的结论（区别于分布式一致性这一概念）,共识算法是实现<a href="https://en.wikipedia.org/wiki/State_machine_replication">state machine replication</a> 的基础，后者是分布式系统中较为重要的备份容错算法策略</p>
<h3 id="分布式CAP"><a href="#分布式CAP" class="headerlink" title="分布式CAP"></a>分布式CAP</h3><p>一个分布式系统只能满足一下三条性质中的两条</p>
<ol>
<li>Consistency(一致性)：每个客户端的读操作均能读取到最近的写入内容或者返回错误</li>
<li>Availability(可用性): 客户端的每个请求均能收到非报错响应（回复不一定正确即不一定保证C）</li>
<li>Partition Tolerance(分区容错性)：不同结点或分区之间的网络故障不会影响系统的正常运行</li>
</ol>
<p>为什么说分布式系统只能保证其中两条？</p>
<ol>
<li>分布式系统由于系统结点/分区分布于不同的网络环境，通过网络进行通信，不可避免地会出现结点由于网络故障下线问题，为了系统的可用性，必须保证Partition Tolerance</li>
<li>当部分结点出现网络故障时，面对客户端请求，系统有两种处理方式<ul>
<li>回退操作，相当于保证了C但舍弃了A</li>
<li>继续执行操作，由于部分结点下线，此时执行操作无法保证C，但是保证了A</li>
</ul>
</li>
</ol>
<p>单机系统由于不会出现分布网络失效，所以也就不需要P，可以同时保证CA</p>
<span id="more"></span>
<h3 id="Paxos"><a href="#Paxos" class="headerlink" title="Paxos"></a>Paxos</h3><blockquote>
<p>The Paxos protocol was first submitted in 1989 and named after a fictional legislative consensus system used on the <a href="https://en.wikipedia.org/wiki/Paxi">Paxos</a> island in Greece</p>
</blockquote>
<p>Paxos是经典的基于消息传递共识算法，然而理解难度过高，简单整理以加深印象，算法中定义了四种不同的角色（一个结点可以有多个不同的角色）</p>
<ol>
<li>Client: 客户端向系统发送请求（例如：向分布式文件系统写入）</li>
<li>Proposer: 提出提案</li>
<li>Acceptor:  判断是否接受提案</li>
<li>Learner: 当提案被接受时，执行具体操作（例如：执行写入操作）</li>
<li>Leader: 唯一的Proposer</li>
</ol>
<p>其他概念</p>
<ol>
<li>Proposal: 提案，每个提案由提案号n和提案值组成v:<n, v></li>
<li>Quorums: 由大多数Accepter组成的集合，用来投票确定是否接受提案</li>
<li>safety/liveness/fault tolerance: 共识算法的三个形式，类似于CAP三者只能满足其二<ul>
<li>safety: “坏事”永远不会发生</li>
<li>liveness: “好事”终将发生</li>
<li>没太理解和fault tolerance之间的关系和区别</li>
</ul>
</li>
</ol>
<p>Paxos保证safety+fault tolerance，不保证liveness</p>
<img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Paxos/image-20220531103804034.png" class="" title="image-20220531103804034">
<h4 id="算法流程（Basic-Paxos）"><a href="#算法流程（Basic-Paxos）" class="headerlink" title="算法流程（Basic Paxos）"></a>算法流程（Basic Paxos）</h4><p>分为确定提案值+接受提案两个阶段</p>
<ol>
<li>阶段1（prepare+promise）：该阶段用来收集<strong>历史提案信息</strong><ul>
<li>由Proposer创建Prepare message，其中包括此次提案号n（n保证大于之前任何Prepare中的n），发送给至少Quorums数量的Acceptor</li>
<li>接收到Prepare message的Acceptor，根据提案号n确定返回消息类型<ul>
<li>若当前提案号小于历史Prepare message和提交提案的提案号，不进行响应（过滤历史）</li>
<li>若大于则根据是否存在历史提交提案，返回历史或者空的<strong>提案值+提案号</strong>（收集信息）</li>
</ul>
</li>
<li>Proposer收到大多数Acceptor的响应信息，进入下一阶段</li>
</ul>
</li>
<li><p>阶段2（accept+accepted）：提交<strong>提案</strong></p>
<ul>
<li>Proposer根据接收到的响应类型确定提案value，之后发送Accept message (n,v)到Acceptor<ul>
<li>若响应中包含提案号m+提案value，将当前提案value设置为最大m对应的value</li>
<li>若只有Promise响应，将提案value设置为 originally wanted value</li>
</ul>
</li>
<li>若当前提案号小于Acceptor历史Prepare message和提交提案的提案号，不进行响应</li>
<li>否则提交当前提案，返回信息</li>
<li>Proposer收到大多数Acceptor的响应信息，确认提交，将提案值发给所有Learner</li>
</ul>
<img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Paxos/image-20220531112542422.png" class="" title="image-20220531112542422">
</li>
</ol>
<p>只看算法流程很难理解paxos为什么要这么设计，结合以下几个设计出发点方便理解</p>
<ol>
<li>paxos整个流程只针对一个值达到共识（多个提案仅仅是不同Proposer对于当前值的提议，最终决定一个），与直觉上的一个提案对应一个共识不同</li>
<li>算法基于FIFO的原则，现提出提案的最先可能达成共识</li>
</ol>
<p>根据对算法流程的分析，Proposer和Acceptor的行为可以理解为：</p>
<ul>
<li>Acceptor：无条件接收更新的提案</li>
<li>Proposer：第一任务是继续提交之前未提交成功的提案值（第一阶段收集信息的作用），只有在没有历史的时候才能自己确定提交value</li>
</ul>
<p>Paxos之所以能够保证safty，即保证得到共识，可以从以下角度理解：</p>
<ul>
<li>阶段1的存在，保证了不会出现split vote问题（一旦有过半的Acceptor提交了提案，后续继续提交提案的value只能从这些Acceptor中确定）</li>
<li>提案号递增+多数提交，保证了一致性</li>
</ul>
<p>Basic Paxos无法保证livness，即无法保证算法能够终止找到共识</p>
<ul>
<li>两个Proposer在Accept阶段争抢,如下<ul>
<li>Proposer1此时提出提案1，即将走到Accept阶段时，Proposer2提出提案2，导致提案1的编号因为小于最新提案2，无法通过，Proposer1重新提交提案3，导致提案2小于提案3，无法通过</li>
<li>最终导致无限的抢占，无法达共识</li>
</ul>
</li>
<li>通过Proposer的随机休眠避免抢占冲突发生</li>
</ul>
<img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Paxos/image-20220531112719316.png" class="" title="image-20220531112719316">
<h4 id="Multi-Paxos"><a href="#Multi-Paxos" class="headerlink" title="Multi Paxos"></a>Multi Paxos</h4><p>Paxos存在的如下问题，导致其只适合作为理论上的分布式共识算法模型</p>
<ol>
<li>模型上只针对一个共识的实现，无法重复进行多次共识操作</li>
<li>两阶段流程在高并发情况下不仅存在livness问题，通信成本价高</li>
</ol>
<p>为了解决Paxos算法上述问题，改进得到Multi Paxos算法</p>
<ol>
<li>选举一个固定的Proposer,提案编号中变为 Proposer编号+提案编号<ul>
<li>通过一个和basic Paxos相同两阶段选举主Proposer</li>
</ul>
</li>
<li>每次提出提案，直接进行第二阶段</li>
</ol>
<img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Paxos/image-20220531143508710.png" class="" title="image-20220531143508710">
<p>Multi Paxos就和Raft以及Zab很接近了</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a href="https://en.wikipedia.org/wiki/Paxos_(computer_science">Paxos (computer science) From Wikipedia</a>)</li>
<li><a href="https://zhuanlan.zhihu.com/p/31780743">知乎：Paxos算法详解</a></li>
</ol>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>分布式理论</category>
      </categories>
      <tags>
        <tag>共识算法</tag>
        <tag>CAP</tag>
      </tags>
  </entry>
  <entry>
    <title>共识算法-Raft</title>
    <url>/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Raft/</url>
    <content><![CDATA[<h2 id="分布式共识算法-Raft"><a href="#分布式共识算法-Raft" class="headerlink" title="分布式共识算法-Raft"></a>分布式共识算法-Raft</h2><p>Raft是基于Paxos改进的共识算法，其最开始的设计目标即为容易理解且容易实行，所以相较于Paxos学起来更容易（<del>确实比Paxos容易，但我还是看不懂啊</del>）。</p>
<p>Raft中定义了三种角色：</p>
<ol>
<li>Leader：唯一的leader，负责与Client交互，在其他follower上备份log日志。</li>
<li>Follower: 多个Follower，接收Leader的日志备份请求，当Leader出现问题时，选举成为新的leader。</li>
<li>Candidate: 当Follower认为Leader失效时，Follower状态转换为Candidate，进行Leader的竞选。</li>
</ol>
<p>Raft中只包括两种类型的RPC消息</p>
<ol>
<li>RequestVote RPC：参与选举的Candidate发送的选举消息</li>
<li>AppendEntries RPC:  Leader向Follower发送的添加日志信息</li>
</ol>
<span id="more"></span>
<h3 id="Raft的一般流程"><a href="#Raft的一般流程" class="headerlink" title="Raft的一般流程"></a>Raft的一般流程</h3><p>算法主要包括两个步骤（均基于Majority Vote思想）</p>
<ol>
<li>Leader election: 选举出全局的Leader</li>
<li>Log replication: 日志备份，每当Leader接收到Client的一个请求，Leader将日志成功的备份到多数Follower后，提交操作，向Client返回操作信息</li>
</ol>
<p>基本流程</p>
<ol>
<li>Server集群启动，开始选举，部分Follower Server转化为Candidate状态向其他的Server发送RequestVote RPC</li>
<li>每个收到RequestVote RPC的Server按照”first come first serve”的原则，给对应的Candidate投票，获得过半投票的Candidate成为Leader</li>
<li>Leader接受Client请求，写入log，并向其他Follower发送AppendEntries RPC备份log，保证所有server的state machine状态一致</li>
</ol>
<p>Raft将时间划分为了固定的 “term”，不同服务器通过term作为逻辑时钟实现同步和一致性的保持</p>
<img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Raft/image-20220602150043559.png" class="" title="image-20220602150043559">
<h3 id="Leader-election"><a href="#Leader-election" class="headerlink" title="Leader election"></a>Leader election</h3><p>要了解选举，首先要了解<strong>触发选举的条件</strong>，为了避免Leader崩溃导致的系统单点失效，Raft通过“heart beat”机制实现Leader状态的监控</p>
<ol>
<li>Leader周期性所有的Follower的发送heartbeart消息（不包含log的(AppendEntriesRPC）</li>
<li>每个Follower设定了一个 “election timeout” 超时时间，当Follower超过该时间为收到来自Leader的heartbeart消息时，该Follower认为Leader失效，发起选举。</li>
<li>服务器启动时初始化所有服务为Follower状态，并启动”election timeout”计时器</li>
</ol>
<p>所以Leader election的<strong>触发时机为某个Follower超时，转化为Candiate状态</strong></p>
<img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Raft/image-20220602153212870.png" class="" title="image-20220602153212870">
<h4 id="选举过程"><a href="#选举过程" class="headerlink" title="选举过程"></a>选举过程</h4><img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Raft/image-20220602153043100.png" class="" title="image-20220602153043100">
<p>Follower转化为Candidate后，开启新一轮选举，执行一下几个操作后，等待选举结果</p>
<ol>
<li>term增加1，代表进入下一个逻辑时间段</li>
<li>为自己投票</li>
<li>重置election timer</li>
<li>向其他Server发送RequestVote RPC，请求成为Leader</li>
</ol>
<p>其他Server收到请求后，按照”first come first serve”原则向Candidate投票</p>
<ol>
<li><p>若RequestVote RPC的term值小于当前term值，说明当前投票信息已过期，拒接投票</p>
</li>
<li><p>若未投过票，则未接收到的第一个 RV RPC投票，否则拒绝投票（”first come first serve”）</p>
</li>
<li><p><strong>election restriction</strong>：RequestVote RPC中的 more up-to-date，投票，否则拒绝</p>
<img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Raft/image-20220602201511973.png" class="" title="image-20220602201511973">
</li>
</ol>
<p>等待成为Leader的Candidate可能等到三种情况</p>
<ol>
<li>收到了过半的投票（包括自己）<strong>-&gt;</strong> 竞选成功，转换状态为Leader</li>
<li>收到了来自其他Server的term大小相同的AppendEntries RPC  <strong>-&gt;</strong> 其他Server竞选成功，转换状态为Follower</li>
<li>直到election timer超时也未收到上述两个情况响应 <strong>-&gt;</strong> 进入下一轮选举</li>
</ol>
<p>如果不断的重复出现3，也就是<strong>split votes</strong>问题</p>
<ul>
<li>多个candidate争抢，导致每个candidate均无法达到过半的票数</li>
<li>解决方法：Raft使用随机选举超时机制（<strong>randomized election timeouts</strong>），错开不同的Follower成为Candidate的时机</li>
</ul>
<p>一系列操作最终保证 <strong>Election Safety</strong> : 一个term内最多存在一个Leader</p>
<ul>
<li>一个term内可以不存在Leader</li>
<li>系统同时可以存在多个leader，但每个leader一定是不同的term内选出的</li>
</ul>
<h3 id="Log-replication"><a href="#Log-replication" class="headerlink" title="Log replication"></a>Log replication</h3><p>选举Leader的最终目的还是为了保证log的一致性，保证log的一致性是为了保证所有Server上state machine按照相同顺序执行相同操作以达到备份效果,Raft保证以下性质:</p>
<img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Raft/image-20220602155409902.png" class="" title="image-20220602155409902">
<p><strong>具体发送流程</strong>如下：</p>
<ol>
<li>Leader为每个Follower维护一个nextIndex，代表下一个AppendEntries RPC中要携带的log entry的index</li>
<li>AppendEntries RPC中不止携带当前要添加的log entry(nextIndex指向的entry),还包括上一个log entry的term和index</li>
<li>Follower判断当前的last entry是否与AppendEntries RPC中携带的上一个log entry的term和index（<strong>Consistent Check</strong>）<ul>
<li>一致 接受添加</li>
<li>不一致 拒绝添加</li>
</ul>
</li>
<li>Leader 根据 Follower的响应信息，进行下一步操作<ul>
<li>若Follower拒绝，将nextIndex - 1</li>
<li>若Follower接受，将nextIndex + 1</li>
</ul>
</li>
<li>Leader不断发送AppendEntries RPC， 直到Follower的nextIndex等于Leader的last log entry</li>
</ol>
<p><strong>commit时机</strong>：</p>
<img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Raft/image-20220602193022746.png" class="" title="image-20220602193022746">
<img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Raft/image-20220602161725209.png" class="" title="image-20220602161725209">
<ol>
<li><p>Leader收到超半数接受信息后即commit当前log entry</p>
</li>
<li><p>Follower根据AppendEntries RPC中携带的leaderCommit号，更新自身的commitIndex，若leaderCommit &gt; commitIndex,将commitIndex目标值</p>
<img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Raft/image-20220602191533241.png" class="" title="image-20220602191533241">
</li>
<li><p>当Follower检测到 lastApplied小于commitIndex,提交lastApplied的log,并自增lastApplied</p>
<img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Raft/image-20220602191623397.png" class="" title="image-20220602191623397">
</li>
</ol>
<p>特殊情况：<strong>Raft不会提交历史主动term的日志</strong></p>
<ul>
<li>非常难理解这么做的原因（<a href="https://www.zhihu.com/question/68287713">知乎回答</a> + 论文中的图8 可以促进理解）</li>
<li>我的理解：如果一个历史日志没有提交，却在当前Leader中携带，那么有两种可能的情况<ol>
<li>历史日志未备份到多数派，其任期内的Leader就崩溃<ul>
<li>该情况下，历史任期 $Leader_{history}$和当前$Leader_{current}$中间存在空窗期，由于历史日志未备份到多数派，空窗期内可能选举出一个<strong>不含历史日志的$Leader_{mid}$</strong>,，即Leader转化流程为 $Leader_{history}-&gt; Leader_{mid} -&gt;Leader_{current}$</li>
<li>$Leader_{current}$ 任期内未提交日志，$Leader_{mid}$ 就有可能再次当选为leader，此时若写入新日志，就会导致历史日志被覆盖</li>
<li>如果$Leader_{current}$ 提交了历史日志，日志覆盖就导致了<strong>提交丢失</strong></li>
<li>如果$Leader_{current}$ 提交了当前任期的日志，$Leader_{mid}$ 如果不携带历史日志肯定不携带当前任期日志（日志追加），不可能被选为leader，也就不会覆盖历史日志</li>
</ul>
</li>
<li>历史日志已经备份到多数派，其任期内的leader没来得及提交就崩溃的情况<ul>
<li>该情况之后选举出的所有leader都一定含有历史日志，也就不存在覆盖问题</li>
</ul>
</li>
</ol>
</li>
<li>若当前未提交log的term小于最新term，我先不能提交（图8中三个2），等当前Server成为Leader拿到最新term（term = 4），准备提交当时term的log时（图8中三个4），再提交当前log（图8中可能覆盖当前log的S5不可能被选上了，因为term = 3 小于当前 term = 4</li>
</ul>
<h3 id="Cluster-membership-changes-成员修改"><a href="#Cluster-membership-changes-成员修改" class="headerlink" title="Cluster membership changes 成员修改"></a>Cluster membership changes 成员修改</h3><p>Raft支持动态修改配置（例如：增加，删除Server）,为了避免在配置切换过程中出现脑裂问题，Raft通过两段式提交实现配置切换</p>
<ol>
<li>首先由Leader创建包括新旧配置log entry的特殊 AE RPC：$C_{old_new}$, 向所有的Follower发送，超过半数同意后提交，实现 <strong>joint concensus</strong></li>
<li>实现后<strong>joint concensus</strong>，Leader创建 新配置log entry：$C_{new}$，按照一般添加log entry的流程执行，成功commit后，完成配置切换</li>
</ol>
<img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Raft/image-20220603152218579.png" class="" title="image-20220603152218579">
<p>在配置切换时，可能选举出多个leader的原因来自于：</p>
<ul>
<li>$C_{new}$的部分传播导致 <strong>持有$C_{new}$的server</strong> 和 <strong>持有 $C_{old}$的server</strong> 对于 投票时的<strong>majority</strong> 判断标准不同，两个server子集合在一个term内选举出了两个Leader</li>
<li>例如下图：新配置为系统添加了两个新server(4和5)，Server1 未收到新配置，在竞选Leader时收到了 自己和Server 2的投票，即认为过半，转换为状态为Leader;Server 5使用新配置，收到了自己、Server3、Server4的投票，同样认为过半，转换状态为Leader</li>
</ul>
<img src="/2022/06/08/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Raft/image-20220603151523710.png" class="" title="image-20220603151523710">
<p>Raft引入了以下几个规则避免了上述问题的出现</p>
<ol>
<li>Log entry 在所有配置的Server($C_{old}$ 和$C_{old_new}$)上均进行备份（避免切换过程中，未收到$C_{old_new}$的Server遗失Log entry）</li>
<li>此时的Majority要满足$C_{new}$ 和  $C_{old}$两个配置中的Majority（单依靠$C_{new}$ 选不出Leader）、</li>
<li>Server收到新配置立即生效，无论配置日志是否commit</li>
</ol>
<p>导致的结果（要么老配置生效，要么兼容老配置和新配置生效，不会出现新配置单独生效的问题）</p>
<ol>
<li>要么选出的leader带有$C_{old}$，根据一致性约束，将带有$C_{old_new}$的Server回退，相当于未发生配置切换</li>
<li>要么选出的leader只带有$C_{old_new}$，实现了joint concensus</li>
</ol>
<h3 id="Snapshot"><a href="#Snapshot" class="headerlink" title="Snapshot"></a>Snapshot</h3><p>当系统运行时间后，难以避免的会出现log过多，导致占用大量存储空间、Server的重启状态时间过长等问题，Raft采用快照机制实现log 压缩</p>
<ol>
<li>每个Server单独对已commit的log entry进行快照（降低单一快照+网络传递的通信成本）</li>
<li>Leader定期向Follower发送自己的快照，Follower根据快照情况修改本地log和快照（<strong>InstallSnapshot RPC</strong>）<ul>
<li>基本思路就是：快照里有的log entry,Follower直接删除, 直到快照和log完美衔接</li>
</ul>
</li>
</ol>
<p>既然为了避免通信开销已经让每个Server自己备份了，为什么还需要第二步骤？</p>
<ul>
<li>为了避免Follower落后Server过多，Server已经将某一个log快照以后删除，导致Follower永远收不到来自Leader的这个log</li>
</ul>
<h3 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h3><p>客户端连接Server部分主要有三个逻辑：</p>
<ol>
<li><strong>客户端如何确定Leader是谁？</strong>随机选择Server，由Server返回Leader地址</li>
<li><strong>如何实现幂等？</strong><ul>
<li>Leader在Commit某操作后未返回结果直接崩溃，Client会尝试重发请求，这就需要系统支持幂等操作。</li>
<li>Raft在客户端为每个操作添加序列号，Leader维护每个Client已Commit的最新序列号，对已执行过的操作直接返回结果</li>
</ul>
</li>
<li><strong>对应Read-only  op，如何保证返回最新结果?</strong> 换句话说就是保证Raft的强一致性（Zookeeper中会详细讲什么是强一致性-<strong>Linearizability</strong>）<ul>
<li>Leader在每个term开始时，提交一个no-op entry，保证Leader得到所有的Commit Entry（由于Raft 永远不会通过计算副本数目的方式来提交之前任期内的日志条目，<strong>为了避免存在部分多数entry未提交的情况</strong>）</li>
<li>Leader每次响应只读操作前，首先通过心跳机制确定自己还是leader(以免已有另外的leader被选举，导致返回结果stale)</li>
</ul>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Raft说实话理解难度是比paxos降低了，但是细节和涉及到的分布式知识太多了，我目前只能说弄懂了部分细节，整体上理解还是欠缺，即做到了深入，但以我目前的水平还做不到浅出（<del>反正研究生不搞这个，能懂点面试吹吹nb即可</del>）</p>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>分布式理论</category>
      </categories>
      <tags>
        <tag>共识算法</tag>
        <tag>Raft</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark：分布式容错的内存计算框架</title>
    <url>/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Spark%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%B9%E9%94%99%E7%9A%84%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/</url>
    <content><![CDATA[<h2 id="Spark-分布式容错的内存计算框架"><a href="#Spark-分布式容错的内存计算框架" class="headerlink" title="Spark:分布式容错的内存计算框架"></a>Spark:分布式容错的内存计算框架</h2><blockquote>
<p><a href="https://www.usenix.org/conference/nsdi12/technical-sessions/presentation/zaharia">Resilient Distributed Datasets: A Fault-Tolerant Abstraction forIn-Memory Cluster Computing</a></p>
</blockquote>
<p>Spark是目前大数据领域较为火热的批处理框架之一（也支持流处理），经过论文阅读，不难发现Spark的核心原理并不复杂，通过简单的抽象，有效的解决了内存计算问题，我想这就是Spark流行起来的原因之一，如果要理解Spark，最关键一步的是理解RDD这一抽象内存模型。</p>
<h3 id="RDDs（Resilient-Distributed-Datasets）"><a href="#RDDs（Resilient-Distributed-Datasets）" class="headerlink" title="RDDs（Resilient Distributed Datasets）"></a>RDDs（Resilient Distributed Datasets）</h3><p>Spark中将计算操作的基本内存数据单元抽象为一个只读、分区的数据集-弹性分布数据集（RDD），将一次Spark任务定义为RDD经过一系列操作不断变换状态的过程，如下图所示：</p>
<ul>
<li>每一步操作将一个RDD转化为另一个逻辑上的RDD（例如map、filter、join等）</li>
<li>不同RDD之间通过操作链接起来，形成一个RDD转换父子关系链</li>
<li>Spark通过记录RDD转化的父子关系以及操作类型，实现容错（在RDD丢失后，根据计算链重新计算）</li>
</ul>
<img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Spark%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%B9%E9%94%99%E7%9A%84%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/image-20220726150407285.png" class="" title="image-20220726150407285">
<p>系统中通过五个元信息定义一个RDD：一系列分区、一系列父RDD依赖、转化操作、元数据（分区放置、分区信息），依赖关系如下图，分为一对一（narrow）和多对一（wide）。</p>
<img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Spark%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%B9%E9%94%99%E7%9A%84%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/image-20220726153152468.png" class="" title="image-20220726153152468">
<span id="more"></span>
<p>与其他分布式内存数据存储（kv数据库）的区别在于：</p>
<ul>
<li>RDD在状态变更逻辑上更加”粗“（<strong>Coarse-grained</strong>），执行的操作一般是对于RDD中所有元素执行的统一操作（如：过滤特定单词、计数）</li>
<li>如KV内存数据库等状态变更逻辑上粒度更加”细“（<strong>Fine-grained</strong>），执行操作一般是修改某一个表项或者某一个键值</li>
</ul>
<p>RDD的“粗”来自于批处理任务的特点，也为Spark带来了许多优势：</p>
<ul>
<li>批处理实际上就是一批数据执行某种操作到另一种批数据的过程，这与RDD变换的逻辑是一致的</li>
<li>RDD理论上不需要checkpoint机制进行容错，可以通过记录RDD变换操作，在特定RDD丢失后，直接重新计算获得丢失RDD</li>
<li>另外由于RDD的隔离性（计算并不修改RDD，而是生成RDD），可以通过复制RDD到其他机器解决“stragglers”问题</li>
</ul>
<p>在了解过RDD以后，心里不由自如的冒出这么个想法：在批处理环境下内存编程模型就应该是这样，简单且有效；然而想出从批处理-&gt;抽象模型这一步是真的太难。</p>
<h3 id="Spark实现"><a href="#Spark实现" class="headerlink" title="Spark实现"></a>Spark实现</h3><p>论文中简单介绍了基于RDD模型实现的Spark框架，省略了许多实现细节，且由于论文发表较早，其描述的实现机制肯定与目前Spark内部细节还是有较大差异，简单理解即可，基本框架图如下：</p>
<ul>
<li>对于每个Spark程序，采用一个Driver进程+多个Worker进程的实现方式。</li>
<li>Driver负责追踪RDD计算链（RDDs’ lineage）</li>
<li>Worker负责执行计算、在内存中存储RDD</li>
</ul>
<img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Spark%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%B9%E9%94%99%E7%9A%84%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/image-20220726152248126.png" class="" title="image-20220726152248126">
<p>RDD被实现为由元素类型决定的静态类型对象（如RDD[int]），支持的一系列transformation和action操作：</p>
<ul>
<li>transformation操作包括map、reduceByKey等，采用“懒执行”机制，即不真正执行，只是记录操作、定义新RDD</li>
<li>action操作包括count、collect等，直接执行，返回给用户结果</li>
</ul>
<p>Spark中任务的执行通过一个中心调度器（scheduler），进行任务分配和RDD分区</p>
<ol>
<li>当执行action类型操作时，scheduler构建DAG图，并划分stage</li>
<li>按照传输计算不传输数据的元组，尽量按照数据分布分配计算任务（例如具有依赖关系的父子RDD partition分布在一台机器上）</li>
</ol>
<img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Spark%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%B9%E9%94%99%E7%9A%84%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/image-20220726154038748.png" class="" title="image-20220726154038748">
<h3 id="Checkpointing-wide-dependency容错"><a href="#Checkpointing-wide-dependency容错" class="headerlink" title="Checkpointing-wide dependency容错"></a>Checkpointing-wide dependency容错</h3><p>当存在wide dependency的子RDD失效时（例如Page Rank中distinct() rdd结果丢失），由于其依赖于所有的上层RDD，所以需要重新在所有依赖的父RDD上重新执行相同操作，此类型错误恢复执行的成本过高。</p>
<p>Spark提供API<strong>支持Checkpointing机制</strong>持久化RDD,但是Spark将持久化的时机交给用户决定，由用户主动调用持久化API进行Checkpointing</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Spark实际上就是将maprudce模型迁移到内存中，降低了任务中间结果输入输出的IO成本，其适用于例如pagerank、逻辑回归等的需要进行迭代计算的任务（中间结果不重要），如果将类似任务在mapreduce做，每一次迭代相当于一个mapreduce任务。Spark使用的RDD模型同样存在以下局限：</p>
<ol>
<li>不合适处理数据状态局部更新的应用（web site interactions, 增量采集数据的网络爬虫）</li>
<li>不支持处理流式数据</li>
</ol>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>分布式理论</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spanner：“真”分布式数据库</title>
    <url>/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Spanner%EF%BC%9A%E2%80%9C%E7%9C%9F%E2%80%9D%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
    <content><![CDATA[<h2 id="Spanner：“真”分布式数据库"><a href="#Spanner：“真”分布式数据库" class="headerlink" title="Spanner：“真”分布式数据库"></a>Spanner：“真”分布式数据库</h2><blockquote>
<p><a href="https://dl.acm.org/doi/abs/10.1145/2491245">Spanner: Google’s Globally Distributed Database</a></p>
</blockquote>
<p>Spanner作为Google开发的全球分布式数据系统，具备外部一致性、支持分布式事务、多副本容错等特性，相较于Aurora，更具“分布式”特征。</p>
<h4 id="从何而来？"><a href="#从何而来？" class="headerlink" title="从何而来？"></a>从何而来？</h4><p>论文的intro介绍Spanner是从类似于MegaStore的基于BigTable的kv存储系统演化而来的，MegaStore虽然能够很好得解决大部分客户的数据存储需求，但还存在一部分问题</p>
<ol>
<li>对于关系模型的支持较差，难以支持复杂的、经常变化的模型</li>
<li>无法在使用大范围副本的同时保证强一致性，并且支持分布式事务</li>
</ol>
<p>针对以上问题，一个支持更强关系模式的多时间版本的数据库-Spanner诞生了。</p>
<span id="more"></span>
<h4 id="基本架构与存储设计"><a href="#基本架构与存储设计" class="headerlink" title="基本架构与存储设计"></a>基本架构与存储设计</h4><p>类似于Aurora,Spanner同样采用了多数据中心+多server的架构模式，一个Spanner部署称为一个universe，一个universe跨越不同的数据中心(Zone)，每个Zone内部有多个span server，其中架构中不同职责角色如下图</p>
<ul>
<li>universe maseter：管理所有的zone信息</li>
<li>palcament driver：负责跨zone的数据移动</li>
<li>zone master：管理当前Zone的所有span server</li>
<li>location proxy：通知客户端访问数据所在的spanserver位置</li>
<li>span server：存储并为客户端提供数据</li>
</ul>
<img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Spanner%EF%BC%9A%E2%80%9C%E7%9C%9F%E2%80%9D%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/image-20220719155057293.png" class="" title="image-20220719155057293">
<p>每个span server的内部组成如下图所示，自下而上进行分析：</p>
<ul>
<li><p>底层数据存储在Colossus（基于GFS改进的文件系统）上</p>
</li>
<li><p>底层数据以类似于bigtable中<strong>tablet</strong>的数据结构为单位进行存储</p>
<script type="math/tex; mode=display">(key:string, timestamp:int64) -> string</script></li>
<li><p>每个server存储100-1000个tablet，每个tablet实现一个基于Paxos的状态机，用来维持副本的一致性，一个tablet的所有副本组成一个<strong>Paxos Group</strong>（意味着同步单位为tablet）</p>
</li>
<li><p>每个span server维护一个<strong>lock table</strong>，维护基于两阶段锁实现的并发控制锁信息</p>
</li>
<li><p>对于每个Paxos Group中被选为leader的span server，在lock table上层同时会实现一个transaction manager，用来协调和管理分布式事务</p>
</li>
</ul>
<img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Spanner%EF%BC%9A%E2%80%9C%E7%9C%9F%E2%80%9D%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/image-20220719160204168.png" class="" title="image-20220719160204168">
<h4 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h4><p>Spanner的数据库模型来自于BigTable的数据模型，区别关系型数据模型，论文中称为Directory Table（<strong>directory-bucketed key-value mappings</strong>），一种类似于关系模型的kv存储模型</p>
<ul>
<li>Directory是一系列key的集合（bulk），是系统放置和同步备份的基本单位</li>
<li>每个table的列为key，行为key对应value(这点是关系模型的特征)，每个key必须定义name</li>
<li>每个表由主键+非主键定义，一张表可以理解为是<strong>主键-&gt;非主键的映射关系</strong>(这点是kv模型的特征)</li>
</ul>
<p>如下图所示，创建了Users和Albums表，其中Albums表是Users表的子表</p>
<ul>
<li>从图中可知，表项按照类似目录的形式进行<strong>交错存储</strong></li>
<li>交错存储考虑到了不同表之间关系，在分布式存储环境下访问具备<strong>locality</strong>性质（我理解的是通常有关系的表要一起访问，不如就交错存储在一起，论文中一部分出发点是为了magastore对于跨行事务支持较差的问题）</li>
</ul>
<img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Spanner%EF%BC%9A%E2%80%9C%E7%9C%9F%E2%80%9D%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/image-20220719163022272.png" class="" title="image-20220719163022272">
<h4 id="分布式事务实现"><a href="#分布式事务实现" class="headerlink" title="分布式事务实现"></a>分布式事务实现</h4><p>Spanner中最难理解的一部分内容就在这一点，通过TrueTime API+分布式锁+Paxios的方式实现了满足强一致性的分布式事务支持，其中设计细节和实现细节较多，理解不到位，仅仅整理一下理解到的内容。</p>
<p><strong>True Time API</strong></p>
<p>Spanner通过True Time API为分布式系统提供全局时间戳，在全局时间戳的基础上实现了分布式事务的线性一致性（linearizability）。其中True Time API提供如下图的几个接口：</p>
<ol>
<li>TT.now()：获取一个当前的时间范围，保证当前时间一定在范围内</li>
<li>TT.after(t)：判断时间t是否已经过去</li>
<li>TT.before(t)：判断时间t是否还未到来</li>
</ol>
<img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Spanner%EF%BC%9A%E2%80%9C%E7%9C%9F%E2%80%9D%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/image-20220720142153487.png" class="" title="image-20220720142153487">
<p><strong>分布式事务</strong></p>
<p>Spanner分布式事务基于两阶段提交的思想，并结合全局时间戳分配+锁实现分布式事务的并发控制，基本的阶段提交过程如下：</p>
<ol>
<li>首先由客户端选择一个Coordinator Group(主块)，将提交信息(以及选择的Coordinator Group)和写入内容发送给所有的Participant Group中的leader（简单描述，首先选一个主group，然后告诉所有group我要提交了，且选的主group是他）</li>
<li>所有的non-coordinator-participant leader首先获取写锁，并生成一个<strong>大于先前所有事务时间戳</strong>的<strong>prepare timestamp</strong>，写入到Paxios日志中后，通知coordinator-participant leader。coordinator-participant leader同时也要获得写锁，但是不生成prepare timestamp</li>
<li>由coordinator-participant leader根据接收到的prepare timestamp，确定一个commit timestamp后，将commit信息写入Paxios日志中<ul>
<li>该commit timestamp确保大于所有的准备时间戳且大于coordinator-participant leader收到客户端发送的提交信息时<strong>TT.now().latest</strong>时间（保证事务时间戳单增，且事务开始时间一定大于事务发起时间）</li>
</ul>
</li>
<li>coordinator-participant leader等待直到TT.after(commit timestamp)为true后，发送commit信息给所有的non-coordinator-participant leader，执行提交操作，并将提交信息记录到Paxios日志中，释放锁资源</li>
</ol>
<p>上述描述过于复杂，从两阶段提交的角度简单理解为：</p>
<ol>
<li>准备阶段：主leader联系所有的协作leader，准备提交事务，获取所有的准备时间戳</li>
<li>提交阶段：主leader确定提交时间戳后，等到时间过了确定的时间戳后，发送提交信息，完成提交</li>
</ol>
<p><strong>考虑时间戳的两阶段提交</strong></p>
<p>相较于普通的两阶段提交，Spanner的实现中考虑了大量的时间戳分配关系，通过时间戳的分配，保证<strong>性质</strong>：<strong>若事务T2开始于T1提交以后，则其提交时间戳必须小于T1提交时间戳</strong>，该性质由以下两个性质保证</p>
<ol>
<li>start：事务开始时间戳不会小于事务到达系统时调用<strong>TT.now().latest</strong>的时间（事务打上开始时间戳时，对应时间保证已经过去）</li>
<li>commit wait：事务的真正提交（或者说提交结果为客户可见）的时间为TT.after(事务结束时间戳)为true以后（事务真实提交时，保证提交时间戳对应的真实事件已经过去）</li>
</ol>
<ul>
<li>“已经过去”指的是TT.after(t)为True</li>
</ul>
<p>简单理解就是</p>
<ol>
<li>start性质保证：事物的提交时间戳不会早于开始时间戳</li>
<li>commit wait性质保证：后续发生事务的开始时间戳不可能小于用户已将看到提交事务的提交的时间戳</li>
</ol>
<p>通过以上两个执行保证了<strong>外部一致性</strong></p>
<p><strong>只读事务的一致性保证</strong></p>
<p>只读事务保证一致性实际上就是保证在某个时间点上的读读到了当前时间点的最新数据，在Spanner中这一点同样是通过时间戳实现，基本思路如下：</p>
<ol>
<li>为只读事务分配时间戳（快照读相当于自带时间戳的只读事务）：若只读事务只涉及一个Paxios Group，则直接由Group leader为其分配时间戳；涉及多个Paxios Group，直接将时间戳设置为最新即TT.now().latest</li>
<li>根据时间戳查找对应新旧程度的副本，保证读取副本的最新时间戳($t_{safe}$)大于等于当前时间戳（如果没有就需要阻塞更新副本）</li>
</ol>
<p>由于以上根据时间戳进行读的特性，RO事务不需要上锁（lock-free），这极大的降低了只读事务的处理速度，同时避免了只读事务影响其他写事务的进行</p>
<img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/Spanner%EF%BC%9A%E2%80%9C%E7%9C%9F%E2%80%9D%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/image-20220720155852284.png" class="" title="image-20220720155852284">
<h4 id="其他实现细节"><a href="#其他实现细节" class="headerlink" title="其他实现细节"></a>其他实现细节</h4><p><strong>选主过程中的时间戳分配问题</strong></p>
<p>每个Paxios leader带有一个超时时长为10s的lease，由于leader需要维护大量的状态信息（锁表、时间戳等），leader短时间交换的成本得不偿失，Spanner设计的Paxios leader机制在我看来更像是一个长期leader，可通过以下两种方式延长lease:</p>
<ol>
<li>每次执行写入事务时，延长lease</li>
<li>在lease快过期时，leader主动向其他server发送请求延长lease</li>
</ol>
<p>由于每个leader维护一个当前最大时间戳，在leader切换时Spanner保证新leader的时间戳与此时间戳重叠</p>
<ul>
<li>老Leader在退位之前，必须等待TT.after(最大时间戳)为True，即等到True Time Api不会生成小于等于老Leader维护时间戳的时候，老Leader再进行退位操作</li>
</ul>
<h3 id="Spanner-vs-Aurora-vs-Frangampani"><a href="#Spanner-vs-Aurora-vs-Frangampani" class="headerlink" title="Spanner vs Aurora vs Frangampani"></a>Spanner vs Aurora vs Frangampani</h3><p>Spanner最难理解一点是通过True Time API实现的外部一致性，在于Aurora对比思考过程中，发现两个系统均实现了强一致性保证，可Aurora并没有用到Spanner用到的True TIme API也没用到分布式事务涉及到的两阶段提交、分布式锁，这点让我想了很久才明白具体区别点在哪。</p>
<p>首先还是要明白Linearizability和 Serializability的区别与联系（已经看了多少次了，总是忘记）</p>
<ul>
<li>Linearizability（线性一致性）：分布式系统中的概念，强调的是能为分布式系统中的发生事务安排全局认可的一个合理的顺序，因为分布式系统并没有类似单机系统全局时钟等的决定发生在不同机器上的事务的顺序，对应分布式系统CAP中的C。</li>
<li>Serializability（可序列化）：单机系统中的概念，指的是一系列并发事务之间通过并发控制，使得并发的事务按照一定的顺序（这个顺序是随机的，却决于锁和事务实现机制），执行结果满足数据库约束。</li>
</ul>
<p>为什么分布式系统中一点要确定一个合理的执行顺序？分布式系统中如果没有全局顺序，不同机器副本上由于网络延迟、宕机等问题，会导致不同副本上操作执行顺序不一致，访问时可能看到不一致的结果（看到过期数据、看到错误数据等）。</p>
<p>通过上述定义，我们可以明白两个性质实际上是两个不同的问题，同样也对应着不同的解决方案</p>
<ol>
<li>Linearizability（线性一致性）：通过Spanner类似的全局时钟或者Aurora类似的自增日志号，实现分布式事务操作顺序的判断</li>
<li>Serializability（可序列化）：单机数据库往往通过锁机制实现并发控制，从而实现一定程度的可序列化</li>
</ol>
<p>当单机数据库迁移到分布式环境中时，需要解决上述两个问题</p>
<ol>
<li>spanner：采用了 全局时钟 + 分布式锁实现</li>
<li>Aurora：采用全局自增编号解决了Linearizability问题，但是并没有其他机制实现并发控制问题，这是为什么？</li>
<li>Frangapani：采用了分布式锁实现解决了并发控制问题，但是没有机制实现Linearizability的问题，这是为什么？</li>
</ol>
<p>原因就在于:</p>
<ol>
<li>Aurora相较于Spanner采用的“伪分布式”，写操作只通过一个server执行，通过log自增即解决了并发控制的问题；</li>
<li>Frangapani同样是“伪分布式”，虽然写可以分布于不同的server，但是底层存储抽象相当于单机磁盘存储，不涉及多副本同步，所以只控制并发即可</li>
<li>Spaner的写操作分布于不同的机器之上，是真“分布式”，即“写分布”+“存储分布”，所以即需要分布式控制也需要单机并发控制。</li>
</ol>
<h4 id="全局顺序-vs-并发控制"><a href="#全局顺序-vs-并发控制" class="headerlink" title="全局顺序 vs 并发控制"></a>全局顺序 vs 并发控制</h4><p>通过上面总结我们可以得出两种一致性的区别实际上就是全局顺序和并发控制的区别</p>
<ul>
<li>相同点：两者的相同点均是为了确定一系列事务的发生顺序</li>
<li>不同点：全局顺序是为了让分布式环境中不同副本均认可事务的发生顺序（认可的顺序不一定是真正的发生顺序，因为每个副本的时钟不可能同步），并发控制是为了让一系列有“同时发生”的事务按照一定的顺序执行</li>
</ul>
<p>其中如果不需要多个副本认可即不需要全局顺序（Frangapani），如果通过全局顺序能够间接实现并发控制（Aurora）即不需要采用类似锁的机制进行并发控制，这两个系统都通过部分<strong>“非分布式”</strong>降低了所谓的<strong>一致性实现</strong>难度</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>数据库知识对于目前阶段的我来说还是有点心有余而力不足，尽力而为。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a href="https://en.wikipedia.org/wiki/Serializability">wikipedia Serializability</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/57579023">知乎：Transaction management：可串行性（serializability）</a></li>
</ol>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>分布式理论</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>分布式数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>ZooKeeper论文总结</title>
    <url>/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/ZooKeeper%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h2 id="分布式消息中间件-ZooKeeper"><a href="#分布式消息中间件-ZooKeeper" class="headerlink" title="分布式消息中间件-ZooKeeper"></a>分布式消息中间件-ZooKeeper</h2><h3 id="线性一致性（linearizability）"><a href="#线性一致性（linearizability）" class="headerlink" title="线性一致性（linearizability）"></a>线性一致性（<strong>linearizability</strong>）</h3><p><strong>线性一致性</strong>是指分布式系统面对网络延迟和系统故障保证不同分布之间的数据一致性，使分布式系统在客户端看起来就像一个没有副本的单机系统。</p>
<ul>
<li><p>也成为 <strong>原子一致性（atomic consistency）</strong>，<strong>强一致性（strong consistency）</strong>，<strong>立即一致性（immediate consistency）</strong> 或 <strong>外部一致性（external consistency ）</strong>。</p>
</li>
<li><p>换一个角度：一旦新的值被写入或读取，所有后续的读都会看到写入的值，直到它被再次覆盖</p>
</li>
</ul>
<span id="more"></span>
<p>线性一致性例子如下：</p>
<ul>
<li>其中Client B最后一个读取请求读到了2，由于Client A在其时间点前的一个读读到了最新的写入4，所以Client B的读取结果违背了线性一致性</li>
<li>通过记录所有请求和响应的时序，并检查它们是否可以排列成有效的顺序，通过该方式判断系统是否满足线性一致性</li>
</ul>
<img src="/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/ZooKeeper%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220625103136312.png" class="" title="image-20220625103136312">
<h4 id="可串行化（Serializability）-VS-线性一致性（Linearizability）"><a href="#可串行化（Serializability）-VS-线性一致性（Linearizability）" class="headerlink" title="可串行化（Serializability） VS 线性一致性（Linearizability）"></a>可串行化（Serializability） VS 线性一致性（Linearizability）</h4><p>两者衡量的问题：</p>
<ol>
<li>可串行化针对的是多个并发数据库事务，这些<strong>并发</strong>事务之间<strong>没有特定的顺序</strong>。可串行化指的是可以找到<strong>任意一个可行事务执行顺序</strong>，使得数据库的状态改变<strong>符合实际情况</strong>。（ <strong>multi-operation, multi-object, arbitrary total order</strong>）</li>
<li>线性一致性针对的是多个具有严格时序的事务，事务的发生顺序由全局时钟（也可能是逻辑时钟）衡量。线性一致性指的是按照事务之间的时序，判断按照该时序，系统状态变化是否出现冲突（<strong>single-operation, single-object, real-time order</strong>）</li>
</ol>
<p>另外两者应用领域不同：</p>
<ol>
<li>可串行化对应数据库ACID属性中的 <strong>“I”</strong>，即isolation 数据库的隔离性</li>
<li>线性一致性对应分布式系统CAP理论中的<strong>“C”</strong>，即Cosistency 分布式系统的一致性</li>
</ol>
<p>可串行化 + 线性一致性 = 数据库的强一致性</p>
<ol>
<li>数据库中基于两阶段锁定的一致性保证同时满足 可串行化 和 线性一致性</li>
<li>可串行化的快照隔离（数据库的弱一致性，不太理解） 无法保证 线性一致性</li>
</ol>
<p>我的简单理解:</p>
<ol>
<li>可串行化就是一个单机系统的概念，单机系统中在执行顺序确定的情况下结果必然是确定的，只有可能在并发条件下才可能出现不一致的问题，可串行化就是说寻求一个顺序使得并发请求结果保持一致。</li>
<li>线性一致性是分布式系统中的概念，由于不同备份之间存在版本差距，即使是顺序确定的一系列请求（访问了不同服务器上不同版本的副本）也可能出现，后发出请求反而读到了旧值的情况，线性一致性是寻求一个顺序使得分布式系统中具有确定顺序的请求出现结果冲突的问题（在分布式全局逻辑时钟衡量下也可能出现并发情况，此时与串行化一致）</li>
</ol>
<h3 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h3><blockquote>
<p>ZooKeeper is essentially a <a href="https://en.wikipedia.org/wiki/Service_(systems_architecture">service</a>) for <a href="https://en.wikipedia.org/wiki/Distributed_computing">distributed systems</a> offering a <a href="https://en.wikipedia.org/wiki/Hierarchical_database_model">hierarchical</a> <a href="https://en.wikipedia.org/wiki/Key-value_database">key-value store</a>, which is used to provide a distributed <a href="https://en.wikipedia.org/wiki/Configuration_management">configuration service</a>, <a href="https://en.wikipedia.org/wiki/Synchronization_(computer_science">synchronization service</a>), and <a href="https://en.wikipedia.org/wiki/Directory_service">naming registry</a> for large distributed systems (see <em><a href="https://en.wikipedia.org/wiki/Apache_ZooKeeper#Use_cases">Use cases</a></em>).<a href="https://en.wikipedia.org/wiki/Apache_ZooKeeper#cite_note-3">[3]</a> ZooKeeper was a sub-project of <a href="https://en.wikipedia.org/wiki/Hadoop">Hadoop</a> but is now a <a href="https://en.wikipedia.org/wiki/Apache_Software_Foundation#Projects">top-level Apache project</a> in its own right.</p>
</blockquote>
<p>ZooKeeper是一个分布式应用程序协调服务(coordination service)，为分布式应用提供配置管理、成员管理以及分布式锁等服务（<del>人如其名</del>），其底层基于Fast Paxos一致性算法，下面通过论文对ZooKeeper原理进行了解和研究。</p>
<h4 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h4><p>ZooKeeper使用znode作为元数据存储节点，其中znode以类似于unix文件系统的树型组织（如下图），每个znode拥有自己的命名空间</p>
<ul>
<li>znode不存储实际的数据，而是存储协调分布式应用的相关元数据（如配置）或仅仅通过znode实现协调功能（分布式锁）</li>
<li>client通过Zookeerper提供的API<strong>修改znode元数据或者添加/删除znode</strong> 实现分布式协调原语</li>
</ul>
<img src="/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/ZooKeeper%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220701141843134.png" class="" title="image-20220701141843134">
<p>znode分为以下不同类型：</p>
<ol>
<li>Regular：普通节点。由client主动创建和删除</li>
<li>Ephemeral：临时节点。同样由client主动创建和删除，但是当创建该节点的client的session结束时，节点自动删除（容错）</li>
</ol>
<p>另外创建znode时，可显式指定znode带有序列号(<strong>sequential flag</strong>)</p>
<ul>
<li>该机制保证新创建的znode的sequential flag在父节点所有儿子节点中最大</li>
<li>sequential flag可以方便的实现读写锁、无<em>羊群效应</em>（herd effect）的排他锁</li>
</ul>
<p>zookeeper提供了znode的<strong>watch</strong>机制，允许client主动监视znode的状态变更（异步）</p>
<ul>
<li><p>client 可对指定znode进行watch标记，当该znode状态修改时（删除、修改配置等），zookeeper发送消息提醒client</p>
</li>
<li><p>watch配合其他机制可方便实现一系列分布式写作原语（分布式锁）</p>
</li>
<li><p>我认为 <strong>watch机制</strong>也是ZooKeeper把自己称为 <strong>wait free</strong>的原因之一</p>
</li>
</ul>
<p>zookeeper将与client的交互定义为<strong>session</strong></p>
<ul>
<li>在session内，当ZooKeeper超过一定时间（session timeout）未收到client的请求，ZooKeeper会主动结束session</li>
<li>client也可主动关闭session，否则通过发送heartbeat请求维持session不会timeout</li>
</ul>
<h4 id="ZooKeeper基本操作"><a href="#ZooKeeper基本操作" class="headerlink" title="ZooKeeper基本操作"></a>ZooKeeper基本操作</h4><p>ZooKeeper本身采用主从复制的形式（如下图），实现高可用分布式协调服务，其中ZooKeeper主要包括两种操作</p>
<ol>
<li>写请求：接收到请求的follower服务器转发给leader，由leader运行共识算法得到共识后，提交当前写请求，并最终会在所有副本上写入</li>
<li>读请求：接收到请求的server直接读取本地数据，返回读取结果</li>
</ol>
<img src="/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/ZooKeeper%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220701150857515.png" class="" title="image-20220701150857515">
<p>不难看出ZooKeeper处理读请求的方法可能会导致client读到stale数据，ZooKeeper设计中接受这种程度一致性，其基本保证为</p>
<ol>
<li><strong>Linearizable writes</strong>(线性一致写)：明显写请求通过共识算法后才提交，能够保证在所有副本上实现写的线性一致性</li>
<li><strong>FIFO client order</strong>：所有来自同一个客户端请求保证按照请求发送顺序执行（client视角的线性一致性：即client不会出现先读到新数据，后读到旧数据的情况，每次读都保证至少和上次读取新旧程度相同的数据）</li>
</ol>
<p>由性质2可以得到，ZooKeeper的设计目标并没有<strong>读写线性一致性</strong>，仅仅保证单客户端的线性一致性，然而在处理读请求时仅仅时简单的读取本地数据无法保证性质二</p>
<ul>
<li>例如：client从一个最新的server上读取数据，该服务器宕机，client被分配到另一个数据不是最新的server,此时新的读请求会读到stale数据</li>
<li>针对此Zookeeper为处理的每个请求打上当前服务器已提交的写请求的<strong>zxid</strong>(<strong>zxid用来标记写请求的顺序</strong>),当client与新server建立session时，server会在确认自己至少拥有client的zxid相同新鲜程度数据的情况下，才与client建立session，否则等待直到数据更新到至少相同新鲜程度（<strong>as recent as the client</strong>）</li>
<li>由于共识算法的作用，大多数server必定带有最新更新，这就<strong>保证了client最终一定会找到可以建立连接的server</strong></li>
</ul>
<p><strong>Atomic Broadcast-Zab</strong></p>
<p><strong>Zab-ZooKeeper Atomic Broadcast</strong>，论文中只是引用了一下，这里简单介绍一下原理：</p>
<ol>
<li>正常状态下与fast paxios区别不大，leader收到一个事务（transaction）,为事务带上<strong>序列号</strong>和<strong>当前leader的epoch号</strong>（ 64位zxid,32epoch,32序列号），之后发送到所有的follower,多数通过即提交</li>
<li>leader和follower通过heartbeat信息交互，当一方超过一定时间未收到另一方的heartbeat，即认为另一方失效，当follower认为leader失效时，其进入错误恢复阶段</li>
<li>恢复模式（选主）：和raft类似，follower向其他follower发送选举请求，投票超过半数，成为新的leader（类似于gossip，follower之间不断pk，直到出现一个过半选票）</li>
<li>同步阶段：leader将最新zxid发送给follower,follower根据zxid与leader保持状态同步</li>
</ol>
<p>zab基于TCP实现leader到follower请求的fifo管道</p>
<p><strong>snapshot</strong></p>
<p>ZooKeeper使用一种 fuzzy snapshot的方式进行系统快照</p>
<ol>
<li>在进行快照的过程中，服务器继续响应请求</li>
<li>得到的快照可能处于中间状态，所以叫<strong>fuzzy snapshot</strong></li>
</ol>
<p>在启用快照时为了避免fuzzy snapshot导致的状态不正确问题，ZooKeeper通过Zab重传操作（ Zookeeper状态变更是幂等，所以即使重复执行也保证状态正常变更），保证最终状态与系统崩溃前一致</p>
<h4 id="Client-API"><a href="#Client-API" class="headerlink" title="Client API"></a>Client API</h4><p>ZooKeeper为Client提供了操作znode基本API，类似于文件系统的增删改查操作，每个操作均包括同步和非同步版本</p>
<ol>
<li>create(path, data, flags)：创建znode，其中flag用来指定znode类型：rugular,ephemeral,sequential</li>
<li>delete(path, version)：删除znode</li>
<li>exists(path, watch): 判断znode是否存在</li>
<li>getData(path, watch): 获得znode中存储的元数据</li>
<li>setData(path, data, version)：修改znode中元数据</li>
<li>getChildren(path, watch)： 获得当前路径的所有子znode</li>
<li>sync(path)：主动同步命令（path无关）</li>
</ol>
<p>通过以上API，Client可以方便的实现多种分布不是协调原语</p>
<ol>
<li><p>分布式互斥锁：基本原理是通过一个znode或者多个同父节点的znode实现锁</p>
<img src="/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/ZooKeeper%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220702090302676.png" class="" title="image-20220702090302676">
</li>
<li><p>Double Barrier：我理解的是双重同步，实现多个进程的同步。基于特定路径子znode的增删。</p>
</li>
<li><p>配置管理/成员管理：znode代表逻辑上的节点，通过znode修改删除等实现管理功能</p>
</li>
</ol>
<h4 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h4><p>ZooKeeper已经成功的应用于分布式用服务，论文中举了几个例子</p>
<ol>
<li>The Fetching Service：Yahoo的搜索引擎通过该服务实现数十亿级的网页爬取，该服务器有master和fecher组成，其中master管理fetcher的配置信息，fetcher进行网页的爬取并向master汇报自身状态信息。<ul>
<li>使用ZooKeeper目标：master容错，master配置职责解耦</li>
<li>用到的原语：配置管理、主节点选举</li>
</ul>
</li>
<li>Katta：分布式索引器，Katta将检索过程分布在不同的服务器上<ul>
<li>使用ZooKeeper目标：master容错，master配置职责解耦，成员容错</li>
<li>用到的原语：配置管理、主节点选举、成员管理</li>
</ul>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>ZooKeeper通过特殊的文件系统+zab算法实现了一个可定制化的分布式协作服务，在我看来ZooKeeper最大的优点在于屏蔽了底层共识算法的复杂细节，为分布式协作提供了简单、通用而又高度可定制化的实现接口。</p>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>分布式理论</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>大数据组件</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式事务总结</title>
    <url>/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h2 id="分布式事务总结"><a href="#分布式事务总结" class="headerlink" title="分布式事务总结"></a>分布式事务总结</h2><p>要理解分布式事务首先要明白事务是什么，从单机事务的理解迁移到分布式事务，下面从两个角度总结单机和分布式事务</p>
<ol>
<li>单机事务 vs 分布式事务</li>
<li>如何通过并发控制保证单机分布式事务的隔离性</li>
</ol>
<h3 id="单机-分布式事务"><a href="#单机-分布式事务" class="headerlink" title="单机/分布式事务"></a>单机/分布式事务</h3><p>事务作为数据库系统读写操作的高层抽象，代表了数据库的基本操作。一个正常提供服务的数据库系统，其事务必须满足ACID四个性质，其中CI两个性质相互关联，是事务性质研究的重点。</p>
<ol>
<li>Atomic（原子性）：每个事务被看作一个不可分割的单元，要么完全成功要么完全失败，不存在两者的中间状态。数据库系统必须保证任意时刻下事务的原子性</li>
<li>Consistency（一致性）：一致性是指数据库满足某种预先定义的约束，任何数据库的操作都必须满足一致性，即从一个满足一致性的状态转移到另一个满足一致性的状态（例如：转账事务要满足转出和转入账户总金额不变）</li>
<li>Isolation（隔离性）：一系列并发进程的执行导致数据库的状态改变和按照某种线性顺序执行的状态改变相同（实际上这是serializability的定义）</li>
<li>Durable（持久性）：事务一旦提交，其对数据状态的改变不会因为意外事件的发生而丢失</li>
</ol>
<p>分布式事务可以看作事务+分布式环境，即一个执行范围跨越多个通过网络连接的不同主机的事务，分布式事务既然是事务，同样要满足ACID性质，但是由于分布式环境的复杂性，原有的事务性质保证手段在分布式环境下需要进行调整和新的设计。</p>
<span id="more"></span>
<h3 id="ACID实现"><a href="#ACID实现" class="headerlink" title="ACID实现"></a>ACID实现</h3><p>Atomic（原子性）和 Durable（持久性）两个性质逻辑上较为相似，即数据库保证事务要么完全不提交，要么完全提交后永不丢失，两条性质通常通过“WAL（Write-ahead-log）” 机制实现</p>
<ul>
<li>WAL（Write-ahead-log）：在事务操作真正影响数据之前，首先将操作写入日志中，日志持久化以后才在合适的时机进行修改操作的实际执行，在事务回退（保证A）或者恢复事务结果（保证D）时，通过日志中记录的操作恢复到对应状态</li>
<li>在单机和分布式事务中，均采用WAL机制+复制的方式保证以上性质<ol>
<li>MySQL：通过redo、undo log实现了WAL的机制</li>
<li>Amazon Aurora：分布式环境中基于redo log的WAL机制</li>
<li>Spanner：事务中所有的操作执行前，首先写入Paxios日志（基于Paxios的分布式WAL）</li>
</ol>
</li>
</ul>
<p>Consistency（一致性）和 Isolation（隔离性）在逻辑上具有关联关系，除去静态的一致性保证外（如外键约束），一致性的破坏往往是由于多个事务并发执行未保证“隔离性”，导致的前后状态不一致性，通过保证“隔离性”，可以间接的保证数据库的一致性，“隔离性”的保证涉及到并发事务的控制，即并发控制。</p>
<h3 id="一致性and隔离性"><a href="#一致性and隔离性" class="headerlink" title="一致性and隔离性"></a>一致性and隔离性</h3><p>以来自于wikipedia的转账事务为例，阐述当事务并发执行时，由于”隔离性“的控制程度的不同，带来的几个不同程度”不一致性“</p>
<ul>
<li>数据库存储两个年龄分别为20、25的用户记录  <img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%80%BB%E7%BB%93/image-20220730105034565.png" class="" title="image-20220730105034565.png">
</li>
</ul>
<ol>
<li><p>脏读（dirty read）</p>
<p>脏读指一个事务能够读到另一个未提交并发执行事务的执行结果，如下图所示。</p>
<ul>
<li>事务1查询Id=1用户的年龄，同时事务2将用户1年龄修改为21</li>
<li><p>事务1在事务2提交之前查询到其修改结果21，之后事务2回滚，用户1年龄回滚为20</p>
<img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%80%BB%E7%BB%93/image-20220730105404765.png" class="" title="image-20220730105404765">
</li>
</ul>
</li>
<li><p>不可重复读（Non-repeatable reads)</p>
<p>不可重复读指一个事务连续两次读取一个对象时，前后状态不一致，即对象值被其他事务修改，如下图所示。</p>
 <img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%80%BB%E7%BB%93/image-20220730110701020.png" class="" title="image-20220730110701020">
</li>
<li><p>幻读（Phantom reads）</p>
<p>幻读与重复读类似，只是不再是某个对象的状态前后不一致，而是数据库表多新数据/少老数据，如下图所示</p>
<ul>
<li>事务1两次查询年龄区间之间，事务2插入了一个表项<img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%80%BB%E7%BB%93/image-20220730111152975.png" class="" title="image-20220730111152975">
</li>
</ul>
</li>
</ol>
<p>针对以上的并发一致性问题，定义了不同程度的隔离性</p>
<ol>
<li><p>可串行化 Serializable</p>
<p>最强的隔离级别，保证并发事务按照特定顺序逐个执行，能够避免所有以上的问题，但是实现可串行化性质，会大幅降低数据库性能</p>
</li>
<li><p>可重复读 Repeatable reads</p>
<p>可重复读，解决脏读、不可重复读问题的隔离级别，但是会出现幻读</p>
</li>
<li><p>读已提交 Read committed</p>
<p>保证事务结果只有在提交以后才能对其他事务可见，能够解决脏读问题</p>
</li>
<li><p>读未提交 Read uncommitted</p>
<p>可以读取未提交的事务的执行结果，最低的隔离级别，无法解决脏读问题</p>
</li>
</ol>
<h3 id="并发控制（concurrency-control）"><a href="#并发控制（concurrency-control）" class="headerlink" title="并发控制（concurrency control）"></a>并发控制（concurrency control）</h3><p>上面简单介绍了Isolation的性质特点，数据库要保证上述性质必须通过并发控制实现，并发控制本质上就是对多个访问相同资源的操作/事务等进行协调，并发控制主要有以下三种策略：</p>
<ol>
<li><strong>optimistic</strong>（乐观）：允许多个并发操作同时操作数据库，当用户想要修改数据时，由数据库判断是否满足一致性和隔离性要求，若不满足则拒绝执行操作（不阻塞，在发生冲突时撤销操作）</li>
<li>pessimistic（悲观）：阻塞所有相同对象的操作（即使不会发生冲突），同一时间一个对象上只有一个事务执行</li>
<li><strong>Semi-optimistic</strong>（半乐观）：结合上述两种思想</li>
<li>乐观锁理论上阻塞更少，并发性能更高，实际上由于冲突回滚，在部分情况（写多读少？）性能不如悲观机制。也就是两种机制都有自己适应的场景</li>
</ol>
<p>主要的手段</p>
<ol>
<li>锁（locking）：为资源设置锁，通过锁控制并发操作<ul>
<li>simple locking：每一个事务需要为每一个共享变量在执行读或写之前请求锁，只有在事务完成或者终止时，才释放锁。</li>
<li>Two-Phase Locking（两阶段锁）：事务可以在执行的过程中不断申请锁，但是一旦释放锁就不能再申请锁<ul>
<li>Growing Phase：不断地根据数据，申请写锁和读锁，shrinking Phase：不断地释放锁，直到事务结束</li>
</ul>
</li>
</ul>
</li>
<li>Serialization graph checking：检验串行化图中是否存在环</li>
<li>Timestamp ordering：通过时间戳对事务执行顺序排序（分布式系统中常用来保证一致性）</li>
<li>Commitment ordering: 为Commit排序</li>
<li>Multiversion concurrency control] (MVCC) ：基于版本快照的并发隔离控制</li>
</ol>
<p>通过以上并发控制策略，保证了数据库的CI性质，如果我们要将对应问题迁移到分布式环境上问题有何不同？解决方案又有何不同？</p>
<h4 id="来到分布式事务（主要介绍两阶段提交）"><a href="#来到分布式事务（主要介绍两阶段提交）" class="headerlink" title="来到分布式事务（主要介绍两阶段提交）"></a>来到分布式事务（主要介绍两阶段提交）</h4><p>从单机到分布式事务，我们首先要找到这个改变带来了哪些新问题？</p>
<ol>
<li>计算节点分布于不同的服务器，单个事务的提交涉及到不同服务器的”子事务“的分别提交</li>
<li>多节点、节点直接通过网络通信等导致系统可能出现更多类型的错误（消息丢失、网络延迟、节点崩溃等等）</li>
</ol>
<p>既然问题发生了改变，单机并发控制的方法并不能简单的迁移到分布式环境中，提出了新的思路-<strong>Two-Phase Commit</strong></p>
<ul>
<li>引入协调者（Coorrdinator），管理事务涉及到的不同子服务器”子事务“提交，由协调者与所有参与事务服务通信判断是否满足事务提交条件</li>
<li>事务的提交过程如下所示，分为两个阶段<ol>
<li>准备阶段：协调者向所有的参与者发送准备消息，若worker可提交本地”子事务“，返回ok，否则返回not ok。协调者只有在收到所有参与者ok信息后才认为事务可以提交，进入第二阶段。（参与者在此阶段准备如锁等的提交事务的资源）</li>
<li>提交阶段：协调者向所有参与者发送提交消息，参与者收到消息后，提交事务，返回成功消息。</li>
</ol>
</li>
<li>两阶段提交存在<strong>单点失效、同步阻塞</strong>等问题</li>
</ul>
<img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%80%BB%E7%BB%93/image-20220715162741486.png" class="" title="image-20220715162741486">
<p>2PC故障恢复问题：</p>
<ol>
<li>协调者故障：若协调者故障不会影响正确性，提交的事务不会丢失，未提交的事务不会提交。</li>
<li>参与者/网络故障：若参与者在准备阶段发送ok之前实现或者ok信息丢失，则协调者需要设置超时机制，超时放弃事务；若参与者在发送ok之后失效，若此时协调者收到了所有ok信息，发送了commit信息，协调者必须等待失效参与者上线、失效参与者也必须能够继续执行提交，否则就会出现部分提交的问题</li>
</ol>
<p>其他分布式事务实现机制以后有机会再继续进行深入研究</p>
<h3 id="分布式中的一致性"><a href="#分布式中的一致性" class="headerlink" title="分布式中的一致性"></a>分布式中的一致性</h3><p>分布式中的一致性与数据库事务中的概念不同，主要考虑的是多副本之间状态的一致性，其实概念上更加接近ACID中的I，按照一致性从强到弱分为以下集中不同的一致性模型</p>
<ol>
<li>线性一致性（Linearizability）：又被称为Strong consistency or Atomic consistency，所有事务全局有序，使得所有的”读“都能读到最近的”写“，使得分布式系统外部看起来就像一个单机系统。（前提是需要建立一个全局时钟或者顺序确定机制）</li>
<li>顺序一致性（Sequential consistency）：放宽了线性一致性的全局性顺序要求，将一致性的要求分为以下两个方面，<strong>放宽了不同机器之间操作的执行顺序先后</strong>（zookeeper）<ul>
<li>从单机角度看，其读写操作的执行严格按照发出的顺序</li>
<li>从全局角度看，全局只有一个执行顺序</li>
</ul>
</li>
<li>因果一致性（<strong>Causal consistency</strong>）：对顺序一致性的进一步放宽，只要求保证具有因果关系操作的先后顺序关系（全局认可），其他不具备因果关系的操作的先后顺序在不同节点实际执行顺序可能不同<ul>
<li>通过本地执行顺序先后、写后的读等确定因果关系</li>
</ul>
</li>
<li>最终一致性（<strong>Eventually Consistency</strong>）：从出现网络分区时，分布式系统无法同时保证可用性和一致性，因此定义最终一致性这一概念，在执行某个操作后系统的状态可以是不一致的，但是能够保证的是过一段时间系统的状态最终会达到一致性<ul>
<li>由于最终一致性的系统不同部分会出现状态不一致的情况，需要特定机制进行状态同步</li>
<li>在不同副本之间通信交换更新和状态更变，得到一个一致的最终状态</li>
</ul>
</li>
</ol>
<p>分布式系统中往往通过<strong>共识算法+ 并发控制</strong>实现一致性的保证</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单的整理了一下目前对于单机事务和分布式事务(<strong>单机并发，分布式并行</strong>)的理解，分布事务概念本身其实与单机事务相差不大，只是分布式事务的实现机制更加的复杂，仅仅了解了两阶段提交，还有其他许多实现方法等待继续学习</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a href="https://zhuanlan.zhihu.com/p/57315959">知乎：分布式系统一致性 - 总结</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/57579023">知乎：Transaction management：可串行性（serializability）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/504106272">知乎：分布式事务综述</a></li>
<li><a href="https://en.wikipedia.org/wiki/Distributed_transaction">wikipedia:Distributed transaction</a></li>
<li>DDIA第七章-事务</li>
</ol>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>分布式理论</category>
      </categories>
      <tags>
        <tag>分布式事务</tag>
      </tags>
  </entry>
  <entry>
    <title>FaRM:无妥协的强一致性分布式数据库</title>
    <url>/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/%E6%97%A0%E5%A6%A5%E5%8D%8F%E7%9A%84%E5%BC%BA%E4%B8%80%E8%87%B4%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
    <content><![CDATA[<h2 id="FaRM-无妥协的强一致性分布式数据库"><a href="#FaRM-无妥协的强一致性分布式数据库" class="headerlink" title="FaRM: 无妥协的强一致性分布式数据库"></a>FaRM: 无妥协的强一致性分布式数据库</h2><blockquote>
<p><a href="https://dl.acm.org/doi/10.1145/2815400.2815425">No compromises: distributed transactions with consistency, availability, and performance</a></p>
</blockquote>
<p>从论文的名字就能看出微软对于自己这套分布式数据库系统的自信程度，强调自己在实现强一致性的同时，并不会妥协可用性和性能，然而深入论文就会发现，虽然确实实现了题目中的性质，但是很多机制局限于其实现条件，另外由于时间和能力有限，<strong>未能看懂论文中对于错误恢复部分的阐述</strong>，未来涉及到相关内容时再重新学习。</p>
<h3 id="构建系统动机"><a href="#构建系统动机" class="headerlink" title="构建系统动机"></a>构建系统动机</h3><p>FaRM的构建动机来自于目前数据中心内部硬件发展的两个趋势：</p>
<ol>
<li>基于RDMA（Remote Direct Memory Access）的快速网络通信技术：server之间进行通信时，不经过CPU，直接将数据写入到目标server存储区/拉取到本地存储区</li>
<li>低成本非易失DRAM存储技术：直接将数据存储在DRAM中，通过单独供电，在断电时备份到SSD中实现非易失</li>
</ol>
<p>在我看来，FaRM的高性能大部分原因来自于上部分阐述的硬件优势，除去优势以外，以上设计同样为FaRM带来了一定限制</p>
<ul>
<li>无法实现跨数据中心分布</li>
<li>增加了分布式事务和消息传递实现的复杂性（<del>这难道就是我看不懂错误恢复的原因？</del>）</li>
</ul>
<span id="more"></span>
<h3 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h3><p>如下图所示，FaRM架构基于1管理+N存储的方式，由一个配置管理服务器+多个存储服务器实现：</p>
<ul>
<li>Configuration Manager（CM）：负责分布式事务处理、容错恢复、配置管理等服务</li>
<li>普通存储服务器：存储数据+事务日志</li>
<li>ZooKeeper：仅仅作为辅助CM实现配置管理的协调服务</li>
</ul>
<img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/%E6%97%A0%E5%A6%A5%E5%8D%8F%E7%9A%84%E5%BC%BA%E4%B8%80%E8%87%B4%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/image-20220723160335830.png" class="" title="image-20220723160335830">
<p>其中数据以Region为单位在不同存储服务服务上分布和管理</p>
<ul>
<li>每个Region为2GB大小的存储块，存储一个primary和f个backup</li>
<li>由CM负责Region的开辟、管理Region分布和映射</li>
</ul>
<p>事务日志（Tx log）存储在基于环状缓冲实现的FIFO队列中，其中Region和Tx log的备份容错基于主从复制机制，一个primary+多个backup server，当primary server失效后，选取新的back up成为新的primary server</p>
<h4 id="分布式事务的实现"><a href="#分布式事务的实现" class="headerlink" title="分布式事务的实现"></a>分布式事务的实现</h4><p>与Spanner从Participant Group中选取一个作为协作者这种纯分布式事务实现方式不同，FaRM通过CM集中管理分布式事务</p>
<ul>
<li>由于FaRM的消息传递不经过CPU，FaRM分布式事务无法采用两阶段提交<strong>？</strong>。采用了如下图所示的更多阶段、复杂的提交方式。</li>
</ul>
<img src="/2022/07/31/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/%E6%97%A0%E5%A6%A5%E5%8D%8F%E7%9A%84%E5%BC%BA%E4%B8%80%E8%87%B4%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/image-20220723170854197.png" class="" title="image-20220723170854197">
<p>FaRM的事务提交分为五个阶段：</p>
<ul>
<li>执行阶段：首先在CM执行事务，并将事务执行结果缓存在CM本地（乐观并发控制）</li>
</ul>
<ol>
<li>Lock(上锁阶段)：CM将事务所需要的锁记录写入到参与事务的primary服务器上（无法直接响应，不经过CPU），primary在处理记录时，尝试根据记录版本号获取对应锁，如果版本号变更或者锁已被获取，返回失败，CM终止事务；否则在CM收到所有成功信息后，进入下一阶段（通过version+锁实现的<strong>乐观并发控制</strong>）、</li>
<li>Validate(验证阶段)：CM在通过one-sided RDMA读取验证所有涉及到的对象的版本号，若版本号出现不一致，则终止事务，否则进入下一阶段（<strong>读不需要锁</strong>，所以会有这一步，因为<strong>读之后事务执行完成之前可能会有其他事务获取锁</strong>）</li>
<li>Commit Backup(备份服务器上提交)：CM向所有的backup服务器发送（one-sided RDMA）Commit backup 信息，当收到所有ACK时，进入下一阶段，否则终止事务</li>
<li>Commit Primary（主服务器上提交）：CM向所有的primary服务器发送（one-sided RDMA）Commit primary信息，primary收到信息后进行修改+版本变更后，释放对应资源锁，事务结果对客户端可见</li>
<li>Truncate（日志截断）：CM在收到某条日志的所有ACK后，在下次消息传递时，通过携带截断信息，截断primary和backup对应日志信息，backup在截断日志时真正的执行修改操作</li>
</ol>
<p>综上所述，可以总结FaRM分布式事务实现区别点如下：</p>
<ol>
<li><strong>乐观并发控制</strong>：由于one-sided RDMA不经过CPU</li>
<li><strong>两阶段提交</strong>：（Lock+Validate）上锁验证截断 +（Commit backup+Commit Primary）主从提交阶段</li>
<li><strong>不保留日志</strong>：由于底层基于非易失存储（或者说论文中假设底层非易失），对于已经提交日志，无需保留</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总觉得FaRM虽然在保证一致性的前提下同时保持了高性能，由于其硬件约束和复杂的分布式事务和容错机制，很难在实际生产场景中应用，其基于乐观并发控制实现的分布式事务思路是读这篇论文的最大收获。</p>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>分布式理论</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>分布式数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>链式复制论文总结</title>
    <url>/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/%E9%93%BE%E5%BC%8F%E5%A4%8D%E5%88%B6%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h2 id="链式复制及其改进"><a href="#链式复制及其改进" class="headerlink" title="链式复制及其改进"></a>链式复制及其改进</h2><p><em>写在前面：CRAQ还涉及到了服务器放置策略等内容，我目前阶段学习并不关心，所以没有深入了解</em></p>
<blockquote>
<p>论文：<a href="https://www.usenix.org/legacy/events/usenix09/tech/full_papers/terrace/terrace.pdf">Object Storage on CRAQ High-throughput chain replication for read-mostly workloads</a></p>
</blockquote>
<p>本文主要提出了一种区别于主从备份容错方式的链式复制容错方式，通过将备份服务器组织链结构，实现数据的冗余备份。本文中的CRAQ(Chain Replication with Apportioned Queries) 是在链式复制（Chain Replication）的基础上改进而来的，两者针对对象存储系统设计实现（object-based storage system），对象存储系统主要包括一下两个操作原语：</p>
<ol>
<li>write(objID, V)：更新某个obj关联的值</li>
<li>read(objID): 获取某个obj的值</li>
</ol>
<p>对象存储只需要考虑修改读取特定对象的请求顺序，而并不需要考虑整个数据库，降低了保证一致性的成本</p>
<span id="more"></span>
<h3 id="链式复制（CR-Chain-Replication）"><a href="#链式复制（CR-Chain-Replication）" class="headerlink" title="链式复制（CR:Chain Replication）"></a>链式复制（CR:Chain Replication）</h3><p>链式复制的原理容易理解，通过将多个存储数据副本的服务器组织成链式形式，在响应读取、更新请求时在链上进行传播处理，拓扑结构如下图所示</p>
<ol>
<li>所有的<strong>写请求wirte</strong> 由Head节点服务器处理，计算得到状态后，在链上传播直到tail节点提交后，向client返回ack完成写请求</li>
<li>所有<strong>读请求read</strong> 由Tail节点服务器处理，直接查询本地值返回client</li>
</ol>
<img src="/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/%E9%93%BE%E5%BC%8F%E5%A4%8D%E5%88%B6%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220705150739033.png" class="" title="image-20220705150739033">
<p>其中链式复制保证<strong>强一致性（strong consistency）</strong></p>
<ul>
<li>所有写请求只有在TAIL提交后才对client可见</li>
<li>所有的读请求均在TAIL节点处理，返回TAIL数据</li>
</ul>
<p>链式复制的特殊请求处理机制保证了所有请求按照其发出的顺序处理，所以实现了<strong>强一致性</strong>。</p>
<h4 id="请求重发"><a href="#请求重发" class="headerlink" title="请求重发"></a>请求重发</h4><blockquote>
<p>While it would be possible to ensure that each request reaching the storage service is guaranteed to be performed, the end-to-end argument suggests there is little point in doing so.</p>
</blockquote>
<p>论文中有意思的一点是认为由服务器保证接收到的请求一定被处理是”<strong>得不偿失</strong>“的</p>
<ul>
<li>由客户端在请求超时一段时间后，向服务器重新发送请求</li>
<li>链式复制容错实现机制中，可能会丢弃一部分服务器接收到但未处理的请求</li>
</ul>
<p>由客户端重发请求引出另外一个问题：如果请求超时是由于客户端未收到服务器处理成功响应，重新发送请求会导致重复执行操作，需要<strong>保证操作的幂等性（idempotent）</strong></p>
<ol>
<li>读操作：读操作不改变系统状态，是幂等的</li>
<li>写操作：论文中没有写怎么解决，只是说可以通过客户端查询判断是否更新已经在server执行，从而避免重发请求</li>
</ol>
<h4 id="应对失效"><a href="#应对失效" class="headerlink" title="应对失效"></a>应对失效</h4><p>要理解CR如何应对实现，首先要理解链上的server的下两个状态</p>
<ol>
<li>$Hist_{objID}$：当前副本已提交的状态更新列表（实际上的待处理写请求，所有server均维护自己的该状态），其中尾节点的$Hist^{tail}_{objID}$定义为<strong>全局已提交的写请求</strong></li>
<li>$Pending_{objID}$：<strong>待处理的读取请求列表</strong>，只有tail节点存储当前状态</li>
</ol>
<p>不同节点之间的hist状态满足以下<strong>Update Propagation Invariant性质</strong>：即后继节点的待提交状态更新是前驱节点的前缀子集（从读请求从头节点向后传播容易理解）</p>
<script type="math/tex; mode=display">
Hist^i_{objID} \preceq Hist^j_{objID},其中i ≤ j</script><p>针对不同节点的失效，CR分情况处理（CR假设了一个永不失效的master节点负责维护节点信息，处理失效情况。实际上master<br>是由paxos算法实现多主从容错）</p>
<ol>
<li>头节点失效: 后续节点取代它成为头节点</li>
<li>尾节点失效：前驱节点成为新的尾节点</li>
<li>中间节点失效：类似于从链表中删除节点操作，更新服务器拓扑结构</li>
</ol>
<p>其中情况2会导致尾节点的待处理请求丢失，然而这在系统中是允许的（把重传的职责交给client）。由于上述Update Propagation Invariant性质的保证，<strong>不会丢失写请求</strong>。</p>
<p>情况3会导致<strong>忘记更新传播</strong>的情况</p>
<ul>
<li>T-节点将更新传播给了T节点，T节点还未来得及传播给T+,T节点失效，导致T-认为已将更新向后传播，T+实际上未收到更新的情况</li>
<li>由master节点在维护链拓扑时，比较T-和T+的$Hist_{objID}$列表，找出两者缺失部分，提醒T-节点重传</li>
</ul>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><ol>
<li>尾节点处理所有读请求，负载过大</li>
<li>写请求按照链式传播，导致写请求的延迟过高（理论上链越长，更新延迟越大，相当于增加了server反而降低了性能）</li>
</ol>
<h3 id="CRAQ（Chain-Replication-with-Apportioned"><a href="#CRAQ（Chain-Replication-with-Apportioned" class="headerlink" title="CRAQ（Chain Replication with Apportioned"></a>CRAQ（Chain Replication with Apportioned</h3><p>Queries）</p>
<img src="/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/%E9%93%BE%E5%BC%8F%E5%A4%8D%E5%88%B6%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220705160141244.png" class="" title="image-20220705160141244">
<p>CRAQ针对CR只能在尾节点处理读请求的问题进行了改进，实现了任意节点的读取，缓解了负载问题</p>
<ol>
<li><p>每个副本server为对象维护一个原子递增的版本号，同时存储多个版本的对象值，每个版本的对象值带上一个是否提交的标志（clean or dirty）</p>
</li>
<li><p>当副本server收到修改对象请求（还是从头向后传播），增加版本号大小并将更新值添加到版本值列表中</p>
<ul>
<li><p>若当前节点非TAIL，将当前值标记为dirty,向后继节点传播</p>
</li>
<li><p>当前节点为TAIL，标记当前节点为clean（完成更新提交），反向沿链传播ack，接收到ack的节点，更新值状态为clean，并删除存储的历史版本值</p>
<img src="/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/%E9%93%BE%E5%BC%8F%E5%A4%8D%E5%88%B6%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20220705161634798.png" class="" title="image-20220705161634798">
</li>
</ul>
</li>
<li><p>当副本server首先读取对象请求，根据最新版本对象值是否为clean执行不同操作</p>
<ul>
<li>clean：直接响应client请求</li>
<li>dirty：询问tail节点，获得最新version的clean值，响应client请求（反向ack还未传播完全）</li>
</ul>
</li>
</ol>
<h4 id="应对失效-1"><a href="#应对失效-1" class="headerlink" title="应对失效"></a>应对失效</h4><p>与CR一致，区别在于管理和容错的master节点不再自己实现（paxios），而是借由第三方分布式协调服务服务实现，如zookeeper等（<del>懒狗，不重复造轮子是吧</del>）</p>
<h4 id="相对于CR的改进"><a href="#相对于CR的改进" class="headerlink" title="相对于CR的改进"></a>相对于CR的改进</h4><p>首先多版本的对象值能够根据应用需求，实现不同的一致性保证（<del>不要更好的一致性，当然是为了更好的性能</del>）</p>
<ol>
<li>强一致性：论文中描述的即保证强一致性的操作，即只读clean数据</li>
<li>最终一致性：允许节点返回未提交的新数据，但是单个节点不能读到历史数据（在CRAQ中，即允许读取dirty数据）</li>
<li>带有最大不一致要求的最终一致性：允许节点返回未提交的新数据，但是需要满足一定约束，（例如单个客户端不能读到历史数据，即每次读取version必须单调递增）</li>
</ol>
<p>最后也是最重要的<strong>性能提升</strong></p>
<ol>
<li>Read-Mostly Workloads（读操作为主的负载）：读操作可以在所有节点执行，吞吐量随着server增加而增加</li>
<li>Write-Heavy Workloads（写操作为主的负载）：写操作过多，会导致大部分非tail节点的数据均为dirty，转而大部分查询请求询问尾节点，然而此情况仍然优于所有读请求由尾部处理的情况</li>
</ol>
<h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p><strong>CR和CRAQ中为什么要使用master节点管理和监控链上节点状态？</strong></p>
<p>如果不使用外部节点，我们的只能通过前驱或者后继节点相互感知，当节点失效时，由其后继或者前驱替代的方式实现容错，然而这样会出现脑裂问题</p>
<ul>
<li>当tail与前驱节点出现网络分区（CAP中的P）问题时，显而易见此时前驱节点会认为TAIL失效，自己成为tail,此时出现了两个TAIL即<strong>脑裂问题</strong></li>
<li>头节点失效类似，也会出现脑裂问题</li>
</ul>
<p>所以必须采用外部的<strong>权威节点</strong>(共识算法：raft、paxos 或分布式协调服务：zookeeper)，来监控整体状态，决定谁是tail或者head</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>正如MIT6.824中所说的当我们使用多个服务器提供服务时，我们首先应该考虑使用N个服务器是否能够带来N倍的性能/吞吐量提升，或者线性提升，然而实际情况中受限于算法实现、一致性的保证、服务器拓扑，性能提升往往是低于预期的。</p>
<p>CRAQ通过简单的链式结构+符合直觉的错误恢复机制，实现了容错备份，但是性能部分有所取舍</p>
<ol>
<li>写入沿链传播，随着链长度的增加，写入延迟增加（增加服务器反而性能下降）</li>
<li>读取通过一系列优化，近似实现了任意节点的随机读取（增加服务器，近似线性提升处理性能）</li>
</ol>
<p>然而链式更新传播，相较于raft、zookeeper等的由master节点广播传播的方式，降低了master节点通信负载</p>
<ul>
<li>master节点广播形式需要传n-1个节点，链式传播每个节点只需要负责传播自己的后继节点</li>
</ul>
<p><del>zookeeper、CRAQ以及GFS的例子，告诉我们一个道理：三个和尚没水喝，如果没必要别搞分布式，费力不讨好</del></p>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>分布式理论</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>复制策略</tag>
      </tags>
  </entry>
  <entry>
    <title>如何做研究-第一部分</title>
    <url>/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%A6%82%E4%BD%95%E5%81%9A%E7%A0%94%E7%A9%B6/%E5%A6%82%E4%BD%95%E5%81%9A%E7%A0%94%E7%A9%B6-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/</url>
    <content><![CDATA[<h2 id="写在开头"><a href="#写在开头" class="headerlink" title="写在开头"></a>写在开头</h2><p><em>开头废话：终于摆脱了没有前途的项目开始写论文了，虽然写论文不一定比做项目更有前途，但是可以预计的是只要我认真干，达到毕业标准是没问题的，而我目前对于实验室工作的目标就是满足毕业要求，所以说捋起袖子，加油干吧。正好李沐出了这个如何搞研究的视频，我也跟着学一学，不止为了写论文，更多的是从写论文这个经历里，为未来工作中各种报告文档的书写留下一点可用的经验。</em></p>
<p>跟着李沐的课程学习搞研究的基本概念，主要的学习资料有</p>
<ol>
<li>李沐的<a href="https://www.bilibili.com/video/BV1hY411T7vy">研究的艺术B站视频</a></li>
<li>研究的艺术中文版第四版</li>
</ol>
<p>计划是先自己读一遍中文书的对应章节，再去看李沐的视频，加深印象，下面是第一部分内容。</p>
<h2 id="研究、研究人员、读者"><a href="#研究、研究人员、读者" class="headerlink" title="研究、研究人员、读者"></a>研究、研究人员、读者</h2><h3 id="什么是研究"><a href="#什么是研究" class="headerlink" title="什么是研究"></a>什么是研究</h3><p>什么是研究？</p>
<ul>
<li><p>为了回答/解决某个问题搜集信息就是研究</p>
</li>
<li><p>日常生活中我们经常有类似的场景（例如百度小米的CEO是谁），只是我们并不会把这个过程规范化，结论文档化</p>
</li>
</ul>
<p>为什么要把研究内容写下来？</p>
<ul>
<li>为了方便记忆（俗话说得好：好记性不如烂笔头）</li>
<li>为了增进对于研究内容的理解。在梳理论据，描述论点的过程中，会加深自己对于相关内容的理解</li>
<li>为了检验自己的思想。只说是无法证明自己思想的正确性，写在纸上，通过论据论述，数学公式的证明，为自己的思想进行强有力的证明</li>
</ul>
<span id="more"></span>
<p>为什么写论文要遵循格式？</p>
<ul>
<li>写作不仅是为了自己，更是为了交流分享，你遵循外在的语言和格式要求，其实是在摆脱自己的主观愿望，使自己的思想向外界方便理解、更加真实的形式转变。（为了适应外界而改变自己，才能够好的理解自己的思想）</li>
<li>论文的格式是在长期实践中形成的，格式存在有其存在合理性（便于作者与读者联系起来，方便阅读等等）</li>
<li>用程序员的思路理解就是<strong>内容要满足通信格式才能在网络上发送和传播</strong></li>
</ul>
<p><strong>总结</strong></p>
<ul>
<li><strong>研究就是为了问题找答案</strong>，而这个寻找过程和答案本身需要<strong>通过规范的格式记录下来</strong></li>
<li>以规范格式记录研究的过程，也就是思考研究本身的过程，这个思考不仅是从<strong>自身的角度</strong>（能否解决问题），更是从<strong>读者的角度</strong>（研究是否有价值、论文是否容易理解和阅读）</li>
</ul>
<h3 id="与读者建立联系"><a href="#与读者建立联系" class="headerlink" title="与读者建立联系"></a>与读者建立联系</h3><p>撰写论文的过程中我们首先要确定我们自身的角色和预期读者的角色，才能确保我们的论文表达方式的正确的，有读者惯性感兴趣的。</p>
<h4 id="确定自己的角色"><a href="#确定自己的角色" class="headerlink" title="确定自己的角色"></a>确定自己的角色</h4><p>研究的过程不是你让你的老师知道你了解很多的事实，学会了很多的方法，应该是告诉老师问题是什么，方法的意义是什么，解决问题的效果如何，和其他方法相比有什么优略，从提供事实的角度自己可以分为三个角色</p>
<ol>
<li>找到了有趣的新信息 -<strong>提供关于问题的信息</strong><ul>
<li>职责：搜集信息</li>
<li>角色任务是陈列相关信息</li>
<li>撰写文档：尽可能全面的展示我搜集到的相关信息</li>
<li>eg: b站上的分享视频（我吃了什么，玩了什么）</li>
</ul>
</li>
<li>解决了重要的实际问题（作报告）-<strong>解决问题</strong><ul>
<li>职责：帮助决策者知道如何解决问题</li>
<li>角色任务是展示解决问题的方案、证明方案的正确性</li>
<li>撰写文档：使用合适的专业词汇，引用正确的信息，找到合适的证据，并以合适的方式表达，以达到说服决策者的目的</li>
<li>eg：csdn上各种代码bug的解决方案的博客</li>
</ul>
</li>
<li>给一个重要的问题找到了答案（<strong>做研究</strong>）-<strong>增进对问题的理解</strong><ul>
<li>职责：更好的理解问题+解决方案</li>
<li>角色任务是分析问题+阐述答案</li>
<li>撰写文档：阐明研究问题+研究方案</li>
</ul>
</li>
</ol>
<p>写作</p>
<h4 id="确定读者的角色"><a href="#确定读者的角色" class="headerlink" title="确定读者的角色"></a>确定读者的角色</h4><blockquote>
<p>读者是谁，读者读到这里知道什么了，接下来他们想要知道什么</p>
</blockquote>
<ol>
<li>图一乐，只是为了了解一下相关信息</li>
<li>希望通过读论文得到解决问题的方案</li>
<li>系统通过读论文对问题得到更深的理解</li>
</ol>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>如何做研究</category>
      </categories>
      <tags>
        <tag>杂文</tag>
        <tag>论文写作</tag>
      </tags>
  </entry>
  <entry>
    <title>如何做研究-第二部分</title>
    <url>/2022/07/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%A6%82%E4%BD%95%E5%81%9A%E7%A0%94%E7%A9%B6/%E5%A6%82%E4%BD%95%E5%81%9A%E7%A0%94%E7%A9%B6-%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/</url>
    <content><![CDATA[<h2 id="第二部分-提问题，找答案"><a href="#第二部分-提问题，找答案" class="headerlink" title="第二部分 提问题，找答案"></a>第二部分 提问题，找答案</h2><p>这一部分主要提供如何从兴趣、专业领域中寻找研究问题，并寻找问题答案过程的建议和方法论。</p>
<h3 id="从话题到问题"><a href="#从话题到问题" class="headerlink" title="从话题到问题"></a>从话题到问题</h3><p>从感兴趣的话题领域到具体的研究问题是一个从大到小的过程</p>
<ul>
<li>话题是广泛的，往往涵盖大量的信息和资料，沉浸在一个话题中搜索查找相关信息，虽然能够搜集大量的信息，然而这些信息大部分往往是没有任何用途的，因为缺乏重点，在读者眼里可能仅仅是堆放在一起的事实。</li>
<li>问题是从话题中产生的，根植于话题的。一个好的问题的好的答案答案能够在一定程度上推进整个领域往前发展</li>
</ul>
<p>然而疑问（question）并不等于问题（problem）</p>
<ul>
<li>一个好的问题是具有实际价值的”疑问“，解答这个问题能给我们带来一些东西（eg：解决某个工程难题，提升程序运行效率等等）</li>
<li>通过问我们自己:”不解决这个问题会怎样’’来区分区分疑问和问题的方式，如果答案是”不会怎么样 so what?”，那么这个问题就<strong>不应该成为我们的研究问题</strong></li>
</ul>
<span id="more"></span>
<h4 id="话题"><a href="#话题" class="headerlink" title="话题"></a>话题</h4><p>在专业背景下从兴趣出发得到自己的话题</p>
<ul>
<li>书上讲的很虚，似乎是没什么方法论，还是要多看相关文献，结合自身的研究背景和兴趣</li>
<li>从我目前的角度来说，我并不需要自己找问题，把老师给的问题解决掉即可（不需要独立做研究）</li>
</ul>
<p>话题从宽到窄</p>
<ul>
<li>话题是研究的出发点，然而如果话题过于宽泛，研究相关资料需要耗费大量精力，很难有相关发现。</li>
<li>通过将话题转化为论点，并通过增加修饰词缩小话题限定范围<ul>
<li>托尔斯泰的自由意志-&gt;托尔斯泰的小说中有自由意志（变成论点）-&gt;托尔斯泰在战争与和平中描写了三次战役中自由意志与必然性之间的冲突</li>
<li>数据中心节能-&gt;数据中心需要节能-&gt;数据中心大量服务器运转消耗大量能量，如何通过优化算法降低能量消耗？（似乎不太贴切）</li>
</ul>
</li>
</ul>
<p>带着问题查阅话题相关资料，如果不是学习目的，而是研究目的，最好带着问题查阅相关资料，这样在查阅资料过程中才会更有针对性，不会陷入无头苍蝇式的无效查询。</p>
<p>从话题到问题，书中提出的三个步骤</p>
<ol>
<li>确定话题，并为话题确定一个名字<ul>
<li>我想了解/研究 xxx</li>
</ul>
</li>
<li>提出问题和间接疑问<ul>
<li>我想了解/研究，因为我想知道xxx</li>
</ul>
</li>
<li>问自己 “so what“<ul>
<li>解决这个问题有什么价值？不解决对别人来说有什么损失？</li>
</ul>
</li>
</ol>
<p>最终目的还是要<strong>找到一个有实际价值的能够解决的问题</strong></p>
<h4 id="寻找问题的意义"><a href="#寻找问题的意义" class="headerlink" title="寻找问题的意义"></a>寻找问题的意义</h4><p>研究问题主要分为两类：</p>
<ol>
<li><strong>实际问题</strong>：这类问题来源于现实中的某种情况（垃圾邮件、数据中心耗能），这种状况使得我们损失金钱、时间等等，解决这个问题能够为现实生活带来实际的改变</li>
<li><strong>观念问题</strong>：这类问题来源于我们对于世界认知的不完全性，当我们不理解现实中某些事物或者与预期想象不符时，就产生了一个观念问题，解决观念问题不能为现实带来实际的成果，更多的作用是完善我们的认知和观念。</li>
</ol>
<p>无论是哪类研究问题，均满足以下问题的结构：</p>
<ol>
<li>一种情况</li>
<li>这种情况导致的不良后果（这种后果往往是读者关心的），不良后果也就决定着研究问题意义</li>
</ol>
<p>实际问题与观念问题的<strong>主要区别</strong>就在于这种“不良后果”，</p>
<ol>
<li>实际问题的”不良后果”是不解决问题所带来的代价（如不解决数据中心节能问题，会带来大量能量浪费）</li>
<li><p>观念问题的“不良后果”是缺乏对某些问题的正确认知或者只是，导致无法回答另外一个更加重要的问题</p>
<ul>
<li>无法回答 数据中心节能的边界条件-&gt;我们就无法回答使用什么方法能够降低数据中心能耗</li>
</ul>
<p>实际研究问题的意义容易通过实际后果确定，然而<strong>如何确定观念问题的研究意义</strong>（或者说让读者觉得有意义）？</p>
</li>
</ol>
<ul>
<li><p>从初始的研究问题出发，引到更大更重要的问题（研究在中国历史中台湾与大陆的关系变迁-&gt;中国外交水平如何随着历史变迁）</p>
</li>
<li><p>将研究问题与实际问题联系起来（研究在中国历史中台湾与大陆的关系变迁-&gt;解决两岸同胞存在的认同问题）</p>
</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>从兴趣到研究话题再到研究问题是一个复杂的上升过程，在这个过程中我们能做的</p>
<ol>
<li>在开展研究之前，首先要想好研究的意义，不仅是对自己，更是对于读者</li>
<li>研究问题的获得是困难的，没有具体的方法论，需要不断地学习和实践</li>
</ol>
<p><img src="C:\Users\79351\OneDrive\桌面\正在写的文档\如何做研究-第二部分.assets\image-20220703095118304.png" alt="image-20220703095118304"></p>
<h3 id="从问题到资料"><a href="#从问题到资料" class="headerlink" title="从问题到资料"></a>从问题到资料</h3><p>书中说到的方法不太适合理工科资料查找，基本的资料查找方式：</p>
<ul>
<li>寻找研究类似问题的论文，向上找这些论文的引用，向下找引用这些论文的论文</li>
<li>在这些论文中找到较为关键的论文（与研究问题相关度较高的论文）</li>
</ul>
<p>如何评估论文的好坏：</p>
<ul>
<li>引用数量、所属会议和期刊的排名、论文发表时间</li>
<li>与自身研究问题联系密切程度</li>
</ul>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>如何做研究</category>
      </categories>
      <tags>
        <tag>杂文</tag>
        <tag>论文写作</tag>
      </tags>
  </entry>
  <entry>
    <title>读论文1-resNet</title>
    <url>/2022/03/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%871-resNet/</url>
    <content><![CDATA[<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><blockquote>
<p>论文地址 ：<a href="http://cn.arxiv.org/pdf/1512.03385.pdf">Deep Residual Learning for Image Recognition</a></p>
</blockquote>
<p>针对深度神经网络难以训练的问题，ResNet提出了一种特殊的网络结构-残差，有效的解决了深度网络的退化问题，降低了深度网络学习难度</p>
<h3 id="论文结构"><a href="#论文结构" class="headerlink" title="论文结构"></a>论文结构</h3><ol>
<li><p>摘要 abstract</p>
<p>按照<strong>问题-&gt;解决方案-&gt;实验效果</strong>的逻辑，首先提出问题：“深度神经网络难以训练”，引出解决方案-residual net，最后列举在不同数据上的卓越效果（ImageNet，COCO，CIFAR），证明解决方案的有效性</p>
</li>
<li><p>介绍 intro</p>
<p>与摘要的逻辑相同，逻辑非常严密（ps:太丝滑了）</p>
<ol>
<li>为什么要用深度网络？ 因为深度网络有助于捕捉特征，提升任务效果</li>
<li>增加网络深度又会出现<strong>两个主要问题</strong>：一是梯度爆炸/消失，二是深度网络的退化问题</li>
<li>梯度爆炸可以通过 <strong>normalized initialization and intermediate normalization layers</strong> 解决</li>
<li>网络退化如何解决？引出了本文的残差机制-residual</li>
<li>最后又展示了一轮不同数据上的实验效果</li>
</ol>
</li>
</ol>
<span id="more"></span>
<ol>
<li><p>相关工作 related work</p>
<p>没细看。。</p>
</li>
<li><p>算法描述 Deep Residual Learning</p>
<p><strong>残差机制-&gt;网络架构设计-&gt;网络实现</strong></p>
<ol>
<li><p>提出了深度网络之所以会出现退化,是因为 <strong>难以学习直接映射</strong>（只是形式上的理解，没有给出公式证明），很自然引出了残差机制的设计</p>
<blockquote>
<p>might have difficulties in approximating identity mappings by multiple nonlinear layers.</p>
</blockquote>
</li>
<li><p>为了方便对比残差机制是否真的有效，设计了<strong>Plain Networks</strong>和<strong>Residual Networks</strong>两种架构</p>
</li>
</ol>
</li>
<li><p>实验 Experiments</p>
<p>列举了从 ImageNet 到CIFAR10再到PASCAL and MS COCO三个数据集上不同任务的实验效果</p>
<ol>
<li><p>首先对比在ImageNet数据集上34层Plain network和18层效果，证明深层网络确实出现了退化现象，并且排除了是梯度消失造成的可能</p>
<blockquote>
<p> We conjecture that the deep plain nets may have <strong>exponentially low convergence rates</strong>, which impact the reducing of the training error</p>
</blockquote>
</li>
<li><p>然后对比34层和18层Residual Networks效果,证明残差确实能够解决深层网络退化问题,并且能够在训练初期加速网络收敛速度</p>
<blockquote>
<p>This indicates that the <strong>degradation problem is well addressed</strong> in this setting and we manage to obtain accuracy gains from increased depth</p>
</blockquote>
</li>
<li><p>继续对比了 <strong>直接映射 和 投影映射</strong> 两种不同残差计算方式，是否影响残差发挥作用，得到结论：单纯的直接映射残差机制即可解决退化问题，投影映射效果优于直接映射，但是计算量增加较大</p>
</li>
<li><p>之后提出了一种<strong>Deeper Bottleneck Architectures</strong>，在ImageNet上探索残差机制在更深层网络上的效果</p>
<img src="/2022/03/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%871-resNet/image-20220309093015669.png" class="" title="image-20220309093015669">
</li>
<li><p>在小数据集CIFAR-10上验证：<strong>当学习任务足够简单，并不需要过深网络时，深度网络中的残差映射会近似于直接映射</strong>（加了残差块的深层网络 <strong>等价于</strong> 浅层网络+多个直接映射层）</p>
<blockquote>
<p>the residual functions might be generally closer to zero than the non-residual functions.</p>
</blockquote>
</li>
<li><p>最后秀了一波在目标检测任务上的优秀成果</p>
</li>
</ol>
</li>
<li><p>总结 summarize</p>
<p>因为CVPR的页数限制没有总结（？）</p>
</li>
</ol>
<h3 id="残差机制-residual"><a href="#残差机制-residual" class="headerlink" title="残差机制-residual"></a>残差机制-residual</h3><img src="/2022/03/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%871-resNet/image-20220309093814713.png" class="" title="image-20220309093814713">
<p>文章中分别提出了两种残差映射公式，其中第一种为 “Identity Mapping”，直接映射公式如下</p>
<script type="math/tex; mode=display">
y=F(x,\{W_i\}) +x  \tag{1}</script><p>其中 <script type="math/tex">F(x,\{W_i\})</script> 代表残差块内从X到残差的映射（如图2为中间带ReLU激活函数的两个权重层）,直接映射要求输入输出的唯独相同，可以直接叠加</p>
<p>第二种为 “linear projection”，将输入投影到与输出相同维度，方便叠加</p>
<script type="math/tex; mode=display">
y=F(x,\{W_i\}) +W_s x  \tag{2}</script><ul>
<li>论文中已通过实验证明， “Identity Mapping”即可解决深度网络的退化问题</li>
<li>虽然 “linear projection”效果略优于 “Identity Mapping”，但投影操作增加了计算量</li>
</ul>
<h3 id="网络结构设计"><a href="#网络结构设计" class="headerlink" title="网络结构设计"></a>网络结构设计</h3><p>两个基本设计原则</p>
<ol>
<li>当特征图大小缩小一半（<script type="math/tex">224*224->112*112</script>），通道数翻一倍（<script type="math/tex">64->128</script>）</li>
<li>当特征图大小不变时，通道数保持不变</li>
</ol>
<p>网络组成结构如图</p>
<ul>
<li>每个中括号内为一个残差块，每一个例如$conv2_x，conv3_x$ 的卷积层包含多个残差块</li>
<li>跨越不同卷积层时特征图缩小一般，通道数扩大一倍</li>
<li>例如18层网络的结构为<ol>
<li>首先通过 <script type="math/tex">7*7</script> 输出通道为64的卷积层+ <script type="math/tex">3*3</script>的最大池化层</li>
<li>进入第<script type="math/tex">conv2\_x</script>卷积层，包括两个残差块，每个残差块内包含两个<script type="math/tex">3*3*64</script> 的卷积层</li>
<li>进入第<script type="math/tex">conv2\_x</script>卷积层，同样包括两个残差块，由于第一个残差块输入为<script type="math/tex">56*56</script> 输出为<script type="math/tex">28*28</script> 需要使用投影残差计算，其余无差异，其中投影残差作者对比了两种不同的选择<ul>
<li>通过padding升维度，避免投影计算（A)</li>
<li>1*1卷积核，类似于全连接层(B）</li>
</ul>
</li>
</ol>
</li>
</ul>
<img src="/2022/03/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%871-resNet/image-20220309150217483.png" class="" title="image-20220309150217483">
<p><strong>Deeper Bottleneck Architectures</strong></p>
<p>针对ImageNet设计深层网络时，为了降低计算复杂度，设计了一种Bottleneck block</p>
<ul>
<li>与普通残差块不同在于 包含三个卷积层: <script type="math/tex">1*1 + 3*3 + 1*1</script>,其中 <script type="math/tex">1*1</script> 负责升维和降维</li>
<li>上图中34层和50层，虽然增加了16层，但是使用Bottleneck block的50层网络计算量增加不大</li>
</ul>
<img src="/2022/03/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%871-resNet/image-20220309152918557.png" class="" title="image-20220309152918557">
<p><strong>训练超参数</strong></p>
<ol>
<li>使用SGD，256 mini-batch，迭代训练<script type="math/tex">6*10^4</script> 次</li>
<li>初始学习率为0.1，每当错误率达到稳定，学习率缩小10倍</li>
<li>weight decay:0.0001,momentum:0.9,不使用dropout</li>
<li>每个卷积层之后，激活层之前，添加BN层</li>
</ol>
<h3 id="Pytorch-代码实现"><a href="#Pytorch-代码实现" class="headerlink" title="Pytorch 代码实现"></a>Pytorch 代码实现</h3><p>基础残差块：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    expansion: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        inplanes: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        planes: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        stride: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        downsample: <span class="type">Optional</span>[nn.Module] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        base_width: <span class="built_in">int</span> = <span class="number">64</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        dilation: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        norm_layer: <span class="type">Optional</span>[<span class="type">Callable</span>[..., nn.Module]] = <span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        <span class="keyword">if</span> groups != <span class="number">1</span> <span class="keyword">or</span> base_width != <span class="number">64</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;BasicBlock only supports groups=1 and base_width=64&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> dilation &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;Dilation &gt; 1 not supported in BasicBlock&quot;</span>)</span><br><span class="line">        <span class="comment"># Both self.conv1 and self.downsample layers downsample the input when stride != 1</span></span><br><span class="line">        <span class="comment"># 第一个卷积层传入 输入通道，输出通道，步长</span></span><br><span class="line">        self.conv1 = conv3x3(inplanes, planes, stride)</span><br><span class="line">        <span class="comment"># 卷积层后+batchnorm层</span></span><br><span class="line">        self.bn1 = norm_layer(planes)</span><br><span class="line">        <span class="comment"># relu激活函数</span></span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.conv2 = conv3x3(planes, planes)</span><br><span class="line">        self.bn2 = norm_layer(planes)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        identity = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">		<span class="comment"># 若输入与输出不相同，使用投影映射，也就是残差公式2</span></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line">		<span class="comment"># 残差计算+ReLU激活函数</span></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>投影残差函数初始化(就是简单的$1*1$卷积核加batchnorm）:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.inplanes != planes * block.expansion:</span><br><span class="line">    downsample = nn.Sequential(</span><br><span class="line">        conv1x1(self.inplanes, planes * block.expansion, stride),</span><br><span class="line">        norm_layer(planes * block.expansion),</span><br><span class="line">    )</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Bottleneck块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">       identity = x</span><br><span class="line">       <span class="comment"># 1*1</span></span><br><span class="line">       out = self.conv1(x)</span><br><span class="line">       out = self.bn1(out)</span><br><span class="line">       out = self.relu(out)</span><br><span class="line">	<span class="comment"># 3*3</span></span><br><span class="line">       out = self.conv2(out)</span><br><span class="line">       out = self.bn2(out)</span><br><span class="line">       out = self.relu(out)</span><br><span class="line">	<span class="comment"># 1*1</span></span><br><span class="line">       out = self.conv3(out)</span><br><span class="line">       out = self.bn3(out)</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">           identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">       out += identity</span><br><span class="line">       out = self.relu(out)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>resnet网络结构</p>
<img src="/2022/03/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%871-resNet/image-20220309154506166.png" class="" title="image-20220309154506166">]]></content>
      <categories>
        <category>科研学习</category>
        <category>读论文</category>
      </categories>
      <tags>
        <tag>resNet</tag>
      </tags>
  </entry>
  <entry>
    <title>读论文2-transformer</title>
    <url>/2022/03/20/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%872-transformer/</url>
    <content><![CDATA[<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><blockquote>
<p>论文地址：<a href="https://arxiv.org/pdf/1706.03762v5.pdf">Attention Is All You Need</a></p>
</blockquote>
<p>本文提出一个区别于传统RNN、CNN的基于attention机制的网络架构-transformer，在具备RNN捕捉序列特征能力的基础上，实现了并行计算，降低了计算成本（看论文名字就知道作者对论文内容非常自信）</p>
<h3 id="论文结构"><a href="#论文结构" class="headerlink" title="论文结构"></a>论文结构</h3><p>总结下来发现这篇论文没讲故事，就是单纯的讲自己的工作，非常的简洁</p>
<ol>
<li><p>摘要 abstract</p>
<p>从问题提出-&gt;解决方案-&gt;效果，三句话介绍NMT-&gt;encoder-decoder-&gt;transformer模型，剩余的句子全在列举是实验效果，<strong>非常简洁</strong></p>
</li>
<li><p>介绍 intro</p>
<p>还是围绕着 rnn、encoder-decoder、attention这三个主要内容</p>
<ul>
<li>首先介绍rnn在LM和NMT中广泛应用，取得了SOTA成果，但是其还是存在无法并行计算的问题（介绍了为什么）</li>
<li>然后介绍了attention机制能够跨距离建模依赖，但是目前研究大部分还是与rnn结合在一起</li>
<li>最后引出了本文完全基于attention机制的transformer模型</li>
</ul>
</li>
<li><p>背景 background</p>
<ul>
<li>列举了其他为了降低计算量的研究（convS2S, ByteNet）,但是降低效果不如transformer（证明自己工作的价值）</li>
<li>通过列举参考文献，证明 self-attention，decoder-encoder两种设计的有效性（证明自己解决方案的科学性）</li>
</ul>
</li>
</ol>
<span id="more"></span>
<ol>
<li><p>模型架构</p>
<ul>
<li>先是一段话+一张模型总图，整体介绍模型，然后每个层进行拆解，讲解内部机制</li>
<li><strong>总分结构</strong></li>
</ul>
</li>
<li><p>why attention</p>
<p>对比attention，rnn，cnn三种机制的计算的时间复杂度，证明attention机制真的有助于降低计算量</p>
</li>
<li><p>训练 traning</p>
<p>介绍了训练参数等</p>
</li>
<li><p>实验 Results</p>
<p>列举了在NMT以及English Constituency Parsing两个任务上的实验效果</p>
<ul>
<li>WMT 2014 English-to-German，WMT 2014 English-to-French实现了SOTA</li>
<li>为了证明transformer的在其它任务上的可泛化性，做了该实验，除了RNNG这个模型，transformer效果优于之前的所有模型</li>
</ul>
</li>
<li><p>结论 conclusion</p>
<p>总说自己提出了一个完全基于attention的模型，并再提了实验结果，最后说自己要把transformer推广到其他任务上（efficiently handle large inputs and outputssuch as images, audio and video）</p>
</li>
</ol>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>整体架构图如下，标准的encoder-decoder架构，encoder和decoder均为多个相同层的堆叠，其中encoder6层、decoder6层，共12层。</p>
<img src="/2022/03/20/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%872-transformer/image-20220314104109242.png" class="" title="image-20220314104109242">
<p>其他机制不再赘述，之前的<a href="https://fuhaifei.github.io/2021/10/17/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/">博客</a>总结过，简单总结一下几个新理解</p>
<h4 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h4><img src="/2022/03/20/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%872-transformer/image-20220314104956244.png" class="" title="image-20220314104956244">
<p><strong>为什么要Scale？</strong></p>
<blockquote>
<p>We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients[4]. To counteract this effect, we scale the dot products by $\frac{1}{\sqrt d_k}$</p>
</blockquote>
<p>在计算注意力分数时，QK向量相乘后，要除以输入向量维度的平方根，此论文中给出的解释，当输入维度 $d_k$ 较大时，QK向量会变得相对较大，导致softmax函数落在图像两边（如下图），导致梯度过小（梯度消失），通过除以$\sqrt d_k$ 避免此问题的出现</p>
<img src="/2022/03/20/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%872-transformer/image-20220314105426864.png" class="" title="image-20220314105426864">
<p><strong>encoder-decoder attention 的输入是什么</strong></p>
<blockquote>
<p>the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.</p>
</blockquote>
<p>query来自于decoder上一层的多头注意力机制输出，value和key来自于encoder的输出</p>
<p><strong>多头注意力机制为什么有用？</strong></p>
<p>类似于卷积神经网络中的通道的作用，不同的通道代表不同特征空间</p>
<h4 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h4><p><strong>为什么embedding层在+pos encoding前要乘以 $\sqrt d_k$?</strong></p>
<p>随着输入维度的增加，嵌入向量的每个位置的数字会变小（总和接近于一），而pos encoding的大小不变，为了保持嵌入向量和pos encoding的相对大小关系保持不变，乘以 $\sqrt d_k$。</p>
<h3 id="why-self-attention"><a href="#why-self-attention" class="headerlink" title="why self-attention?"></a>why self-attention?</h3><p>原文中通过对比自注意力机制、卷积层、循环神经网络的计算复杂度、顺序计算量(是否可并行计算)、最大序列特征捕捉计算次数三个指标，证明自注意力机制在计算量上的优势</p>
<img src="/2022/03/20/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%872-transformer/image-20220314145009609.png" class="" title="image-20220314145009609">
<p>Complexity per Layer</p>
<ol>
<li>self-Attenion的主要计算量在 Q和V相乘，两个均为 $n<em>d$维度矩阵，相乘复杂度为 $O(n^2</em>d)$</li>
<li>rnn每个时间步是一个输入为d,输出为d的全连接网络，整个seq的计算量为$O(n*d^2)$</li>
<li>卷积层与rnn类似，假设一维卷积核大小为k，每个元素近似在卷积核中卷积k次，整个seq的计算量为$O(k<em>n</em>d^2)$</li>
<li>可见当n &lt; d时，self-Attention的复杂度小于RNN</li>
</ol>
<p>其他理解比较简单</p>
<h3 id="实验参数"><a href="#实验参数" class="headerlink" title="实验参数"></a>实验参数</h3><p><strong>词嵌入</strong></p>
<ol>
<li>WMT 2014 English-German 使用 <strong>byte pair encoding</strong> 做词典，共37000 个token</li>
<li>WMT 2014 English-French 使用 32000 <strong>word-piece vocabulary</strong></li>
</ol>
<p><strong>optimizer</strong></p>
<p>使用Adam作为模型优化器</p>
<ol>
<li><p>三个参数 $\beta_1=0.9,\beta_2=0.98,\epsilon=10^{-9}$</p>
</li>
<li><p>学习率如下，当训练步数小于warmup步数时，取 $step_num*warmup_steps^{-1.5}$,其中warmup取4000步</p>
<img src="/2022/03/20/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%872-transformer/image-20220314150054771.png" class="" title="image-20220314150054771">
</li>
</ol>
<p><strong>dropout</strong></p>
<ol>
<li>每个子层在残差相加之前，增加dropout</li>
<li>pos encoding+embedding后增加dropout</li>
<li>base模型的dropout概率为 $P_{drop} = 0.1$</li>
</ol>
<p><strong>label smoothing</strong></p>
<p>训练时增加了标签平滑，参数值为 $\epsilon_{ls} = 0.1$ </p>
<img src="/2022/03/20/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%872-transformer/image-20220314150919745.png" class="" title="image-20220314150919745">]]></content>
      <categories>
        <category>科研学习</category>
        <category>读论文</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>读论文3-BERT</title>
    <url>/2022/03/20/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%873-BERT/</url>
    <content><![CDATA[<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><blockquote>
<p>论文地址:<a href="https://arxiv.org/abs/1810.04805v2">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
</blockquote>
<p>提出了一个基于transformer的预训练模型，引领了在大规模数据上预训练深度模型，在下游任务上微调的风潮，其主要贡献在两点</p>
<ol>
<li>引领了预训练模型的风潮</li>
<li>特殊的训练机制，实现了”真”双向语言模型</li>
</ol>
<h3 id="论文结构"><a href="#论文结构" class="headerlink" title="论文结构"></a>论文结构</h3><p>明显该论文的工作是基于ELMo和GPT改进，内容组织也是围绕着 前人工作+自己改进+实验效果展开的</p>
<ol>
<li><p>摘要 Abstract</p>
<p>提了一下 GPT和ELMo,强调BERT是区别于两者的基于transformer的“真”双向模型，并列举实验效果</p>
</li>
<li><p>介绍 Intro</p>
<p>主要是围绕着直接解决的问题以及相对于前人解决方案的优越性进行阐述</p>
<ol>
<li>首先强调 sentence-level 和 token-level 两种不同类型任务，需要聚焦于不同level的模型</li>
<li>又介绍了目前两种主要的预训练应用手段，一是特征提取，用预训练模型做特征提取，输入到下游任务模型；二是微调，预训练模型在下游任务数据上继续训练，微调参数</li>
<li>然后列举GPT和ELMo主流预训练模型存在的问题： 局限于语言模型的特性，无法实现真”双向”</li>
<li>引出了自己通过MLM+“next sentence prediction”,实现真双向，同时解决sentence level和token level两个问题</li>
</ol>
<p><strong>自己提出问题，自己解决问题，自圆其说</strong></p>
</li>
</ol>
<span id="more"></span>
<ol>
<li><p>相关工作 Related Word</p>
<p>从三个方面介绍了预训练相关工作情况，主要还是为了增加自己工作的可信度</p>
</li>
<li><p>bert</p>
<ul>
<li>模型架构</li>
<li>输入</li>
<li>预训练的两个target</li>
</ul>
<p>没细讲模型结构，主要强调思路，把有些难懂的细节放在了附录</p>
</li>
<li><p>实验 Experiments</p>
<p>列举了包括GLUE、SQuAD、SWAG四个任务上的微调的效果，验证了bert模型在token-level、sentence-level不同类型任务上的有效性</p>
</li>
<li><p>对比试验 Ablation Studies</p>
<ol>
<li><p>通过控制变量实验验证MLM和NSP对于提升模型特征抽取能力是有效的</p>
<img src="/2022/03/20/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%873-BERT/image-20220319143433975.png" class="" title="image-20220319143433975">
</li>
<li><p>不同模型深度对于模型效果的影响</p>
<img src="/2022/03/20/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%873-BERT/image-20220319143540227.png" class="" title="image-20220319143540227">
</li>
<li><p>bert用于特征抽取时，下游任务的效果（CoNLL-2003 命名实体识别任务的实验）</p>
</li>
</ol>
<img src="/2022/03/20/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%873-BERT/image-20220319143621944.png" class="" title="image-20220319143621944">
</li>
<li><p>结论</p>
<blockquote>
<p>Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks</p>
</blockquote>
</li>
</ol>
<p>​        简单强调了一下自己的主要贡献，即探索了深度双向架构预训练模型在解决一系列NLP任务中的作用。</p>
<h3 id="MLM-amp-NSP"><a href="#MLM-amp-NSP" class="headerlink" title="MLM &amp; NSP"></a>MLM &amp; NSP</h3><p>bert通过设计MLM和NSP这两个预训练目标，使得bert既能解决token-level，又能解决sentence-level的一系列NLP任务。</p>
<h4 id="Masked-LM"><a href="#Masked-LM" class="headerlink" title="Masked LM"></a>Masked LM</h4><p>由于语言模型本身约束，为了避免当前词看到未来词，只能训练单向语言模型，或者将两个方向的单向模型拼接在一起近似双向，bert在与训练过程中，将一部分词替换掉，训练模型预测被替换的词</p>
<ol>
<li><p>替换掉序列中 15%的 WordPiece token(还得保证替换之后，预训练预料和finetuning语料分布差距不会过大)</p>
<ul>
<li>其中 80% 替换为 [mask]</li>
</ul>
</li>
</ol>
<ul>
<li><p>10% 替换为词表中的任意一个词</p>
<ul>
<li>10% 保持原词不变</li>
</ul>
<p>在附录中的对比实验中，通过语言模型和NER两个任务效果对比，确定了8:1:1的比率设置</p>
</li>
</ul>
<ol>
<li><p>使用替换位置的最后一层隐藏状态，预测原词</p>
<blockquote>
<p>will be used to predict the original token with cross entropy loss.</p>
</blockquote>
</li>
</ol>
<p>通过Mask LM任务，bert具备了双向特征提取能力（bidirectional）</p>
<h4 id="NSP"><a href="#NSP" class="headerlink" title="NSP"></a>NSP</h4><p>为了使得BERT能够具备建模sentence level的任务（例如 QA,自然语言推断），预训练过程中增加 Next Sentence Prediction任务</p>
<img src="/2022/03/20/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%873-BERT/image-20220320190738243.png" class="" title="image-20220320190738243">
<ol>
<li>输入两个句子：A和B，其中 50% 训练数据 B为A的next（labeled as IsNext），50%训练数据 A和B随机抽取组合(labeled as NotNext），两句子拼接，中间使用特殊字符 [SEP]分隔</li>
<li>两个句子的<strong>token长度和小于512</strong></li>
<li>使用 <strong>[CLS]</strong> token最后一层输出做概率预测</li>
</ol>
<blockquote>
<p>The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.</p>
</blockquote>
<p>在预训练过程中，两个任务并行训练，训练损失为 <strong>平均mask概率 + 平均NSP概率</strong> 损失</p>
<h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>BERT预训练输入词向量包括三个不同Embeddings求和，三种Embeddings层均在训练中得到</p>
<ol>
<li>Token Embeddings 词嵌入，首先经过WordPieces模型分词后，在预训练模型中训练词嵌入</li>
<li>Segment Embeddings 段嵌入，与NSP任务相关，区分两个句子中的token，在预训练模型中训练得到</li>
<li>Position Embeddings 位置嵌入，也采用了 训练得到，<strong>未使用transformer中的正弦周期函数</strong></li>
</ol>
<img src="/2022/03/20/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%873-BERT/image-20220320192229816.png" class="" title="image-20220320192229816">
<h3 id="模型训练参数"><a href="#模型训练参数" class="headerlink" title="模型训练参数"></a>模型训练参数</h3><ol>
<li><strong>batch size=256</strong>, 每个batch的<strong>seq_length最长为512</strong>，训练了 <strong>40 epochs</strong>,大约1000000步</li>
<li>Adam优化器，lr= $1e^{-4}$ ,$\beta_1=0.9$, $\beta_2 = 0.999$ , weight decay $0.01$ ,dropout = 0.1</li>
<li>学习率随着训练epoch线性下降</li>
<li>使用 gelu实现替代了transformer中的relu函数</li>
</ol>
<p><strong>特殊的训练trick</strong></p>
<p>由于训练数据token长度分布大部分长度为128，为了加快收敛速度，在与训练过程中分为两阶段</p>
<ol>
<li>首先使用 <strong>128作为每个batch的seq_length</strong> 训练90%的step</li>
<li>再使用 <strong>seq_length=256</strong> 训练10%的step，用来学习position embedding（没太理解这么做的原因）</li>
</ol>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>读论文</category>
      </categories>
      <tags>
        <tag>bert</tag>
      </tags>
  </entry>
  <entry>
    <title>读论文4-GPT系列</title>
    <url>/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p>GPT作为NLP预训练语言模型的基石之一，从gpt1,gpt2到gpt3，不断扩大模型规模，将问题从fine-tuning拓展到zero-shot,试图解决更基础，但是更困难的无监督学习问题，三篇论文中的模型结构相同，不同的是试图解决的问题。</p>
<h3 id="GPT1"><a href="#GPT1" class="headerlink" title="GPT1"></a>GPT1</h3><blockquote>
<p><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a></p>
</blockquote>
<h4 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h4><p>NLP中有监督训练数据较少，存在大量的无标签文本数据，如何使用这些数据解决NLP领域中各类问题？GPT提出了 <strong>自监督预训练+fine tuning</strong> 的方式，主要解决两个问题</p>
<ol>
<li>预训练的<strong>训练目标是什么</strong>？损失函数是什么？</li>
<li>预训练得到的特征表示<strong>如何迁移</strong>到下游任务中？</li>
</ol>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>使用语言模型作为预训练的目标，在下游任务上做有监督的fine tuning</p>
<span id="more"></span>
<p><strong>预训练</strong></p>
<p>预训练使用无标签语料作为输入，以语言模型的最大似然函数为训练目标（给定前n-1个token，预测第n个token的条件概率）</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328083725904.png" class="" title="image-20220328083725904">
<p>模型使用transformer-decoder结构，最后一层通过softmax计算预测词的概率</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328084046552.png" class="" title="image-20220328084046552">
<p><strong>下游任务-fine tuning</strong></p>
<p>fine-tuning 使用下游任务的有标签语料，增加一个全连接sfotmax输出层，预测对应标签，损失函数为所有训练数据的log最大似然</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328084332027.png" class="" title="image-20220328084332027">
<p>另外增加了一个辅助训练目标，在下游任务上训练语言模型，使用两个损失函数求和作为<strong>最终的训练目标</strong>（$\lambda$ 为控制辅助训练目标对结果影响程度的参数）</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328084506127.png" class="" title="image-20220328084506127">
<p>针对不任务，仅仅使用softmax预测概率无法满足任务要求（如句子相似度衡量，需要输入两个句子，判断两个句子之间的相似度），gpt针对不同任务设计了不同类型的输入（<strong>Task-specific input transformations</strong>）</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328084857519.png" class="" title="image-20220328084857519">
<h4 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h4><p>模型细节</p>
<ol>
<li>12层的transformer-decoder，768隐藏层维度+12注意力头数，去掉了与encoder输出共同计算的注意力层（或者说<strong>使用 masked Multi Self Attention的encoder</strong>）</li>
<li>使用GLUE作为激活函数</li>
<li>使用 bytepair encoding (BPE) 词典，包含4000词</li>
<li>使用<strong>模型学习position encoding</strong>，而不是transformer论文中的三角函数计算</li>
</ol>
<p>训练细节</p>
<ol>
<li>训练数据集为 BooksCorpus dataset（contains over 7,000 unique unpublished books）</li>
<li>预训练：优化器为Adam，最大学习率为 $2.5*10^{-4}$，dropout=0.1，在batchsize=64,seq_length=512的条件下，训练100个epoch</li>
<li>微调：lr = 6.25e-5 , batchsize = 32, dropout = 0.1，训练3个epoch</li>
</ol>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>本文主要针对两个问题提出了解决方案，并在不同NLP上验证，得到了不错的效果</p>
<ol>
<li><p>预训练的<strong>训练目标是什么</strong>？</p>
<p>使用<strong>语言模型</strong>作为与训练目标（predict next token）</p>
</li>
<li><p>预训练得到的特征表示<strong>如何迁移</strong>到下游任务中？</p>
<p><strong>Task-specific input transformations + softmax predict层</strong> 在下游任务微调</p>
</li>
</ol>
<h3 id="GPT2"><a href="#GPT2" class="headerlink" title="GPT2"></a>GPT2</h3><blockquote>
<p><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">Language Models are Unsupervised Multitask Learners</a></p>
</blockquote>
<p>在BERT刷榜之后，GPT2带着更大的模型、更多的训练数据卷土重来，不再将关注点聚焦于 预训练+微调，转而研究语言模型在无监督多任务学习上的可能性</p>
<h4 id="要解决的问题-1"><a href="#要解决的问题-1" class="headerlink" title="要解决的问题"></a>要解决的问题</h4><p>论文中提出目前NLP领域主流的 <strong>预训练+微调</strong> 存在问题，即该方法仍需要监督学习，需要下游任务的大量有标签训练数据。本文将NLP的一般任务和特定任务定义为两种条件分布</p>
<ol>
<li>一般任务为 $p(output|input)$ 给定输入，对应输出的条件分布</li>
<li>特定任务  $p(output|input,task)$ 在特定任务上，输出不仅取决于输入，还取决于任务类型</li>
</ol>
<p>传统的预训练+fine-tuning方式</p>
<ol>
<li>预训练即模拟 $p(output|input)$条件分布</li>
<li>fine-tuning 通过在不同任务上<strong>调整模型架构、参数</strong>等，模拟  $p(output|input,task)$条件分布</li>
</ol>
<p>gpt2想要<strong>避免监督学习过程</strong>，即不在下游任务上做微调，引出了两个问题</p>
<ol>
<li>如何预训练能使得预训练过程中学到下游任务的信息？（原文中的句子：<strong>the global minimum of the unsupervised objective</strong><br><strong>is also the global minimum of the supervised objective.</strong> 全局非监督的收敛 等价于 全局监督学习的收敛）<ul>
<li>举个例子：像人学英语一样，背单词+看文章看看多了，就算从来没刷过题，题目也能做得效果不错</li>
</ul>
</li>
<li>如何<strong>不调整模型参数或者架构</strong>，<strong>实现从  $p(output|input)$ 到  $p(output|input,task)$</strong> 转变，以适应下游任务？</li>
</ol>
<h4 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h4><p>基本思路和GPT1区别不大，预训练语言模型+下游任务输入调整，不进行微调训练，个人觉得他的第二章Approach 前面关于理论来源的部分写的非常好，我读完感觉非常科学（也可能是我知识比较薄，看不出问题），分问题阐述一遍</p>
<p><strong>如何预训练能使得预训练过程中学到下游任务的信息</strong></p>
<blockquote>
<p>Preliminary experiments confirmed that sufficiently large <strong>language models are able to perform multitask learning</strong> in this toy-ish setup but learning is much slower than in explicitly supervised approaches.</p>
</blockquote>
<p>从两个角度解决该问题：</p>
<ol>
<li><p><strong>训练具备多任务学习能力的模型</strong>：语言模型 </p>
<ul>
<li>通过论文证明，语言模型具备多任务学习的能力，缺点是训练速度较慢</li>
</ul>
</li>
<li><p><strong>输入包含多任务信息的数据</strong>：WebText 数据集</p>
<ul>
<li><p>本文认为网络文本信息量巨大，包括各种下游任务中需要的数据信息，使用大量网络文本训练，能够使得模型学到不同任务的信息（multi task learning）</p>
</li>
<li><p>本文构造了一个WebText数据集，作者举了一些包含下游任务(NMT)数据的例子</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328101903526.png" class="" title="image-20220328101903526">
</li>
</ul>
</li>
</ol>
<p><strong>如何不调整模型参数或者架构，就能适应下游任务</strong></p>
<p>延续了GPT中的一些思路，通过修改输入表达方式，实现对下游任务的适应，以从英文到法文的机器翻译任务为例，在输入源句子的同时，以”english sentence = french sentence”为条件合并输入模型（文中成为 task hint）</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328143447270.png" class="" title="image-20220328143447270">
<p>在摘要任务中，论文尝试剔除了这种 <strong>”task hint“</strong>，发现任务指标下降了6.4个点，作者认为这证明了 使用自然语言提示模型针对任务改变的可行性</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328143917773.png" class="" title="image-20220328143917773">
<h4 id="顺带探讨的问题"><a href="#顺带探讨的问题" class="headerlink" title="顺带探讨的问题"></a>顺带探讨的问题</h4><p>在研究论文主要问题时，还顺带提了一下<strong>数据污染</strong>问题，即训练数据集和测试数据集之间重叠的问题，作者推荐在构建划分新的NLP数据训练集和测试集时，使用基于n-gram重叠的方法验证是否存在该问题</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328144755337.png" class="" title="image-20220328144755337">
<h4 id="实现细节-1"><a href="#实现细节-1" class="headerlink" title="实现细节"></a>实现细节</h4><p>模型细节</p>
<ol>
<li><p>使用<strong>BPE构建子词字典</strong>，为了避免无意义单词出现（”dog.“,”dog?”），限制不同类型字符共同出现，<strong>字典大小为 50257</strong></p>
</li>
<li><p>将transformer檐式结构结构中的 Post-LN 修改为了 Pre-LN(即把layer normalization移动到每个子层的前面，输入做归一化，输出不做)</p>
<ul>
<li>图片来自 <a href="https://arxiv.org/pdf/2002.04745v2.pdf">On Layer Normalization in the Transformer Architecture</a></li>
</ul>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328151023322.png" class="" title="image-20220328151023322">
</li>
</ol>
<p>训练细节</p>
<ol>
<li><p>batchsize=512，seq_length=1024</p>
</li>
<li><p>共有四个模型大小，最小的和gpt一一致，第二个与bert_large一致，最大的为GPT2</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220328151832377.png" class="" title="image-20220328151832377">
</li>
</ol>
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p>本文使用更大的模型尝试证明语言模型在zero-shot上的可行性，在许多任务上取得不错的成绩。</p>
<h3 id="GPT3"><a href="#GPT3" class="headerlink" title="GPT3"></a>GPT3</h3><p>GPT3是对GPT2在zero shot learning上的进一步推进，GPT2效果没有达到预期，就在GPT3上继续增加模型和训练数据规模，提升效果，在完全预训练模型上继续推广，提出了三种将预训练应用到下游任务的非预训练方式（所谓的 <strong>in-context learning</strong>）</p>
<ol>
<li><p>Few-Shot 解决下游任务时，在输入中提供几个任务样例</p>
<ul>
<li><p>一个任务提示</p>
</li>
<li><p>几个任务样例（prompt）</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220329154323252.png" class="" title="image-20220329154323252">
</li>
</ul>
</li>
<li><p>One-shot</p>
<ul>
<li><p>只给一个任务提示</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220329154359207.png" class="" title="image-20220329154359207">
</li>
</ul>
</li>
<li><p>Zero-shot 完全不给任务提示，只告诉任务是什么</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220329154433953.png" class="" title="image-20220329154433953">
</li>
</ol>
<p>GPT3将模型规模增大到原来的1000倍</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220329154640571.png" class="" title="image-20220329154640571">
<p>又构建了一个集合以往各种文本数据的巨大数据集</p>
<img src="/2022/04/03/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E8%AF%BB%E8%AE%BA%E6%96%87/%E8%AF%BB%E8%AE%BA%E6%96%874-GPT%E7%B3%BB%E5%88%97/image-20220329154729495.png" class="" title="image-20220329154729495">
<h4 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h4><p>GPT3文章太长，我只粗略地读了一遍，总体思路还是企图证明大规模预训练语言模型，在下游任务中，即使没有经过预训练，也能实现不错的效果，相较于前两篇论文没有太多新的东西</p>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>读论文</category>
      </categories>
  </entry>
  <entry>
    <title>lc2212.射箭比赛中的最大比赛</title>
    <url>/2022/03/27/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/lc2212.%E5%B0%84%E7%AE%AD%E6%AF%94%E8%B5%9B%E4%B8%AD%E7%9A%84%E6%9C%80%E5%A4%A7%E6%AF%94%E8%B5%9B/</url>
    <content><![CDATA[<h3 id="2212-射箭比赛中的最大得分"><a href="#2212-射箭比赛中的最大得分" class="headerlink" title="2212. 射箭比赛中的最大得分"></a><a href="https://leetcode-cn.com/problems/maximum-points-in-an-archery-competition/">2212. 射箭比赛中的最大得分</a></h3><p>打周赛遇到的一道非常典型的0-1背包问题，但是因为太久没写过背包问题，写了40分钟才写出来，再温习一遍。</p>
<h4 id="什么是0-1背包？"><a href="#什么是0-1背包？" class="headerlink" title="什么是0-1背包？"></a>什么是0-1背包？</h4><p>一共有N个物品，每个物品有对应的重量weight[i] 和价值value[i]，给定一个固定容量W的背包，问能够放入背包的最大价值是多少？非常经典又相对简单的动态规划问题，关键还是在于定义状态以及状态转移公式。</p>
<p>贪心的角度出发，假设我选取某个物品后，新的贪心目标变为 在剩余物品和空间的条件下，选取最大价值的物品，因此父问题到子问题的转化可以定义为：（按照从左到右一个一个选的思路）</p>
<ul>
<li><strong>父问题</strong>：是否选取第i个物品，使得总价值最大</li>
<li><strong>子问题</strong>：父问题 <strong>选 / 不选 </strong>第i个物品， 背包剩余 <strong>W-w[i] / W</strong>空间，如何在前i-1物品中合适选取使得子问题价值最大</li>
</ul>
<span id="more"></span>
<p>dp状态可以定义为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dp[i][j]: 将前i个物品装到容量为j的背包中的最大价值</span><br></pre></td></tr></table></figure>
<p>状态转移公式为:</p>
<ol>
<li><p>选取第i个物品</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dp[i][j] = v[i] + dp[i][j - w[i]]</span><br></pre></td></tr></table></figure>
</li>
<li><p>不选第i个物品（或者当前容量无法选取第i个物品）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dp[i][j] = dp[i - 1][j] </span><br></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li><p>最后公式为</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dp[i][j] = max(v[i] + dp[i][j - w[i]], dp[i - 1][j])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>复杂度分析</strong>:</p>
<ol>
<li><strong>时间复杂度：</strong>自下而上（前i物品）计算状态，每个物品需要遍历所有的背包容量，<strong>时间复杂度为 O(NW)</strong>，即物品数量乘以背包容量</li>
<li><strong>空间复杂度：</strong> 类似时间复杂度分析，存储所有状态需要 O(NW)，若使用存储压缩，只需要一个长度等于背包容量的数组，但是若要存储解，则无法压缩，<strong>空间复杂度为 O(NW) 或者 O(W)</strong></li>
</ol>
<p><strong>模板伪代码</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># dp数组长度为w+1,多一个为了减少越界判断</span><br><span class="line">dp[0,1,2,3....,w] = 0</span><br><span class="line">for i in [0,1,2,3,4,5,6,7,8,9....n]:</span><br><span class="line">	for j in [w,w - 1,....1]:</span><br><span class="line">		# j &gt; w[i],反向遍历避免状态丢失（因为只用了一维数组存储状态）</span><br><span class="line">		dp[j] = max(dp[j], dp[j−w[i]]+v[i])</span><br></pre></td></tr></table></figure>
<h4 id="射箭问题"><a href="#射箭问题" class="headerlink" title="射箭问题"></a>射箭问题</h4><p>非常明显的0-1背包问题，箭支数量为”背包容量”，占领每个区域所需要的箭为”物品重量“，每个区域的分数为”价值”，转化为0-1背包问题套模板即可，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] maximumBobPoints(<span class="keyword">int</span> numArrows, <span class="keyword">int</span>[] aliceArrows) &#123;</span><br><span class="line">        <span class="comment">// dp状态数组</span></span><br><span class="line">        <span class="keyword">int</span>[] dpArray = <span class="keyword">new</span> <span class="keyword">int</span>[numArrows + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span>[][] solution = <span class="keyword">new</span> <span class="keyword">int</span>[aliceArrows.length][numArrows + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; aliceArrows.length;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = numArrows;j &gt; <span class="number">0</span>;j--)&#123;</span><br><span class="line">                <span class="keyword">if</span>(j &gt; aliceArrows[i] &amp;&amp; i + dpArray[j - aliceArrows[i] - <span class="number">1</span>] &gt; dpArray[j])&#123;</span><br><span class="line">                    dpArray[j] = i + dpArray[j - aliceArrows[i] - <span class="number">1</span>];</span><br><span class="line">                    solution[i][j] = <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//生成结果数组(比较重要的一部分代码)</span></span><br><span class="line">        <span class="keyword">int</span>[] result = <span class="keyword">new</span> <span class="keyword">int</span>[aliceArrows.length];</span><br><span class="line">        <span class="keyword">int</span> curArrows = numArrows;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = aliceArrows.length - <span class="number">1</span>;i &gt; <span class="number">0</span>;i--)&#123;</span><br><span class="line">            <span class="keyword">if</span>(solution[i][curArrows] == <span class="number">1</span>)&#123;</span><br><span class="line">                result[i] = aliceArrows[i] + <span class="number">1</span>;</span><br><span class="line">                curArrows -= aliceArrows[i] + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        result[<span class="number">0</span>] = curArrows;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法相关</category>
        <category>力扣刷题</category>
      </categories>
      <tags>
        <tag>0-1背包</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>lc437.路径总和III</title>
    <url>/2021/09/30/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/lc437.%E8%B7%AF%E5%BE%84%E6%80%BB%E5%92%8CIII/</url>
    <content><![CDATA[<h3 id="437-路径总和-III"><a href="#437-路径总和-III" class="headerlink" title="437. 路径总和 III"></a><a href="https://leetcode-cn.com/problems/path-sum-iii/">437. 路径总和 III</a></h3><p>前后一共做了三次，每次都想不出来使用前缀和的方法。</p>
<h4 id="思路1：DFS遍历"><a href="#思路1：DFS遍历" class="headerlink" title="思路1：DFS遍历"></a>思路1：DFS遍历</h4><p>先序遍历整个二叉树，遍历到某一结点后，以该节点为子树，查找当前子树中，是否存在一条从根节点出发的路径，满足路径和条件。</p>
<p><strong>复杂度分析</strong>：</p>
<ul>
<li>时间复杂度：与DFS遍历不同的点，在于每到达一个节点都需要重新遍历子树，寻找备选最优解，<strong>时间复杂度为O(N^2)</strong></li>
<li>空间复杂度：两次遍历不影响栈的深度，最大栈深度与DFS相同，<strong>空间复杂度为O(logN)</strong></li>
</ul>
<span id="more"></span>
<p><strong>代码实现：</strong></p>
<p>思路比较简单，实现也比较简单</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getSolution</span><span class="params">(TreeNode curNode ,<span class="keyword">int</span> curSum, <span class="keyword">int</span> targetSum)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(curNode == <span class="keyword">null</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">    curSum += curNode.val;</span><br><span class="line">    <span class="keyword">if</span>(curSum == targetSum)&#123;</span><br><span class="line">        result++;</span><br><span class="line">    &#125;</span><br><span class="line">    result += getSolution(curNode.left, curSum, targetSum) + getSolution(curNode.right, curSum, targetSum);</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">pathSum</span><span class="params">(TreeNode root, <span class="keyword">int</span> targetSum)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//简单的dfs</span></span><br><span class="line">    <span class="keyword">return</span> getSolution(root, <span class="number">0</span>, targetSum) + pathSum(root.left, targetSum) + pathSum(root.right, targetSum);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="思路2：前缀和"><a href="#思路2：前缀和" class="headerlink" title="思路2：前缀和"></a>思路2：前缀和</h4><p>DFS计算路径和只能计算从根路径到当前节点的总和，但是满足目标解的路径不一定从根节点开始，我们可以将不从根节点开始的序列，转化成 两个从根出发序列的差，其中一个序列是另一个序列前缀。</p>
<p>如何实现这种存储，并且方便查询？深度遍历过程中将当前路径和存储到哈希表中，当遍历到某一个节点时，哈希表存储的是从根节点到当前节点的序列的<strong>所有前缀子序列的路径和</strong>,原问题转化为</p>
<script type="math/tex; mode=display">
sum_{seq\_i} = sum_{rootSeq_i} - sum_{preSeq_i}</script><p>在回退时，需要删除再是前缀的序列的前缀和（也就是当前序列）</p>
<p><strong>复杂度分析：</strong></p>
<ul>
<li>时间复杂度：与DFS一致，时间复杂度为树中的节点数目，<strong>时间复杂度为O(N)</strong></li>
<li>空间复杂度：哈希表的最大存储数量为O(logN)，<strong>空间复杂度为O(logN)</strong></li>
</ul>
<p><strong>代码实现：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//前缀和算法，包含当前节点</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">dfs</span><span class="params">(TreeNode curNode,<span class="keyword">int</span> curSum,<span class="keyword">int</span> targetSum,Map&lt;Integer, Integer&gt; preSumMap)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(curNode == <span class="keyword">null</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">    curSum += curNode.val;</span><br><span class="line">    <span class="keyword">if</span>(preSumMap.containsKey(curSum - targetSum) &amp;&amp; preSumMap.get(curSum - targetSum) != <span class="number">0</span>)&#123;</span><br><span class="line">        result += preSumMap.get(curSum - targetSum);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//将当前前缀和添加到hashmap中</span></span><br><span class="line">    <span class="keyword">int</span> times = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(preSumMap.containsKey(curSum))&#123;</span><br><span class="line">        times += preSumMap.get(curSum);</span><br><span class="line">    &#125;</span><br><span class="line">    preSumMap.put(curSum, times);</span><br><span class="line">    result += (dfs(curNode.left, curSum, targetSum, preSumMap) + dfs(curNode.right, curSum, targetSum, preSumMap));</span><br><span class="line">    <span class="comment">//回滚</span></span><br><span class="line">    preSumMap.put(curSum, times - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">pathSum</span><span class="params">(TreeNode root, <span class="keyword">int</span> targetSum)</span> </span>&#123;</span><br><span class="line">    Map&lt;Integer, Integer&gt; preSumMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    preSumMap.put(<span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> dfs(root, <span class="number">0</span>, targetSum, preSumMap);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li><p>初始化应加入根节点的”假”前缀序列(序列和为0)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">preSumMap.put(<span class="number">0</span>, <span class="number">1</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>有序树中可能有负值节点，不能只记录前缀值的否出现，应记录出现次数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> times = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span>(preSumMap.containsKey(curSum))&#123;</span><br><span class="line">	times += preSumMap.get(curSum);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>算法相关</category>
        <category>力扣刷题</category>
      </categories>
      <tags>
        <tag>树</tag>
        <tag>前缀思路</tag>
      </tags>
  </entry>
  <entry>
    <title>lc59.两数相除</title>
    <url>/2021/10/17/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/lc59.%E4%B8%A4%E6%95%B0%E7%9B%B8%E9%99%A4/</url>
    <content><![CDATA[<h3 id="29-两数相除"><a href="#29-两数相除" class="headerlink" title="29. 两数相除"></a><a href="https://leetcode-cn.com/problems/divide-two-integers/">29. 两数相除</a></h3><p>难点在于处理不能用long解决溢出问题</p>
<h4 id="思路1：“二进制”减法-没有满足越界要求"><a href="#思路1：“二进制”减法-没有满足越界要求" class="headerlink" title="思路1：“二进制”减法(没有满足越界要求)"></a>思路1：“二进制”减法(没有满足越界要求)</h4><p>不能用除法，最简单的思路就是被除数(dividend)不断地减除数(divisor)，直到减到剩余余数，相减的次数就是结果。该方法的问题就是时间复杂度太高，</p>
<ul>
<li>二进制优化，首先找到最大的n使得 $divisor <em> 2^n &lt; dividend$，每次减去 $ divisor </em> 2^n， divisor <em> 2^{n-1} ，divisor </em> 2^{n-2}  ……$,直到减到 $ divisor $</li>
<li>实际上就是将原来的逐个减去，变为二进制减去（找商的二进制表示），由于任何数都能由二进制表示，所以该方法必定有解</li>
<li>我的实现方法<strong>越界无法规避</strong>，只能用Long</li>
</ul>
<p><strong>复杂度分析：</strong></p>
<p>没有分析的必要，由于只有32位整数，二进制一共只需要移动32次，时间复杂度与空间复杂度均为O(1)</p>
<p><strong>代码实现：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">divide</span><span class="params">(<span class="keyword">int</span> dividend, <span class="keyword">int</span> divisor)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> result  = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">boolean</span> isNegative = (dividend &lt; <span class="number">0</span> &amp; divisor &gt; <span class="number">0</span>) || (dividend &gt; <span class="number">0</span> &amp; divisor &lt; <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">long</span> temp = <span class="number">1</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//首先绝对值求解</span></span><br><span class="line">    <span class="keyword">long</span> denominator = (<span class="keyword">long</span>)Math.abs((<span class="keyword">long</span>)dividend);</span><br><span class="line">    <span class="keyword">long</span> numerator = (<span class="keyword">long</span>)Math.abs((<span class="keyword">long</span>)divisor);</span><br><span class="line">    <span class="comment">//首先找到最大元素</span></span><br><span class="line">    <span class="keyword">while</span>(numerator &lt;&lt; <span class="number">1</span> &lt;= denominator)&#123;</span><br><span class="line">        temp = temp &lt;&lt; <span class="number">1</span>;</span><br><span class="line">        numerator = numerator &lt;&lt; <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(numerator &gt;= Math.abs((<span class="keyword">long</span>)divisor))&#123;</span><br><span class="line">        <span class="keyword">if</span>(denominator &gt;= numerator)&#123;</span><br><span class="line">            denominator -= numerator;</span><br><span class="line">            <span class="keyword">if</span>(isNegative)</span><br><span class="line">                result -= temp;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                result += temp;</span><br><span class="line">        &#125;</span><br><span class="line">        numerator = numerator &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        temp = temp &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(result &gt; Integer.MAX_VALUE)&#123;</span><br><span class="line">        result = Integer.MAX_VALUE;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">int</span>)result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="思路2：条件约束-二分查找（答案的方法看得我头疼）"><a href="#思路2：条件约束-二分查找（答案的方法看得我头疼）" class="headerlink" title="思路2：条件约束+二分查找（答案的方法看得我头疼）"></a>思路2：条件约束+二分查找（答案的方法看得我头疼）</h4>]]></content>
      <categories>
        <category>算法相关</category>
        <category>力扣刷题</category>
      </categories>
      <tags>
        <tag>位运算</tag>
      </tags>
  </entry>
  <entry>
    <title>lc517.超级洗衣机</title>
    <url>/2021/09/30/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/lc517.%E8%B6%85%E7%BA%A7%E6%B4%97%E8%A1%A3%E6%9C%BA/</url>
    <content><![CDATA[<h3 id="517-超级洗衣机"><a href="#517-超级洗衣机" class="headerlink" title="517. 超级洗衣机"></a><a href="https://leetcode-cn.com/problems/super-washing-machines/">517. 超级洗衣机</a></h3><p>贪心的思路并不难，就是太难想到了</p>
<h4 id="思路1：邻居平分法（我自己的思路）"><a href="#思路1：邻居平分法（我自己的思路）" class="headerlink" title="思路1：邻居平分法（我自己的思路）"></a>思路1：邻居平分法（我自己的思路）</h4><p>类似于最优解的区域法，针对每个洗衣机，将整个数组划分为左边右边两个子区域，计算左右两个区域的衣服总和，并定会存在总数小于或者大于 平均*区域数量 的区域，如果当前洗衣机衣服多，就将多余的衣服分给缺衣服的区域（给了邻居）</p>
<ol>
<li>每遍历一次所有洗衣机等价于一次移动</li>
<li>最后一次遍历所有情况下划分的子区域，均满足 平均*区域数量 的性质</li>
</ol>
<p><strong>复杂度分析：</strong></p>
<ol>
<li>时间复杂度：每次平均都需要从头到尾遍历数组，共需要遍历结果次数的数组，<strong>时间复杂度为O(KN)</strong></li>
<li>空间复杂度：额外开辟了一个存储前缀和的数组，<strong>空间复杂度为O(N)</strong></li>
</ol>
<span id="more"></span>
<p><strong>代码实现：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findMinMoves</span><span class="params">(<span class="keyword">int</span>[] machines)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> average;</span><br><span class="line">        <span class="keyword">int</span> numOFMach = machines.length;</span><br><span class="line">        <span class="keyword">int</span>[] preSum = <span class="keyword">new</span> <span class="keyword">int</span>[numOFMach];</span><br><span class="line">        preSum[<span class="number">0</span>] = machines[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i &lt; numOFMach;i++)&#123;</span><br><span class="line">            preSum[i] = preSum[i - <span class="number">1</span>] + machines[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//判断是否可分</span></span><br><span class="line">        <span class="keyword">if</span>(preSum[numOFMach - <span class="number">1</span>] % numOFMach != <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        average = preSum[numOFMach - <span class="number">1</span>] / numOFMach;</span><br><span class="line">        <span class="keyword">boolean</span> flag;</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            flag = <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; numOFMach;i++)&#123;</span><br><span class="line">                <span class="comment">//如果左边区域小，往左边区域移动</span></span><br><span class="line">                <span class="keyword">if</span>(machines[i] &gt; <span class="number">0</span>)&#123;</span><br><span class="line">                    <span class="keyword">if</span>(i != <span class="number">0</span> &amp;&amp; preSum[i] - machines[i] &lt; i * average)&#123;</span><br><span class="line">                        machines[i] -= <span class="number">1</span>;</span><br><span class="line">                        machines[i - <span class="number">1</span>] += <span class="number">1</span>;</span><br><span class="line">                        preSum[i - <span class="number">1</span>] += <span class="number">1</span>;</span><br><span class="line">                        flag = <span class="keyword">false</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span>(i != numOFMach - <span class="number">1</span> &amp;&amp; preSum[numOFMach - <span class="number">1</span>] - preSum[i] &lt; (numOFMach - i - <span class="number">1</span>) * average)&#123;</span><br><span class="line">                        machines[i] -= <span class="number">1</span>;</span><br><span class="line">                        machines[i + <span class="number">1</span>] += <span class="number">1</span>;</span><br><span class="line">                        preSum[i] -= <span class="number">1</span>;</span><br><span class="line">                        flag = <span class="keyword">false</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//没有平均操作就返回</span></span><br><span class="line">            <span class="keyword">if</span>(flag)&#123;</span><br><span class="line">                <span class="keyword">return</span> result;</span><br><span class="line">            &#125;</span><br><span class="line">            result++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>可惜差两个测试用例，超时</p>
<img src="/2021/09/30/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/lc517.%E8%B6%85%E7%BA%A7%E6%B4%97%E8%A1%A3%E6%9C%BA/image-20210930181801433.png" class="" title="image-20210930181801433">
</li>
</ul>
<h4 id="思路2：官方题解"><a href="#思路2：官方题解" class="headerlink" title="思路2：官方题解"></a>思路2：官方题解</h4><p>基于区域的最优，主要是三个点</p>
<ol>
<li><p>如果将数组划分为前后两个区域，如果是一个多衣服，一个少衣服，要达到平均状态最少也要将</p>
<p>多衣服的区域多的衣服，转移到少衣服区域，也就是 <strong>最少的次数至少也是多衣服区域多出来的衣服</strong></p>
</li>
<li><p>单个区域内要达到平均，最多衣服的洗衣机，要转移成平均，意味着 <strong>最少的次数至少是最多衣服数量洗衣机转移的衣服数量</strong></p>
</li>
<li><p>这种转移可以并行进行，也就得到了最终的贪心策略-<strong>寻找多最多的区域或者某个洗衣机</strong></p>
</li>
</ol>
<p><strong>复杂度分析：</strong></p>
<ol>
<li>时间复杂度：只需要遍历一次数组计算上面两个状态，<strong>时间复杂度为O(N)</strong></li>
<li>空间复杂度：不需要额外空间，<strong>空间复杂度为O(1)</strong></li>
</ol>
<p><strong>代码实现：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findMinMoves</span><span class="params">(<span class="keyword">int</span>[] machines)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//首先求和计算平均值</span></span><br><span class="line">    <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> average;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; machines.length;i++)&#123;</span><br><span class="line">        sum += machines[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(sum % machines.length != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//计算平均数，并用sum作为前i个元素所需要移入或移出的个数</span></span><br><span class="line">    average = sum / machines.length;</span><br><span class="line">    sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> tempNeed;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; machines.length;i++)&#123;</span><br><span class="line">        tempNeed = machines[i] - average;</span><br><span class="line">        sum += tempNeed;</span><br><span class="line">        result = Math.max(Math.abs(sum),Math.max(tempNeed, result));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法相关</category>
        <category>力扣刷题</category>
      </categories>
      <tags>
        <tag>贪心算法</tag>
      </tags>
  </entry>
  <entry>
    <title>lc798.得分最高的最小轮调</title>
    <url>/2022/03/13/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/lc798.%E5%BE%97%E5%88%86%E6%9C%80%E9%AB%98%E7%9A%84%E6%9C%80%E5%B0%8F%E8%BD%AE%E8%B0%83/</url>
    <content><![CDATA[<h2 id="798-得分最高的最小轮调"><a href="#798-得分最高的最小轮调" class="headerlink" title="798.得分最高的最小轮调"></a><a href="https://leetcode-cn.com/problems/smallest-rotation-with-highest-score/">798.得分最高的最小轮调</a></h2><p>第一次写一点思路都没有，扣了半天最终放弃，直奔题解，发现题目主要有两个难点</p>
<ol>
<li>从轮调位置角度考虑转换到每个元素位置考虑<ul>
<li>我的思路一直局限在从选k出发，如何计算出每个k位置的分数？怎么找到一种贪心或者动态规状态传递的方式</li>
<li>没有从元素的角度出发，某个元素满足小于等于index时，k一定在某个范围内，所有元素决定的k的范围交集次数最多的就是最优解（表达不出来这种思维的转变）</li>
</ul>
</li>
<li>如何记录最大交集次数？<ul>
<li>看完一半题解就想到创建一个数组，每计算出一个k的范围，就将范围内记录全部加一</li>
<li>题解提供的差分数组思路”针不错”</li>
</ul>
</li>
</ol>
<span id="more"></span>
<p>如何<strong>计算k的范围</strong>，以index=i处元素为例分情况讨论（基本思路：轮调等价于数组开头一部分元素拼接到数组末尾或者末尾一部分元素拼接到数组开头）</p>
<ol>
<li><p>$i &lt; nums[i]$ 即元素值大于索引，必须增大元素索引才能满足条件</p>
<ul>
<li>左边至少增加 $nums[i] - i$ ，也就意味着 轮调位置k必须在当前元素后面，且k<strong>后边元素个数</strong>必须大于等于当前元素左边至少要增加的元素,可得公式为 $ nums.length - k &gt;= nums[i] - i$ ，得到k的范围为<script type="math/tex; mode=display">
k \in [i+ 1, nums.length - nums[i] +i]</script></li>
</ul>
</li>
<li><p>$i &gt;= nums[i]$ 即索引值大于等于元素值，由于已经满足条件，可以如条件继续在左边增加元素（k在当前位置右边），或者从左边删掉一部分元素（k在当前位置左边）</p>
<ul>
<li><p>左边增加元素个数任意（k在当前位置右边），即k大于i时始终成立</p>
<script type="math/tex; mode=display">
k \in [i + 1, nums.length - 1]</script></li>
<li><p>左边删除一定数量元素，即最多$ i - nums[i]$，</p>
<script type="math/tex; mode=display">
k \in [0, i - nums[i]]</script></li>
</ul>
</li>
</ol>
<ul>
<li>最终第二种情况k范围为<script type="math/tex; mode=display">
k \in [0, i - nums[i]] \cup [i + 1, nums.length - 1]</script></li>
</ul>
<p>差分数组理解：</p>
<ol>
<li>差分数组元素定义为：<strong>differ[i] = nums[i] - nums[i - 1]</strong>，实际代表原数组的与前一个位置元素的差（可以理解为每个元素的参考都是前一个元素）</li>
<li>若要将原数组中<strong>某个区间内元素统一加某个值</strong>，在开始加 $differ[start] + number$，在结束后一个位置减  $differ[end + 1] - number$ </li>
</ol>
<p>如何更新差分数组：</p>
<ol>
<li><p>只需要执行两种操作，<strong>区间的左边界 + 1，区间的右边界右边-1</strong></p>
</li>
<li><p>两种情况的differ数组更新情况</p>
<ol>
<li>$i &lt; nums[i]$ <ul>
<li>differ[i + 1] + 1，differ[nums.length-nums[i] + i + 1] - 1</li>
</ul>
</li>
<li><p>$i &gt;= nums[i]$ </p>
<ul>
<li>differ[i + 1] + 1,右边界越界，不需要记录</li>
<li>differ[0] + 1, differ[i - nums[i] + 1] - 1</li>
</ul>
</li>
<li><p>$ (nums.length + i - nums[i]) % nums.length$ 可以等价与两个右边界情况</p>
</li>
</ol>
</li>
</ol>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">bestRotation</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] differ = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span> tempLow;</span><br><span class="line">        <span class="keyword">int</span> tempHigh;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; nums.length;i++)&#123;</span><br><span class="line">            tempLow = i + <span class="number">1</span>;</span><br><span class="line">            <span class="comment">// 精髓</span></span><br><span class="line">            tempHigh = (nums.length + i - nums[i]) % nums.length;</span><br><span class="line">            differ[tempLow]++;</span><br><span class="line">            differ[tempHigh + <span class="number">1</span>]--;</span><br><span class="line">            <span class="comment">//左右双开区间</span></span><br><span class="line">            <span class="keyword">if</span>(nums[i] &lt;= i)&#123;</span><br><span class="line">                differ[<span class="number">0</span>]++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i &lt; differ.length;i++)&#123;</span><br><span class="line">            differ[i] += differ[i - <span class="number">1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> score = differ[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i &lt; differ.length;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(differ[i] &gt; score)&#123;</span><br><span class="line">                result = i;</span><br><span class="line">                score = differ[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法相关</category>
        <category>力扣刷题</category>
      </categories>
      <tags>
        <tag>其他</tag>
      </tags>
  </entry>
  <entry>
    <title>lc869.重新排序得到2的幂</title>
    <url>/2021/11/21/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/lc869.%E9%87%8D%E6%96%B0%E6%8E%92%E5%BA%8F%E5%BE%97%E5%88%B02%E7%9A%84%E5%B9%82/</url>
    <content><![CDATA[<h3 id="869-重新排序得到-2-的幂"><a href="#869-重新排序得到-2-的幂" class="headerlink" title="869.重新排序得到 2 的幂"></a><a href="https://leetcode-cn.com/problems/reordered-power-of-2/">869.重新排序得到 2 的幂</a></h3><p>关键点是意识到2的幂次是有限</p>
<h3 id="解法1-暴力回溯法"><a href="#解法1-暴力回溯法" class="headerlink" title="解法1-暴力回溯法"></a>解法1-暴力回溯法</h3><p>看到重排序就想到回溯法中的排列树问题，按照排列树的标准模板求解即可,注意排除出现前导零的情况,与</p>
<p><strong>复杂度分析：</strong></p>
<ul>
<li>时间复杂度：由于第i层共有N- i个选择，<strong>时间复杂度为O(N!)</strong></li>
<li>空间复杂度：递归深度为数字的长度，<strong>空间复杂度为O(N)</strong></li>
</ul>
<p><strong>代码：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">backTrace</span><span class="params">(<span class="keyword">int</span> depth, <span class="keyword">int</span> nLength, Long curNumber, <span class="keyword">int</span>[] visited, <span class="keyword">int</span>[] digits)</span></span>&#123;</span><br><span class="line">        <span class="comment">//所有情况选取完毕</span></span><br><span class="line">        <span class="keyword">if</span>(depth == nLength)&#123;</span><br><span class="line">            <span class="keyword">return</span> isTwoPower(curNumber);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; nLength;i++)&#123;</span><br><span class="line">            <span class="comment">//未选用过当前位置数字,前导数字不能为0</span></span><br><span class="line">            <span class="keyword">if</span>(visited[i] == <span class="number">0</span> &amp;&amp; (depth != <span class="number">0</span> || digits[i] != <span class="number">0</span>))&#123;</span><br><span class="line">                visited[i] = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">if</span>(backTrace(depth + <span class="number">1</span>, nLength, curNumber * <span class="number">10</span> + digits[i] , visited, digits))</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                visited[i] = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h3 id="解法2-预先计算存储法"><a href="#解法2-预先计算存储法" class="headerlink" title="解法2-预先计算存储法"></a>解法2-预先计算存储法</h3><p>在数字的取值范围内，一共有$2^0, 2^1,……2^{29}$ 一共30个可能取到的2的幂次，事先计算所有2幂次数字各个位置的0~9 数字出现的次数，按照顺序生成字符串存储到Map中，对于目标数字，按照相同计算步骤生成字符串，查看Map中是否含有相同字符串即可。</p>
<p>不想写了贴个官方题解充个数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    Set&lt;String&gt; powerOf2Digits = <span class="keyword">new</span> HashSet&lt;String&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">reorderedPowerOf2</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        init();</span><br><span class="line">        <span class="keyword">return</span> powerOf2Digits.contains(countDigits(n));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> n = <span class="number">1</span>; n &lt;= <span class="number">1e9</span>; n &lt;&lt;= <span class="number">1</span>) &#123;</span><br><span class="line">            powerOf2Digits.add(countDigits(n));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">countDigits</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">char</span>[] cnt = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">10</span>];</span><br><span class="line">        <span class="keyword">while</span> (n &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            ++cnt[n % <span class="number">10</span>];</span><br><span class="line">            n /= <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> String(cnt);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">作者：LeetCode-Solution</span><br><span class="line">链接：https:<span class="comment">//leetcode-cn.com/problems/reordered-power-of-2/solution/zhong-xin-pai-xu-de-dao-2-de-mi-by-leetc-4fvs/</span></span><br><span class="line">来源：力扣（LeetCode）</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法相关</category>
        <category>力扣刷题</category>
      </categories>
      <tags>
        <tag>其他</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-1.如何在计算机中表示词语</title>
    <url>/2021/08/11/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/</url>
    <content><![CDATA[<img src="/2021/08/11/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210809091658017.png" class="" title="image-20210809091658017">
<p>想要通过计算机解决NLP问题，首先要解决的就是词语表示问题，由于一个词语在不同语境以及不同文化背景下含义的多样性，如何在计算机中有效的存储，表示不同词语的不同含义，是需要解决的重要问题。</p>
<h3 id="传统表示方式"><a href="#传统表示方式" class="headerlink" title="传统表示方式"></a>传统表示方式</h3><h4 id="WordNet-（discrete-representation）"><a href="#WordNet-（discrete-representation）" class="headerlink" title="WordNet （discrete representation）"></a>WordNet （discrete representation）</h4><blockquote>
<p>上义词是对事物的概括性、抽象性说明；下义词是事物的具体表现形式或更为具体的说明</p>
</blockquote>
<p>采用同义词（synonym）和上义词（hypernym）两个相关词语集合来描述当前词语的含义，当前方法一定程度上能够正确表示词语含义，但是存在一定问题</p>
<ol>
<li>忽略了词语在不同语境中的细微语义差异（比如 “中” 只有在河南话中和 “好” 是同义词）</li>
<li>词语的新的词义的添加较为困难</li>
<li>同义词和上义词定义较为主观，需要人工来整理两个词语集合</li>
</ol>
<img src="/2021/08/11/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210809091554618.png" class="" title="image-20210809091554618">
<span id="more"></span>
<h4 id="one-hot-词向量"><a href="#one-hot-词向量" class="headerlink" title="one-hot 词向量"></a>one-hot 词向量</h4><p>引入神经网络的向量思想，使用词的位置独热编码表示作为词在计算机中的表示形式，其中词向量的维度就是词典中词语的个数，这就导致了如果词典中词语数量增加，会导致词向量维度不断增加。</p>
<img src="/2021/08/11/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210809093045800.png" class="" title="image-20210809093045800">
<p>另一问题是所有独热编码表示的词语，均正交，无法表示词语之间的关联关系，由此引出 要将词语的相似度表示在词向量中的想法。</p>
<h4 id="Word-Vector"><a href="#Word-Vector" class="headerlink" title="Word Vector"></a>Word Vector</h4><blockquote>
<p> <strong>Distributional semantics</strong>:一个词语的含义由其上下文的词语所决定</p>
</blockquote>
<img src="/2021/08/11/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210809093839703.png" class="" title="image-20210809093839703">
<p>为了解决独热编码词向量的稀疏问题，引入分布式语义，根据上下文确定当前词语的dense vector，即Word Vector（Word Embedding）</p>
<h3 id="Word2Vec（跳词模型为例）"><a href="#Word2Vec（跳词模型为例）" class="headerlink" title="Word2Vec（跳词模型为例）"></a>Word2Vec（跳词模型为例）</h3><p>通过上下文相关性，确定当前词语的含义，即向量表示，这种相关性定义为某一单词上下文出现另一个单词的概率，如下图表示的为以C为中心词，周围出现单词o的概率。</p>
<img src="/2021/08/11/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210809095950546.png" class="" title="image-20210809095950546">
<p>为了方便计算为每个词典中的词语定义了 中心词向量$v_i$​ 和背景词$u_i$​​两个特征向量。根据由中心词确定上下文的思想，因此整个上下文句子的似然函数为</p>
<script type="math/tex; mode=display">
L(\theta) = \prod^T_{t=1}\prod_{-m<j<m 且j\neq0}P(w_{t+j}|w_t,\theta)\tag{1}</script><p>即以上下文中所有词语作为中心词，在窗口大小为2m的条件下，出现背景词概率乘积的连，对应的损失函数形式为</p>
<script type="math/tex; mode=display">
J(\theta) = -\frac{1}{T}log(L(\theta))= -\frac{1}{T}\sum^T_{t=1}\sum_{-m<j<m 且j\neq0}log(P(w_{t+j}|w_t,\theta))\tag{2}</script><p>带入概率公式求损失函数（2）关于特定中心词向量$v_c$​​​​​的偏导数，可得</p>
<script type="math/tex; mode=display">
\frac{\partial J(\theta)}{\partial v_c} = \sum_{-m<j<m 且j\neq0} [u_{c+j} - \sum_{w \in v} P(w|c)u_w]\tag{3}</script><p>即损失函数以中心词$v_c$​ 背景词$u_{c+j}$​​​​的部分偏导数为：</p>
<ul>
<li><strong>当前背景词的背景向量 - 当前词库所有背景词以$v_c$为中心词概率分布的背景向量$u$的加权平均和</strong></li>
</ul>
<h4 id="为什么设置两个向量（背景向量-u-和-中心向量-v-）"><a href="#为什么设置两个向量（背景向量-u-和-中心向量-v-）" class="headerlink" title="为什么设置两个向量（背景向量$u$ 和 中心向量 $v$）"></a>为什么设置两个向量（背景向量$u$ 和 中心向量 $v$）</h4><p>为了便于损失函数求偏导计算，在真正使用时，使用两个向量的平均值作为词语的表征向量</p>
<h4 id="如何解决计算量过大的问题"><a href="#如何解决计算量过大的问题" class="headerlink" title="如何解决计算量过大的问题"></a>如何解决计算量过大的问题</h4><p>由条件概率公式可得，分母的计算需要遍历整个语料库，计算量过大</p>
<h5 id="随机梯度下降（Stochastic-Gradient-Descent）"><a href="#随机梯度下降（Stochastic-Gradient-Descent）" class="headerlink" title="随机梯度下降（Stochastic Gradient Descent）"></a>随机梯度下降（Stochastic Gradient Descent）</h5><p>按照特定抽样方法，每个训练epoch从语料库中抽取一部分的作为当前训练语料库，抽样能否代表整体，取决于抽样方法的设计</p>
<img src="/2021/08/11/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811084901573.png" class="" title="image-20210811084901573">
<h5 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h5><img src="/2021/08/11/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811093414764.png" class="" title="image-20210811093414764">
<blockquote>
<p>样本量大时，k一般选择2-5,样本量小时，k一般选择5-20</p>
</blockquote>
<p>负采样通过两个思路解决计算量较大的问题：</p>
<ol>
<li>将概率公式从softmax替换为sigmoid</li>
<li>概率公式的计算范围为负采样空间内</li>
</ol>
<p>以词袋模型为例，当 $context(w)$ 预测 $w$ 时，以{ $context(w)$ , $w$ }作为正样本，并从语料库库中选取k个非 $w$ 词语，与 $w$ 上下文构成{ $context(w)$ , $w_i$​​​​ }负样本，以正负样本集对模型进行训练。</p>
<p>替换后的概率公式（D=1或0 1表示该情况出现，0表示该情况不出现）</p>
<script type="math/tex; mode=display">
P(D=1|w_c,w_o) = \sigma(u_o^Tv_c)\tag{4}</script><p>其中 $\sigma$ 为sigmoid函数</p>
<script type="math/tex; mode=display">
\sigma(u_o^Tv_c) = sigmoid(u_o^Tv_c) = \frac{1}{1 + exp(-u_o^Tv_c)}\tag{5}</script><p>新的概率计算公式为,P(w)为随机负采样的概率分布</p>
<script type="math/tex; mode=display">
p(o|c) = p(D=1|w_c,w_o)\prod_{k=1,k\in P(w)}p(D=0|w_c,w_k)</script><p>推导可得</p>
<script type="math/tex; mode=display">
-log(p(o|c)) = -log[\sigma(u_o^Tv_c)]-\sum_{k=1,k\in P(w)}log(1- \sigma(u_k^Tv_c))</script><p>随之而来的问题是<strong>如何进行负采样</strong>才能保证训练是有效的？（trick）</p>
<blockquote>
<p> word2vec论文作者通过观察测试发现最佳采样概率分布是采用特殊的独立分布，概率公式为$p(w_i) = \frac{count(wi)^{\frac{3}{4}}}{\sum_{j= 0}^{total}w_j}$</p>
</blockquote>
<h5 id="层序softmax"><a href="#层序softmax" class="headerlink" title="层序softmax"></a>层序softmax</h5><p>首先按照单词在语料库中出现的频率构建haffman树，每个叶子节点为语料库中的词语，将softmax概率函数转化为每个二叉树节点的二分类逻辑回归（sigmoid）函数，我们的目标就是最大化从根节点到目标词语叶子节点路径的似然函数。（类似于负采样的概率函数）</p>
<img src="/2021/08/11/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811091807339.png" class="" title="image-20210811091807339">
<h4 id="词袋模型的概率公式"><a href="#词袋模型的概率公式" class="headerlink" title="词袋模型的概率公式"></a>词袋模型的概率公式</h4><p>已经知上下文，推导中心词，概率公式如下(窗口大小为2m)</p>
<script type="math/tex; mode=display">
p(w_c|w_{o1},......,w_{o2m}) = \frac{exp(\frac{1}{2m} u_c^T(v_{o1},......v_{om}))}{\sum_{i\in window}exp(\frac{1}{2m} u_i^T(v_{o1},......v_{om}))}</script><p>即背景词出现情况下中心词出现的条件概率（背景词向量取了平均值）</p>
<h3 id="词共现矩阵（co-occurence-matrix）"><a href="#词共现矩阵（co-occurence-matrix）" class="headerlink" title="词共现矩阵（co-occurence matrix）"></a>词共现矩阵（co-occurence matrix）</h3><p>首先限定句子窗口长度后，统计每个词在窗口长度内其他词出现的次数，记录在共现矩阵中，记录完成后以行作为每个词的特征向量</p>
<img src="/2021/08/11/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811094558672.png" class="" title="image-20210811094558672">
<p>虽然一定程度上解决了独热编码的过于系数，无法表示词语之间关系的问题，仍存在向量维度爆炸，且过于稀疏的问题，课程中提发了集中解决方案：</p>
<ol>
<li>使用SVD分解后的矩阵作为特征向量，以降低向量维度</li>
</ol>
<h3 id="Glove"><a href="#Glove" class="headerlink" title="Glove"></a>Glove</h3><p>Glove算法通过结合传统计数思路对word2vec模型进行了改进，词与词之间的关系不再通过神经网络式的预测模拟，而是通过拟合词与词之间共同出现的条件概率，实现词向量的构建。</p>
<img src="/2021/08/11/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811100512861.png" class="" title="image-20210811100512861">
<p>Glove为了实现计算出来向量满足线性计算，将 两个词语的词向量乘积 与  共现条件概率 对应起来</p>
<img src="/2021/08/11/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811104406982.png" class="" title="image-20210811104406982">
<p>通过拟合共现条件概率，实现词向量的学习</p>
<img src="/2021/08/11/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811104602858.png" class="" title="image-20210811104602858">
<p>动手学习机器学习中的讲解（另一个角度理解）</p>
<img src="/2021/08/11/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-1.%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD/image-20210811104020990.png" class="" title="image-20210811104020990">
<h3 id="如何评估一组词向量"><a href="#如何评估一组词向量" class="headerlink" title="如何评估一组词向量"></a>如何评估一组词向量</h3><ol>
<li>通过后续任务（文本分类，问答系统）等的效果评估</li>
<li>词向量是否易于构建（维度低，效果好）</li>
<li>通过余弦相似度，衡量词语之间的相似度</li>
</ol>
]]></content>
      <categories>
        <category>AI理论</category>
        <category>cs224n</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-10.上下文词嵌入</title>
    <url>/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/</url>
    <content><![CDATA[<h2 id="课程大纲"><a href="#课程大纲" class="headerlink" title="课程大纲"></a>课程大纲</h2><img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211013201952293.png" class="" title="image-20211013201952293">
<h2 id="预训练到来之前"><a href="#预训练到来之前" class="headerlink" title="预训练到来之前"></a>预训练到来之前</h2><h3 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h3><p>迄今为止，我们主要使用包括Word2Vec，Glove等词嵌入向量，通过大量的数据预训练，提供给下游任务作为单词输入</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211013202450741.png" class="" title="image-20211013202450741">
<p>之后我们遇到了 OOV(out of dic)和各种英语后缀不同导致单词含义不同得问题，传统预训练向量难以解决，引入了character-level的向量嵌入，如fastText。</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211013203324779.png" class="" title="image-20211013203324779">
<p>然而以往的词向量存在<strong>两个问题</strong></p>
<ul>
<li>以往的词向量嵌入<strong>不考虑上下文语境信息（或者说只考虑一种固定的语境）</strong>，仅仅是固定的一个词向量，应用于不同的下游任务</li>
<li>一个词不止有字面意思，还有包括词性，语法以及适用的语境等其他隐含信息，以往词向量只考虑了词的字面含义</li>
</ul>
<span id="more"></span>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211013204802812.png" class="" title="image-20211013204802812">
<h3 id="TagLM-“pre-EMLO”"><a href="#TagLM-“pre-EMLO”" class="headerlink" title="TagLM-“pre-EMLO”"></a>TagLM-“pre-EMLO”</h3><p>出发点：想要获得拥有上下文信息的词嵌入向量，但是又不想使用大量的标记数据</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211013211127036.png" class="" title="image-20211013211127036">
<p>TagLM模型使用大量无标签数据，训练一个嵌入模型和一个基于RNN的语言模型，用两个模型的输出作为当前词语的嵌入向量，输入到词性标注模型中进行监督训练。</p>
<ul>
<li><strong>嵌入模型</strong>由字符集嵌入和词嵌入两个模型共同生成两个嵌入向量拼接，输入第一层双向RNN</li>
<li><strong>语言模型</strong>，由大量的无标签数据预训练后，输入上下文嵌入后与RNN第一层隐藏层输出拼接输入第二层</li>
<li>预训练<strong>语言模型</strong>，在预训练完成后就<strong>冻结</strong></li>
</ul>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211013212332139.png" class="" title="image-20211013212332139">
<h3 id="EMLO"><a href="#EMLO" class="headerlink" title="EMLO"></a>EMLO</h3><p>ELMo 采用语言模型的思路，不输出固定的词向量表述，而是按照语言模型的方式根据上下文和当前词语，输入当前此与的向量表示。</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211013213914902.png" class="" title="image-20211013213914902">
<p>ELMO使用了多层堆叠的双向LM（LSTM），与传统只选取最后一层隐藏状态输出不同，ELMO认为不同层抽出不同的特征信息（隐藏状态），使用各个层的隐藏状态加权平均，作为输出。</p>
<ul>
<li>加权平均的不同层权重 $s^{task}_j$ 以及学习率 $\gamma^{task}$ 取值，由下游任务(task)决定</li>
</ul>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211014162907326.png" class="" title="image-20211014162907326">
<p>ELMo可以有不同的深度，提取不同深度的特征信息，适用于对应的问题</p>
<ul>
<li>浅层网络：Part-of-speech(词性标注)，syntactic dependencies（句法依存），NER（命名实体识别）</li>
<li>深层网络：sentiment（情感），Semantic role labeling(SRL 语义角色标注)，question andwering（阅读理解），SNLI（自然语言推理）</li>
</ul>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211014164651518.png" class="" title="image-20211014164651518">
<h3 id="ULMfit-基于迁移学习思路"><a href="#ULMfit-基于迁移学习思路" class="headerlink" title="ULMfit-基于迁移学习思路"></a>ULMfit-基于迁移学习思路</h3><img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211014182546795.png" class="" title="image-20211014182546795">
<p>ULMfit模型同样使用语言模型做词嵌入，共分为三步（文本情感分类为例）</p>
<ol>
<li>使用大量的领域数据对语言模型进行预训练</li>
<li>使用目标任务输入文本对训练好的语言模型进行微调（fine-tuning）</li>
<li>在模型的最后一层添加两个情感分类神经元</li>
</ol>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211014183233285.png" class="" title="image-20211014183233285">
<h2 id="预训练迁移模型-爆发"><a href="#预训练迁移模型-爆发" class="headerlink" title="预训练迁移模型 爆发"></a>预训练迁移模型 爆发</h2><img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211014184204582.png" class="" title="image-20211014184204582">
<h3 id="Transformers简介"><a href="#Transformers简介" class="headerlink" title="Transformers简介"></a>Transformers简介</h3><p>为了解决RNN无法并行计算的问题，Transformer结合之前模型的经验，实现了高度的并行性</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-10.%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%8D%E5%B5%8C%E5%85%A5/image-20211014185419211.png" class="" title="image-20211014185419211">
]]></content>
      <categories>
        <category>AI理论</category>
        <category>cs224n</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-11.Transformers</title>
    <url>/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/</url>
    <content><![CDATA[<h2 id="主要概念理解"><a href="#主要概念理解" class="headerlink" title="主要概念理解"></a>主要概念理解</h2><h3 id="self-Attention-自注意力机制"><a href="#self-Attention-自注意力机制" class="headerlink" title="self-Attention 自注意力机制"></a>self-Attention 自注意力机制</h3><blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/265108616">注意力机制介绍</a></p>
</blockquote>
<p>自注意力机制与注意力不同点在于，自注意力机制在序列内部进行注意力的计算，每个词与相邻词进行F(Q,K)计算，当前词作为Q，临近词作为K，计算主力注意力分数后，进行嵌入向量的加权平均。</p>
<ul>
<li>一个词语的含义取决于上下文临近词语的含义（语境），自注意力机制解决了RNN<strong>无法抽取长距离信息</strong>的问题</li>
<li>自注意力机制的计算可以通过矩阵运算实现并行化，解决了RNN<strong>无法并行优化</strong>的问题</li>
</ul>
<span id="more"></span>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211016162648608.png" class="" title="image-20211016162648608">
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017152513315.png" class="" title="image-20211017152513315">
<p><strong>self-attention过程简单总结</strong></p>
<ol>
<li>为每个输入词语随机初始化<strong>Query、Key、Value</strong>三个<strong>向量</strong>，其中，Value与注意力分数相乘做加权平均，输出结果</li>
<li>当需要计算某个词语表示时候，以当前词语的Query和相邻词Key（包括自己）用来计算注意力分数</li>
<li>使用上一步计算的注意力分数，输入softmax归一化，对相邻词的Value向量进行加权平均后输出当前词的向量表示</li>
</ol>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211016170802649.png" class="" title="image-20211016170802649">
<h3 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h3><p>降维思想，将原来的Q,K,V映射到多个不同得子空间(即一系列得$Q_i K_i V_i$ ,transformers中为八个)，这种映射通过矩阵乘法实现，每个一head维护一组$W_{iQ} W_{iK} W_{iV}$，通过单头中的$Q_i K_i V_i$ 与对应权重矩阵相乘得到对应得$Q_i K_i V_i$</p>
<ul>
<li>$QKV \in R^{1*512}$ </li>
<li>$W_{iQ}W_{iK}W_{iV} \in R^{512*64}$ </li>
<li>$Q*W_{iQ} = Q_i$ </li>
<li>可得 $Q_i K_i V_i \in R^{1*64}$，共计8个不同head，实现了从高维到低维得转换，但是没有增加参数数量。</li>
</ul>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017103444937.png" class="" title="image-20211017103444937">
<p>在不同得子空间上进行注意力机制得计算后，进行拼接乘以权重矩阵获得多头注意力机制的输出</p>
<p>​                        <script type="math/tex">MultiAttention(Q, K, V) = concat(head_1,......,head_2) * W_{out}</script>  其中 $W_{out} \in R^{512*512}$</p>
<p>相当于 输出结果得每个位置都是由所有参数加权平均得到</p>
<p><strong>为什么要使用多头注意力机制</strong></p>
<ul>
<li>通过将QKV映射到不同的子空间，实现了从不同角度对于当前词语含义的捕捉，而在不同的翻译任务中，我们对不同角度的重视程度不同（例如 <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> 举得it的例子），这种映射有助于模型自适应的捕捉特征，提升NMT的效果。</li>
<li>我觉得这就是个trick，到底有什么用也没人说清楚了（对黑盒算法强行解释），能用好用就完事了</li>
</ul>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017105229086.png" class="" title="image-20211017105229086">
<h2 id="网络结构理解"><a href="#网络结构理解" class="headerlink" title="网络结构理解"></a>网络结构理解</h2><img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017151555753.png" class="" title="image-20211017151555753">
<h3 id="Postion-Encoding-位置编码"><a href="#Postion-Encoding-位置编码" class="headerlink" title="Postion-Encoding 位置编码"></a>Postion-Encoding 位置编码</h3><p>transformer在对输入词语进行embedding后，由于transformer舍弃了RNN的时间序列输入形式，整个序列中不同位置的单词在输入时的地位是相同的，<strong>通过位置编码重赋予输入位置特征</strong>。</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017110556688.png" class="" title="image-20211017110556688">
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017110713718.png" class="" title="image-20211017110713718">
<p>位置编码存在两种计算方式</p>
<ol>
<li><p>通过固定的公式，根据字符在序列中的位置，计算得到 (<a href="https://www.zhihu.com/question/347678607/answer/864217252">公式理解</a>),总结就是又要体现不同位置的差异性，又不能影响embedding的词语含义。</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017111718315.png" class="" title="image-20211017111718315">
</li>
<li><p>随机初始化pos编码，通过模型学习生成对应编码</p>
</li>
</ol>
<h3 id="LayerNormalization层"><a href="#LayerNormalization层" class="headerlink" title="LayerNormalization层"></a>LayerNormalization层</h3><p>LN方法与BN方法类似，在一条数据不同特征之间进行归一化处理（BN对不同数据的同一特征归一化处理），由于RNN类似于一个不固定batchsize(时间步)的CNN网络，无法使用BN方法，采用LN方法一定程度上也可以实现BN方法的效果。</p>
<h3 id="Feed-Forward（FFN）层"><a href="#Feed-Forward（FFN）层" class="headerlink" title="Feed Forward（FFN）层"></a>Feed Forward（FFN）层</h3><p>类似于两个卷积核大小为1的卷积操作，目的是为了应用非线性激活函数RELU，增加<strong>非线性</strong>性质。</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-11.Transformers/image-20211017150618854.png" class="" title="image-20211017150618854">
<h3 id="Decoder端"><a href="#Decoder端" class="headerlink" title="Decoder端"></a>Decoder端</h3><p>与encoder段基本一致，每个decoder块每部增加了一个 “encoder-decoder attention” 注意力机制，以encoder输出的K，V矩阵为输入，和Encoder当前词的Q进行注意力机制运算。</p>
<p><img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif" alt="img"></p>
<p>基本流程如图</p>
<p><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif" alt="img"></p>
]]></content>
      <categories>
        <category>AI理论</category>
        <category>cs224n</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-12.自然语言生成</title>
    <url>/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/</url>
    <content><![CDATA[<h3 id="内容回顾"><a href="#内容回顾" class="headerlink" title="内容回顾"></a>内容回顾</h3><h4 id="Beam-Search：不同K的区别"><a href="#Beam-Search：不同K的区别" class="headerlink" title="Beam Search：不同K的区别"></a>Beam Search：不同K的区别</h4><ol>
<li>小K值可能导致生成序列效果较差（语法上、语义上、流畅度上）</li>
<li>大K值虽然能够解决以上问题，但是会带来更多的计算成本，一定程度上减低NMT任务中的BLUE分数<ul>
<li>开放式问答任务中，更大的K可能导致 回答更加的宽泛（与原问题的关联度更低）</li>
</ul>
</li>
</ol>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019145744076.png" class="" title="image-20211019145744076">
<p><strong>其他的解决方案</strong></p>
<ol>
<li><strong>Sampling-based decoding</strong>（与Beam search不同在于decoder每一步只需要追踪一个词）<ul>
<li>Pure Sampling：每次随机选择概率分布中的某一个词，作为decoder输出词</li>
<li>Top-n Sampling：每次从前N个概率大小词语中选择某一个词，作为decoder输出词</li>
</ul>
</li>
</ol>
<h4 id="Softmax-temperature-带温度系数的Softmax方法"><a href="#Softmax-temperature-带温度系数的Softmax方法" class="headerlink" title="Softmax temperature: 带温度系数的Softmax方法"></a>Softmax temperature: 带温度系数的Softmax方法</h4><blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/132785733">$\tau$解释</a></p>
</blockquote>
<p>在原始的Softmax函数中添加 <strong>temperature hyperparameter: $\tau$</strong> .</p>
<ol>
<li>$\tau$ 起到一种平滑作用， $\tau$ 越大softmax计算得到的概率分布越平滑，t越小分布越不均匀</li>
<li>在训练开始将 $\tau$ 值设置较大，概率分布较为平滑，loss较大可以避免模型落入局部最优解，随着训练的进行，不断增大 $\tau$ 值，从而提升模型的效果。（某一个 $\tau$ 值并不影响模型的结果）</li>
</ol>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019151158168.png" class="" title="image-20211019151158168">
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019151618394.png" class="" title="image-20211019151618394">
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019151751505.png" class="" title="image-20211019151751505">
<span id="more"></span>
<h3 id="NLG"><a href="#NLG" class="headerlink" title="NLG"></a>NLG</h3><p>NLG主要解决的问题就是，给定输入文本，对文本进行分析和抽取，输出我们需要的指定文本信息（summary或者再创作）</p>
<ol>
<li>输入可以是单个文档，也可以是多个文档（多个文档通常内容是相关的）</li>
</ol>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019152628788.png" class="" title="image-20211019152628788">
<p>目前NLG任务领域的主要数据集有</p>
<ol>
<li><p>single-document任务</p>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019153334719.png" class="" title="image-20211019153334719">
</li>
<li><p>句子简化（sentence simplification）</p>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019153513309.png" class="" title="image-20211019153513309">
</li>
</ol>
<p>NLG任务的<strong>两种</strong>主要实现方式</p>
<ol>
<li>抽取性（extractive）摘要</li>
<li>生成式（abstractive）摘要</li>
</ol>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019153742775.png" class="" title="image-20211019153742775">
<h4 id="神经网络之前的NLG"><a href="#神经网络之前的NLG" class="headerlink" title="神经网络之前的NLG"></a>神经网络之前的NLG</h4><p>一般的单文本摘要流程如下</p>
<ol>
<li>首先从原文中选取特定的句子，作为摘要的句子组成内容（content selecting）</li>
<li>对选取的句子进行排序（Information Ordering）</li>
<li>对句子进行调整和修改（Sentence realization）</li>
</ol>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019154611084.png" class="" title="image-20211019154611084">
<p>重点在于<strong>如何选择句子</strong></p>
<ol>
<li>基于算法对句子评分，选取评分高的句子作为候选摘要内容<ul>
<li>根据关键词出现频率，如tf-idf</li>
<li>句子在文章中出现的位置</li>
</ul>
</li>
<li>基于图算法（句子视为节点，句子之间的相似度视为边）</li>
</ol>
<h4 id="摘要评价-ROUGE"><a href="#摘要评价-ROUGE" class="headerlink" title="摘要评价-ROUGE"></a>摘要评价-ROUGE</h4><img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019164507272.png" class="" title="image-20211019164507272">
<p>ROUGE（Recall-Oriented Understudy for Gisting Evaluation）与BLUE类似，均基于n-gram共现评估生成文本与目标摘要文本的差距</p>
<ol>
<li>ROUGE在计算的过程中引入了召回率的概念，相较于NMT任务，摘要任务对召回率的重视程度更高</li>
</ol>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019164852685.png" class="" title="image-20211019164852685">
<p>一种ROUGE评分只针对一种n-gram语法</p>
<ol>
<li>ROUGE-1: unigram overlap</li>
<li>ROUGE-2: bigram overlap</li>
<li>ROUGE-L: LCS overlap</li>
</ol>
<h4 id="Neural-Summarization-（2015-）发展历史"><a href="#Neural-Summarization-（2015-）发展历史" class="headerlink" title="Neural Summarization （2015-）发展历史"></a>Neural Summarization （2015-）发展历史</h4><ol>
<li><p>第一次使用seq2seq + attention实现单文档摘要</p>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211019170212035.png" class="" title="image-20211019170212035">
</li>
<li><p>2015后出现的一系列解决方案</p>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211020112720437.png" class="" title="image-20211020112720437">
</li>
</ol>
<h4 id="Copy-Mechanisms：解决seq2seq局限于特征"><a href="#Copy-Mechanisms：解决seq2seq局限于特征" class="headerlink" title="Copy Mechanisms：解决seq2seq局限于特征"></a>Copy Mechanisms：解决seq2seq局限于特征</h4><p>由于seq2seq机制中decoder基于encoder输出特征进行生成，无法满足<strong>保留源语句中某些关键词</strong>的要求（虽然fluent，但不够imformative），通过Copy Mechanisms解决这个问题。</p>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211020113937063.png" class="" title="image-20211020113937063">
<p>Copy Mechanisms 通过attention机制，将input中的某些词或者某些句子，<strong>直接作为output中的内容输出</strong>，通过与seq2seq结合，更好的适应文档摘要问题</p>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211020114155828.png" class="" title="image-20211020114155828">
<p>几个复制机制的实现</p>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211020114245710.png" class="" title="image-20211020114245710">
<p>存在的问题（单词级别的复制机制）</p>
<ol>
<li>复制的度难以把握，容易退化成抽取算法</li>
<li>复制时难以考虑整篇文章的信息，当文章过长时，复制效果不理想</li>
<li>如何选择，没有一个确定的策略</li>
</ol>
<p><strong>Bottom up summarization</strong></p>
<p>解决单词级别复制问题，将摘要过程分为两个阶段</p>
<ol>
<li>内容选择阶段: 使用标注模型，对原文章中的词进行标记是否需要</li>
<li>摘要阶段：同样的seq2seq+attention摘要模型，只是对上一步标记的不需要词语进行掩码覆盖，不参与预测</li>
</ol>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211023091146567.png" class="" title="image-20211023091146567">
<h3 id="Dialogue-对话生成任务"><a href="#Dialogue-对话生成任务" class="headerlink" title="Dialogue 对话生成任务"></a>Dialogue 对话生成任务</h3><p>对话生成任务的范围较为广泛，从日常对话、智能客服到辩论等，主要包括任务型对话和社会对话两种</p>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211023094731197.png" class="" title="image-20211023094731197">
<h4 id="pre-neural"><a href="#pre-neural" class="headerlink" title="pre-neural"></a>pre-neural</h4><p>由于开放式文档问题难度较高，神经网络应用之前的系统往往使用模板式回答或者从一系列语料中寻找回答的方式，实现对话。</p>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211023095226942.png" class="" title="image-20211023095226942">
<h4 id="seq2seq-based-dialogue"><a href="#seq2seq-based-dialogue" class="headerlink" title="seq2seq-based dialogue"></a>seq2seq-based dialogue</h4><p>虽然很会人们就发现seq2seq模型可以应用于解决对话生成任务，但是目前随着应用的开展，发现存在一系列的问题</p>
<ol>
<li>回答宽泛（虽然给了答案，但是答案没什么用） Genericness</li>
<li>回答与上下文不相关 Irrelevant</li>
<li>不断地重复 Repetition</li>
<li>缺乏上下文信息 （对话的上下文，对话的人等）</li>
</ol>
<p><strong>解决irrelevant问题</strong></p>
<ol>
<li>生成的一般回答，而不是想要的回答 （i don’t know）</li>
<li>回答的主题与问题不相关</li>
</ol>
<p>传统NN模型的优化目标往往是最大化  $p(Target|Source)$ ，即给定目前问题下得到对应答案的条件概率；当前解决方案采用互信息MMI作为优化目标\</p>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211023101005268.png" class="" title="image-20211023101005268">
<p>在公式上体现为原条件概率减去了当前回答出现的概率（形式上理解为减去回答概率，对在预料中出现频率较高的通用回答进行了惩罚）</p>
<p><strong>解决genericness问题</strong></p>
<img src="/2021/11/21/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-12.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90/image-20211023101625189.png" class="" title="image-20211023101625189">
<p><strong>解决repetition问题</strong></p>
<p>简单解决方案: 对于decoder输出的n-gram,禁止n-gram句子中存在相同词语。（简单而粗暴）</p>
<p>复杂方案</p>
<ol>
<li>避免注意力机制计算时重复的计算相同背景词（从源头铲除相同词语）</li>
<li>设置训练目标，减少重复出现</li>
</ol>
<h3 id="NLG-Evaluation"><a href="#NLG-Evaluation" class="headerlink" title="NLG Evaluation"></a>NLG Evaluation</h3><p>传统基于词覆盖的评价方式，例如BLUE、ROGUE等，并不适用于NLG任务（或者说无法真正反映一个NLG解决方案的好坏），既然无法找到一种衡量NLG任务好坏的方式，我们不如聚焦于生成结果的某一个方面</p>
<ul>
<li>流利性</li>
<li>正确的风格</li>
<li>多样性</li>
<li>相关输入</li>
<li>简单的长度和重复</li>
<li>特定于任务的指标，如摘要的压缩率</li>
</ul>
<p>人类评估（省略）</p>
<h3 id="NLG发展方向"><a href="#NLG发展方向" class="headerlink" title="NLG发展方向"></a>NLG发展方向</h3><ul>
<li>将离散潜在变量纳入NLG（不同任务可能有不同的潜在特征）<ul>
<li>可以帮助在真正需要它的任务中建模结构，例如讲故事，任务导向对话等</li>
</ul>
</li>
<li>严格的从左到右生成的替代方案<ul>
<li>并行生成，迭代细化，自上而下生成较长的文本</li>
</ul>
</li>
<li>替代teacher forcing的模型训练方法<ul>
<li>更全面的句子级别的目标函数（而不是单词级别）</li>
</ul>
</li>
</ul>
<p>目前NLG发展</p>
<ul>
<li>在NLP+深度学习的早期，社区主要将成功的机器翻译方法转移到NLG任务中。</li>
<li>现在，越来越多的创新NLG技术出现，针对非NMT生成环境。</li>
<li>越来越多（神经）NLG研讨会和竞赛，特别关注开放式NLG<ul>
<li>NeuralGen workshop</li>
<li>Storytelling workshop</li>
<li>Alexa challenge</li>
<li>ConvAI2 NeurIPS challenge</li>
</ul>
</li>
<li>这些对于组织社区提高再现性、标准化评估特别有用</li>
<li>最大障碍是评估</li>
</ul>
]]></content>
      <categories>
        <category>AI理论</category>
        <category>cs224n</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-2.使用神经网络解决nlp问题</title>
    <url>/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h3 id="神经网络介绍"><a href="#神经网络介绍" class="headerlink" title="神经网络介绍"></a>神经网络介绍</h3><p>神经网络介绍部分省略</p>
<h3 id="命名实体识别（NER）"><a href="#命名实体识别（NER）" class="headerlink" title="命名实体识别（NER）"></a>命名实体识别（NER）</h3><p>标注中句子中的目标词性词语-实体，其中实体是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/image-20210831093150777.png" class="" title="image-20210831093150777">
<p>命名实体存在的困难：</p>
<ol>
<li>难以确定实体的上下文边界（王小明 还是 小明）</li>
<li>难以确定词语是否为 实体</li>
<li>实体的具体含义依赖于上下文，难以确定</li>
<li>难以辨别不知道的实体（特定语境 特定环境下的某些词作为实体）</li>
</ol>
<span id="more"></span>
<h4 id="1-Binary-word-window-classification"><a href="#1-Binary-word-window-classification" class="headerlink" title="1.Binary word window classification"></a>1.Binary word window classification</h4><p>由于词语根据语境等具有不同的含义和词性，单独识别句子中的某个单词是不现实的，使用一个上下文窗口对词语进行分类</p>
<p><strong>思路：</strong> 使用上下文窗口中的背景词对当前词进行分类</p>
<ul>
<li><p>对窗口内词向量进行平均，作为分类输入。<strong>问题：</strong>忽略了位置信息和词语之间的关系</p>
</li>
<li><p>使用窗口内词向量拼接向量作为输入</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/image-20210831095343422.png" class="" title="image-20210831095343422">
</li>
</ul>
<p>训练分类函数</p>
<ol>
<li><p>softmax 或者 交叉熵函数，通过反向传播更新词向量和权重</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/image-20210831095949882.png" class="" title="image-20210831095949882">
</li>
<li><p>unnormarlized score</p>
<ul>
<li><p>选取一个正例窗口和一个反例窗口</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/image-20210831100250180.png" class="" title="image-20210831100250180">
</li>
<li><p>将拼接的窗口向量输入到具有一个隐藏层和一个全连接输出层的神经网络，输出一个评价分数（score)</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/image-20210831100708874.png" class="" title="image-20210831100708874">
<ul>
<li><strong>添加隐藏层的目的</strong>在于通过隐藏层的非线性激活函数，学习输入与输出之间的非线性关系（non-linear interaction）</li>
</ul>
</li>
<li><p>训练目标为：正例窗口分数最大，负例窗口分数最小</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/image-20210901090231216.png" class="" title="image-20210901090231216">
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-2.%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3nlp%E9%97%AE%E9%A2%98/image-20210901090217098.png" class="" title="image-20210901090217098">
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>AI理论</category>
        <category>cs224n</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-3.依存分析问题</title>
    <url>/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h4 id="句法分析"><a href="#句法分析" class="headerlink" title="句法分析"></a>句法分析</h4><p>句法分析与上下文无关文法相对立，强调通过对于句子语法结构的分析，实现对于句子的理解。最常见的三种句法分析任务如下</p>
<ol>
<li>句法结构分析 识别句子中的短语结构和层次关系</li>
<li>依存关系分析 识别句子中词与词之间的依存关系，确定词语的含义</li>
<li>深层文法句法分析 利用深层文法对句子进行分析</li>
</ol>
<h4 id="依存句法分析（Dependency-Parsing）"><a href="#依存句法分析（Dependency-Parsing）" class="headerlink" title="依存句法分析（Dependency Parsing）"></a>依存句法分析（Dependency Parsing）</h4><p>依存结构展示了句子中依赖于其他词语的单词，这种依赖体现为被修饰或者被限定</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909082635347.png" class="" title="image-20210909082635347">
<span id="more"></span>
<p>同一个句子，在不同的依存关系下，理解的含义可能有所不同</p>
<ol>
<li>介词修饰歧义（preposition attachment ambiguity）(刀杀死了男人，还是杀死了带刀的男人）</li>
</ol>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909083302896.png" class="" title="image-20210909083302896">
<ol>
<li><p>修饰范围歧义（Coordination scope ambiguity）</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909084806696.png" class="" title="image-20210909084806696">
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909084821340.png" class="" title="image-20210909084821340">
</li>
<li><p>形容词修饰歧义</p>
</li>
</ol>
<h4 id="依存句法（Dependency-Grammar）"><a href="#依存句法（Dependency-Grammar）" class="headerlink" title="依存句法（Dependency Grammar）"></a>依存句法（Dependency Grammar）</h4><p>依存句法假设句子中的词语存在语法结构上的关联，这种通常为非对称的二元关联关系成为依赖（Dependency）</p>
<ol>
<li>依存语法树</li>
</ol>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909090638171.png" class="" title="image-20210909090638171">
<ol>
<li><p>直接在句子上标注</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909092703216.png" class="" title="image-20210909092703216">
</li>
</ol>
<p><strong>依赖关系的一般约束</strong></p>
<ol>
<li>依赖不循环</li>
<li>相依赖的词语一般距离较近</li>
<li>依赖项一般能够构成树形结构</li>
</ol>
<p><strong>依存句法分析的基本方法</strong></p>
<ol>
<li>Dynamic programming</li>
<li>Graph algorithms</li>
<li>Constraint Satisfaction</li>
<li>“Transition-based parsing” or “deterministic dependency parsing”</li>
</ol>
<h4 id="Greedy-transition-based-parsing-基于贪婪思想转换的依存分析"><a href="#Greedy-transition-based-parsing-基于贪婪思想转换的依存分析" class="headerlink" title="Greedy transition-based parsing 基于贪婪思想转换的依存分析"></a>Greedy transition-based parsing 基于贪婪思想转换的依存分析</h4><img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909103541425.png" class="" title="image-20210909103541425">
<p>简单理解就是 从句子的开头逐个单词进行压栈，判断即将入栈元素和栈顶元素的依存关系，根据依存关系在边集合<script type="math/tex">arcs A</script> 添加对应方向的边，弹出栈顶元素（reduce）(如果不存在关系，不弹)，并将当前元素压栈（shift）</p>
<p><strong>Arc-Standard transition-based parser</strong></p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909104413757.png" class="" title="image-20210909104413757">
<p><strong>MaltParser</strong></p>
<p>引入机器学习分类器，通过分类器判断添加的依赖，以及依赖的方向。避免了搜索，提供了一种线性的解析方式</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909104740587.png" class="" title="image-20210909104740587">
<h4 id="如何衡量依存分析的效果？"><a href="#如何衡量依存分析的效果？" class="headerlink" title="如何衡量依存分析的效果？"></a>如何衡量依存分析的效果？</h4><p><strong>Dependency Accurracy</strong></p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909105300077.png" class="" title="image-20210909105300077">
<h4 id="基于神经网络的依存分析器构建"><a href="#基于神经网络的依存分析器构建" class="headerlink" title="基于神经网络的依存分析器构建"></a>基于神经网络的依存分析器构建</h4><img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909110120011.png" class="" title="image-20210909110120011">
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-3.%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909111052510.png" class="" title="image-20210909111052510">
]]></content>
      <categories>
        <category>AI理论</category>
        <category>cs224n</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-4.语言模型与RNN引入</title>
    <url>/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-4.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8ERNN%E5%BC%95%E5%85%A5/</url>
    <content><![CDATA[<h4 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h4><p>通过语言模型的构建，实现能够根据已知序列推断序列中的下一个单词，如搜索引擎中的搜索推断等。</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-4.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8ERNN%E5%BC%95%E5%85%A5/image-20210911095229424.png" class="" title="image-20210911095229424">
<p>模型的形式化定义如下，给定一系列单词，预测下一个单词的概率分布，该概率分布为当前模型词典库上词语的概率分布</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-4.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8ERNN%E5%BC%95%E5%85%A5/image-20210911095436865.png" class="" title="image-20210911095436865">
<span id="more"></span>
<p>条件概率角度理解语言模型形式如下，即一系列条件概率的连乘</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-4.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8ERNN%E5%BC%95%E5%85%A5/image-20210911095643154.png" class="" title="image-20210911095643154">
<h4 id="如何构建一个语言模型（N-grams模型）"><a href="#如何构建一个语言模型（N-grams模型）" class="headerlink" title="如何构建一个语言模型（N-grams模型）"></a>如何构建一个语言模型（N-grams模型）</h4><blockquote>
<p><strong><a href="https://baike.baidu.com/item/马尔可夫/2774684">马尔可夫</a></strong>（Markov）假设：下一个词的出现仅仅依赖于前面的几个词，而不是整个前部分句子（简化概率计算，减少模型参数）</p>
<p><strong>大数定理</strong>：在试验不变的条件下，重复试验多次，随机事件的频率近似于它的概率。偶然中包含着某种必然</p>
</blockquote>
<p>N-grams模型基于马尔可夫假设，指给定的一段文本或语音中N个项目（item）的序列，项目可以是音节、词语或者碱基对等，N-gram模型中的N，即对应概率依赖的词语个数，例如：</p>
<ul>
<li><p>当N = 1时，1-gram模型（unigram）：$p(T) = p(W1)p(W2)…..p(Wn)$ 即词语的出现相互独立</p>
</li>
<li><p>当N = 2 时，2-gram模型（bigram）：$p(T) = p(W1)p(W2|W1)…..p(Wn|Wn-1)$ 即依赖前一个词语</p>
</li>
</ul>
<p>以N-gram模型为例子，根据大数定理可得，在语料库足够大的条件下，$W_n$关于$W_1…….W_{n-1}$的条件概率为</p>
<script type="math/tex; mode=display">
p(Wi|Wi-1) = \frac{count(W_1,W_2,.......W_{N-1},W_n)}{count(W_1,W_2,.......W_{N-1})}</script><p>即语料种$W_1^n$语句的出现次数除以$W_i^{n-1}$语句的次数</p>
<h5 id="单词稀疏问题（Sparsity-Problem）"><a href="#单词稀疏问题（Sparsity-Problem）" class="headerlink" title="单词稀疏问题（Sparsity Problem）"></a>单词稀疏问题（Sparsity Problem）</h5><p>根据N-grams的计算公式，如果语料库中不存在（或几乎没有）当前上文与目标推断单词共同出现的情况，则概率公式的分母为0，即<strong>稀疏问题</strong>，</p>
<p>解决方式为通过在概率公式中增加一个增量，使词库中的所有单词在给定上文预测的情况下，都具有一定的概率值（即使是不可能的单词），称之为<strong>平滑法</strong>。随着n元语法中的n不断增大，单词稀疏问题会更加严重</p>
<ul>
<li>Add‐One 平滑</li>
<li>Add‐K 平滑</li>
<li>等</li>
</ul>
<h5 id="固定窗口神经网络语言模型"><a href="#固定窗口神经网络语言模型" class="headerlink" title="固定窗口神经网络语言模型"></a>固定窗口神经网络语言模型</h5><p>固定输入窗口大小，将输入窗口内的词语向量拼接，作为模型输入，预测下一个单词的概率，解决了非神经网络语言模型存在的单词稀疏和存储稀疏问题，但是仍存在</p>
<ol>
<li>固定窗口有些时候可能太小</li>
<li>扩大窗口会导致模型参数量爆炸</li>
<li>窗口内不同位置单词对应不同的权重值（？）</li>
</ol>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-4.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8ERNN%E5%BC%95%E5%85%A5/image-20210911103426907.png" class="" title="image-20210911103426907">
<h5 id="基于RNN的语言模型"><a href="#基于RNN的语言模型" class="headerlink" title="基于RNN的语言模型"></a>基于RNN的语言模型</h5><img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-4.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8ERNN%E5%BC%95%E5%85%A5/image-20210911103854491.png" class="" title="image-20210911103854491">
]]></content>
      <categories>
        <category>AI理论</category>
        <category>cs224n</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-5.RNN主要问题与改进</title>
    <url>/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/</url>
    <content><![CDATA[<h3 id="RNN梯度消失-爆炸问题（vanishing-exploding-gradient-problem）"><a href="#RNN梯度消失-爆炸问题（vanishing-exploding-gradient-problem）" class="headerlink" title="RNN梯度消失/爆炸问题（vanishing/exploding gradient problem）"></a>RNN梯度消失/爆炸问题（vanishing/exploding gradient problem）</h3><p>由于RNN对于不同时间步的输入使用同一个神经元，在反向传播计算梯度时，随着时间步的增加，会出现梯度爆炸和消失问题</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914151209756.png" class="" title="image-20210914151209756">
<span id="more"></span>
<p>RNN时间步t计算公式为</p>
<script type="math/tex; mode=display">
S_t = \sigma(S_{t-1}W_s + X_tW_x+b_t) \\
O_t = W_oS_t+b_o</script><p>反向传播的损失函数求导结果为</p>
<script type="math/tex; mode=display">
\frac{\partial L_t}{\partial W_x} = \sum_{k=0}^t \frac{\partial L_t}{\partial O_x}\frac{\partial O_t}{\partial S_t}(\prod_j^t \frac{\partial S_j}{\partial S_{j-1}})\frac{\partial O_k}{\partial W_x}</script><p>其中 $\frac{\partial s_j}{\partial S_{j-1}} = \sigma’ W_s$ (不考虑激活函数的情况下),由于累乘的性质，随着神经元系数W的累乘次数增加，如果$w$大于1，产生梯度爆炸问题；如果$w$小于，出现梯度消失问题。</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914152544786.png" class="" title="image-20210914152544786">
<p>梯度消失问题导致 <strong>RNN难以捕捉长时间步距离中的有效信息</strong></p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914153650599.png" class="" title="image-20210914153650599">
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914154543524.png" class="" title="image-20210914154543524">
<h4 id="解决梯度爆炸问题"><a href="#解决梯度爆炸问题" class="headerlink" title="解决梯度爆炸问题"></a>解决梯度爆炸问题</h4><p>梯度剪切（clipping）</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914154902059.png" class="" title="image-20210914154902059">
<h3 id="Long-Short-term-Memory-LSTM"><a href="#Long-Short-term-Memory-LSTM" class="headerlink" title="Long Short-term Memory(LSTM)"></a>Long Short-term Memory(LSTM)</h3><p>在原RNN每个时间步拥有一个隐藏状态 $h^{(t)}$ 的基础上，增加一个存储单元 $c^{(t)}$（cell state）存储RNN无法捕捉的长距离时间步信息</p>
<ul>
<li>LSTM能够从存储单元中删除、写入以及读取相关信息，是否执行这些操作由对应的控制门（gate）决定</li>
<li>每个时间步gate值由当前时间步信息动态计算</li>
</ul>
<p>三种类型门的值均采用神经网络方式预测得到</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914161611434.png" class="" title="image-20210914161611434">
<p><strong>基本计算流程：</strong></p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914162031478.png" class="" title="image-20210914162031478">
<ol>
<li>首先根据上一时间步隐藏状态和当前输入，计算当前存储单元候选值</li>
<li>根据<strong>遗忘门和输入门</strong>有选择的组合历史/当前存储单元计算值，获得当前存储单元值（忘记多少从前，留下多少现在）</li>
<li>根据<strong>输出门</strong>，决定隐藏状态值</li>
</ol>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914162308396.png" class="" title="image-20210914162308396">
<h4 id="为何LSTM能够解决梯度消失问题"><a href="#为何LSTM能够解决梯度消失问题" class="headerlink" title="为何LSTM能够解决梯度消失问题"></a>为何LSTM能够解决梯度消失问题</h4><p>关注点不在于解决梯度消失或者梯度爆炸，在于解决由梯度消失带来的 <strong>长距离信息无法捕捉的问题</strong>（不解决因，而改善果），类似于resnet中的残差思路</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210914163439128.png" class="" title="image-20210914163439128">
<h3 id="Gated-Recurrent-Units-GRU-降低了LSTM的复杂度"><a href="#Gated-Recurrent-Units-GRU-降低了LSTM的复杂度" class="headerlink" title="Gated Recurrent Units(GRU) - 降低了LSTM的复杂度"></a>Gated Recurrent Units(GRU) - 降低了LSTM的复杂度</h3><p>不使用cell存储单元，只采用们的思想控制历史隐藏状态以及当前隐藏状态对于结果的影响。、</p>
<ol>
<li>$u^t$ 更新门：控制当前生成当前隐藏状态时保留的历史状态和当前隐藏状态的比例</li>
<li>$r^t$ 重置门：历史隐藏状态信息 流入 当前候选隐藏状态的量</li>
</ol>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210915191812848.png" class="" title="image-20210915191812848">
<p>与LTSM应用效果区别不大，如果问题特别重视长距离信息的保存，默认使用LSTM</p>
<h3 id="其他类型神经网络中同样存在梯度消失爆炸问题"><a href="#其他类型神经网络中同样存在梯度消失爆炸问题" class="headerlink" title="其他类型神经网络中同样存在梯度消失爆炸问题"></a>其他类型神经网络中同样存在梯度消失爆炸问题</h3><p>随着网络层数的不断增加，梯度消失/爆炸更容易出现，导致深层神经网络的训练难度更高，出现了几种特殊类型网络，解决此类问题</p>
<ol>
<li><p>resnet 残差网络</p>
<blockquote>
<p><a href="https://www.cnblogs.com/shine-lee/p/12363488.html">相比于让$F(x)$学习成恒等映射，让$F(x)$学习成0要更加容易——后者通过L2正则就可以轻松实现</a></p>
</blockquote>
<p>残差网络采用 shortcut-connection的思想，将几个layer成为block，假设每个block拟合函数为$F(x)$ ,目标函数为$H(x)$。传统网络思想通过不断训练用拟合函数$F(x)$映射目标函数$H(x)$，resnet将拟合目标修改为$H(x)-x$ ,在输出增加一个<strong>shortcut connection</strong>，直接将$x$ 加到拟合结果上，实现与传统网络相同的效果。</p>
<p>也正是因为<strong>shortcut connection</strong>的存在，实现了长距离信息的保存</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210915194728672.png" class="" title="image-20210915194728672">
</li>
<li><p>densenet（<strong>待详细了解</strong>）</p>
<p>每一层的输入都包括前面所有层的输出</p>
</li>
</ol>
<h3 id="双向RNN（bidirectional）"><a href="#双向RNN（bidirectional）" class="headerlink" title="双向RNN（bidirectional）"></a>双向RNN（bidirectional）</h3><p>为了解决单向RNN存在的句子歧义问题（I am terribly exciting 是难受 还是特别兴奋？），通过两个独立的RNN网络，分别正/逆输入句子，将两个网络相同词语获得的隐藏状态拼接，形成新的隐藏状态，输入到输入层获得输入结果。</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210915195839882.png" class="" title="image-20210915195839882">
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210915200357516.png" class="" title="image-20210915200357516">
<h5 id="双向RNN的局限性"><a href="#双向RNN的局限性" class="headerlink" title="双向RNN的局限性"></a>双向RNN的局限性</h5><p>双向RNN只有在输入数据完整句子的情况下才能够使用，LM模型不适用。</p>
<h3 id="多层RNN（multi-layer）"><a href="#多层RNN（multi-layer）" class="headerlink" title="多层RNN（multi-layer）"></a>多层RNN（multi-layer）</h3><p>多个RNN单元堆积在一起组成深度网络，类似于卷积网络的思想，通过增加网络的深度，实现从浅层特征学习=&gt;深层特征学习。</p>
<img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210915200931271.png" class="" title="image-20210915200931271">
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><img src="/2021/09/15/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-5.RNN%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B/image-20210915201518821.png" class="" title="image-20210915201518821">
]]></content>
      <categories>
        <category>AI理论</category>
        <category>cs224n</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-6.机器翻译</title>
    <url>/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/</url>
    <content><![CDATA[<h3 id="早期机器翻译"><a href="#早期机器翻译" class="headerlink" title="早期机器翻译"></a>早期机器翻译</h3><img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923165912958.png" class="" title="image-20210923165912958">
<p>采用 单词对应词典的形式，存储在磁带上，翻译时通过查字典的形式，找到对应词组合成句子。</p>
<h3 id="基于统计的机器翻译（Statistical-Machine-Translation-SMT）"><a href="#基于统计的机器翻译（Statistical-Machine-Translation-SMT）" class="headerlink" title="基于统计的机器翻译（Statistical Machine Translation SMT）"></a>基于统计的机器翻译（Statistical Machine Translation SMT）</h3><img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923165944670.png" class="" title="image-20210923165944670">
<p>从概率的角度解决机器翻译问题， 首先语料中学习概率模型（不同语言单词之间的概率和语言内的语言模型），通过建立的概率模型实现翻译功能。</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923170157170.png" class="" title="image-20210923170157170">
<span id="more"></span>
<h3 id="基于神经网络的机器翻译（Nerual-Machine-Translation）2014"><a href="#基于神经网络的机器翻译（Nerual-Machine-Translation）2014" class="headerlink" title="基于神经网络的机器翻译（Nerual Machine Translation）2014"></a>基于神经网络的机器翻译（Nerual Machine Translation）2014</h3><p>采用了一种叫做seq2seq（sequence to sequence）的模型,使用两个RNN单元（可以是任意类型的RNN）实现机器翻译问题</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923171554655.png" class="" title="image-20210923171554655">
<ol>
<li>Eecoder RNN：输入源语言句子，对该句子进行编码</li>
<li>Decoder RNN：输入编码器生成的编码（隐藏状态）和开始符，不断地生成预测单词，最后组合成为翻译句子</li>
</ol>
<p>几个主要的seq2seq类型任务</p>
<ol>
<li>总结摘要（原文章 到 摘要/总结）</li>
<li>对话 （前一句话 到 后一句话）</li>
<li>代码生成等</li>
</ol>
<p>seq2seq模型是一种<strong>条件语言</strong>模型</p>
<ol>
<li>seq2seq的作用仍然是给定上文推断出下一个词语，所以是语言模型</li>
<li><strong>条件</strong>体现在这种推断是基于源语言段落输入（在源语言的条件下）</li>
</ol>
<p>seq2seq相较于SMT更加优越是在于</p>
<ol>
<li><p>seq2seq模型直接模拟 目标语言在源语言条件下的语言模型</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923190656093.png" class="" title="image-20210923190656093">
</li>
<li><p>SMT 将这种模拟拆成两个子问题，源语言到目标语言的概率 和  目标语言的语言模型</p>
</li>
</ol>
<h3 id="如何训练seq2seq模型"><a href="#如何训练seq2seq模型" class="headerlink" title="如何训练seq2seq模型"></a>如何训练seq2seq模型</h3><p>输入 平行语料（源语言-目标语言的句子对），分别作为Encoder和Decoder的输入，Decoder每个时间步的预测输出与目标结果单独计算loss函数，最后以所有时间步的平均loss作为优化目标loss，反向传播实现训练。</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923191307147.png" class="" title="image-20210923191307147">
<ul>
<li>与预测不同，Decoder的每个时间步的输出不再作为下一个时间步的输入，输入来自于源语言对应的目标语言（类似于监督学习）。</li>
</ul>
<h3 id="解码器的解码策略"><a href="#解码器的解码策略" class="headerlink" title="解码器的解码策略"></a>解码器的解码策略</h3><h4 id="贪婪解码（greedy-decoding）"><a href="#贪婪解码（greedy-decoding）" class="headerlink" title="贪婪解码（greedy decoding）"></a>贪婪解码（greedy decoding）</h4><p>每个时间步选择预测概率最大的词作为预测结果，但是存在无法回退的问题</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923192446719.png" class="" title="image-20210923192446719">
<h4 id="束搜索解码（beam-search-decoding）"><a href="#束搜索解码（beam-search-decoding）" class="headerlink" title="束搜索解码（beam search decoding）"></a>束搜索解码（beam search decoding）</h4><p>由于穷举搜索的成本过高，贪婪解码又可能错过最优解，采用一种折中的思路，每次选取预测概率最大的k个词作为备选词，进入下一次预测，下一次同样选取概率最大的k个序列,其中k为束搜索的宽度。</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923193034552.png" class="" title="image-20210923193034552">
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923193403947.png" class="" title="image-20210923193403947">
<h5 id="如何结束搜索？"><a href="#如何结束搜索？" class="headerlink" title="如何结束搜索？"></a>如何结束搜索？</h5><ol>
<li>限定束搜索的最大时间步数</li>
<li>限定生成完整翻译句子的数字（最后一个时间步输出\<end\>标签）</li>
</ol>
<h5 id="解决束搜索倾向问题"><a href="#解决束搜索倾向问题" class="headerlink" title="解决束搜索倾向问题"></a>解决束搜索倾向问题</h5><p>由于score函数的计算方式，预测的翻译序列越长，其得分越高，导致束搜索更加倾向于短的翻译结果，通过对score归一化解决</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923194001899.png" class="" title="image-20210923194001899">
<h4 id="NMT的优势"><a href="#NMT的优势" class="headerlink" title="NMT的优势"></a>NMT的优势</h4><img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923194329188.png" class="" title="image-20210923194329188">
<h3 id="如何评价机器翻译效果"><a href="#如何评价机器翻译效果" class="headerlink" title="如何评价机器翻译效果"></a>如何评价机器翻译效果</h3><h4 id="BLUE-Bilingual-Evaluation-Understudy"><a href="#BLUE-Bilingual-Evaluation-Understudy" class="headerlink" title="BLUE(Bilingual Evaluation Understudy)"></a>BLUE(Bilingual Evaluation Understudy)</h4><p>将机器翻译结果语句（candidate）与一系列人类翻译的参考语句（references）相比较，比较相似度：</p>
<ol>
<li>依赖n-gram语法(以长度为n的子句子作为衡量的单元),计算 candidate中 n-gram在references中出现的个数 比上references中所有n-gram出现的次数。</li>
<li>并给倾向于生成过短翻译的系统以惩罚</li>
</ol>
<p>基本计算公式如下</p>
<script type="math/tex; mode=display">
BLUE=\frac{\sum_{n-gram \sub references}\sum_{n-gram \sub candidate \\ \& \\ n-gram \sub reference} Count_{clip}(n-gram)}{\sum_{n-gram \sub references}\sum_{n-gram \sub reference}}</script><p>通过召回率和惩罚因子，解决了重复出现和翻译较短给分较高的问题</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210923200515633.png" class="" title="image-20210923200515633">
<h3 id="机器翻译目前问题"><a href="#机器翻译目前问题" class="headerlink" title="机器翻译目前问题"></a>机器翻译目前问题</h3><ol>
<li>遇到语料库之外的词语，翻译效果较差</li>
<li>如果训练材料较为局限，训练出来的模型也不具备普适性（Domain mismatch）</li>
<li>无法解决长文本、书籍等的翻译问题（聚焦于句子，没有对全文信息的参考）</li>
<li>翻译好的 训练数据较少</li>
</ol>
<h3 id="Attention机制"><a href="#Attention机制" class="headerlink" title="Attention机制"></a>Attention机制</h3><p>编码器最后将整个句子的信息编码进入一个输出向量中，该向量可能无法包含所有的句子信息，导致解码器在信息缺失的情况下进行生成翻译短文（information bottleneck），注意力机制提供了一种解决这种问题的方法。</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210924144203179.png" class="" title="image-20210924144203179">
<p>在Decoder的每个时间步，不再直接使用隐藏状态作为预测输出层的输入，而是需要计算注意力输出</p>
<ol>
<li><p>当前时间步与编码器的每个时间隐藏状态输出点乘获得一个多个注意力分数，组成向量</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210924145511050.png" class="" title="image-20210924145511050">
</li>
<li><p>输入注意力分数向量到softmax计算概率分布</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210924145544523.png" class="" title="image-20210924145544523">
</li>
<li><p>对Encoder的每个时间步的隐藏状态进行概率分布加权平均（”注意力“ 就是 对哪个实践步隐藏状态的概率值大小，越大说明我对对应词语注意力越集中）</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210924145757503.png" class="" title="image-20210924145757503">
</li>
<li><p>将注意力输出与当前隐藏状态拼接，作为当前时间步输出层的输入，获得预测词语</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210924145905073.png" class="" title="image-20210924145905073">
</li>
</ol>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-6.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/image-20210924144729428.png" class="" title="image-20210924144729428">
<p>Attention机制的优势</p>
<ol>
<li>显著提升了NMT系统的效果</li>
<li>解决了编码器存在的信息瓶颈问题</li>
<li>有助于解决梯度消失问题</li>
<li>提供了一定的可解释性（注意力分数）</li>
</ol>
]]></content>
      <categories>
        <category>AI理论</category>
        <category>cs224n</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-7.机器问答</title>
    <url>/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-7.%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94/</url>
    <content><![CDATA[<h3 id="Question-Answering"><a href="#Question-Answering" class="headerlink" title="Question Answering"></a>Question Answering</h3><p>相较于检索更进一步，给出一个问题，自动的找出这个问题的最合适答案，可以将这个问题划分为两个步骤</p>
<ol>
<li>找到包含问题答案的文档</li>
<li>在文档中找到当前问题的答案（阅读理解 Reading Comprehension）</li>
</ol>
<p>如果一个机器理解了一段问题，机器应该能够提供问题的正确答案，且答案中不包含与问题无关的相关信息</p>
<h3 id="SQuAD-Stanford-Question-Answering-Dataset"><a href="#SQuAD-Stanford-Question-Answering-Dataset" class="headerlink" title="SQuAD(Stanford Question Answering Dataset)"></a>SQuAD(Stanford Question Answering Dataset)</h3><p>每个问题对应一篇文章，答案是文章内的一段单词序列。</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-7.%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94/image-20210926161057907.png" class="" title="image-20210926161057907">
<p>为每个问题提供多个可选的标准答案</p>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-7.%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94/image-20210926161326100.png" class="" title="image-20210926161326100">
<h4 id="SQuAD如何评估（V1-1）"><a href="#SQuAD如何评估（V1-1）" class="headerlink" title="SQuAD如何评估（V1.1）"></a>SQuAD如何评估（V1.1）</h4><ul>
<li>为每个问题提供三个标准答案</li>
<li>使用两种评分机制<ol>
<li>Exact match：按照字面意思理解，如果答案在三个标准答案中(1),不在标准答案中(0)</li>
<li>F1-score：分别计算对于每个问题回答的F1-score(<strong>具体怎么算法不知道</strong>)，对整个数据集上求平均后得到结果</li>
</ol>
</li>
<li>两种评分机制军忽略标点符号和无关的词汇（a,an,the）</li>
</ul>
<span id="more"></span>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-7.%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94/image-20210926164152196.png" class="" title="image-20210926164152196">
<h4 id="SQuAD如何评估（V2-0）"><a href="#SQuAD如何评估（V2-0）" class="headerlink" title="SQuAD如何评估（V2.0）"></a>SQuAD如何评估（V2.0）</h4><p>SQuAD2.0中1/2的问题的答案在文章中，1/2的问题的答案不在文章中（并不是所有的问题的答案都在文章中）</p>
<ul>
<li>对于无答案问题的评价，答案是无答案，给1；找到了某个答案，给0。（对于原有的两种评分机制都一样）</li>
</ul>
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-7.%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94/image-20210926165758040.png" class="" title="image-20210926165758040">
<img src="/2021/09/30/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-7.%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94/image-20210926165628432.png" class="" title="image-20210926165628432">
<h4 id="SQuAD存在的问题"><a href="#SQuAD存在的问题" class="headerlink" title="SQuAD存在的问题"></a>SQuAD存在的问题</h4><ul>
<li>所有问题的答案都是文章的子序列，没有难度更高的隐含、推理等类型的答案（后者问题更为常见）</li>
<li>SQuAD中问题的构建是基于文章的，问题倾向于与文章相似（体现在结构、用词等），降低了回答的难度</li>
</ul>
]]></content>
      <categories>
        <category>AI理论</category>
        <category>cs224n</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-8.NLP中的CNN</title>
    <url>/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-8.NLP%E4%B8%AD%E7%9A%84CNN/</url>
    <content><![CDATA[<h3 id="从RNN到CNN"><a href="#从RNN到CNN" class="headerlink" title="从RNN到CNN"></a>从RNN到CNN</h3><p>RNN的输入是一个完整的序列，其每一个时间步的输出均受到之前时间步的影响，RNN最终捕捉的是整个序列的特征信息，而一些NLP问题可能更加关注于句子的局部信息(例如本文分类)，这一点是CNN的强项。</p>
<h4 id="CNN解决NLP问题的出发点"><a href="#CNN解决NLP问题的出发点" class="headerlink" title="CNN解决NLP问题的出发点"></a>CNN解决NLP问题的出发点</h4><p>按照窗口大小，在原序列上进行滑动，获得不同相同长度的子序列（语义可能无关），对于这些个子序列分别计算向量信息</p>
<ul>
<li>忽略了语法和语义信息，只是距离上的临近</li>
<li>没有RNN的语法语义上的说服性（你就这样随便划分，提取出来的向量能有用？）</li>
</ul>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-8.NLP%E4%B8%AD%E7%9A%84CNN/image-20211005093410774.png" class="" title="image-20211005093410774">
<span id="more"></span>
<p>NLP中的CNN类似于图像处理中的多通道一维卷积问题(conv1d)</p>
<ul>
<li>同样通过增加空向量，实现padding操作</li>
<li>使用多个卷积核，实现多通道输出</li>
<li>pooling over time，k-max pooling over time,dilation pooling</li>
</ul>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-8.NLP%E4%B8%AD%E7%9A%84CNN/image-20211005095925832.png" class="" title="image-20211005095925832">
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-8.NLP%E4%B8%AD%E7%9A%84CNN/image-20211005104515907.png" class="" title="image-20211005104515907">
<h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><blockquote>
<p>解释来自 <a href="https://www.cnblogs.com/shine-lee/p/11989612.html">Batch Normalization详解</a> </p>
</blockquote>
<p>作为网络的一层，对输入的一个batch进行标准化处理（减均值，除以方差），能够有效的降低深度网络的学习难度</p>
<p><strong>batch分布不断变化导致模型拟合偏差</strong></p>
<ul>
<li>每次梯度下降根据输入batch的分布计算，或者说拟合的是输入的分布</li>
<li>不同batch分布不同，导致每次拟合不断改变方向（类似于无头苍蝇），导致学习速率减慢</li>
<li>在机器学习或者浅层模型中，这种问题并不严重，而在深度网络中，每一层都在进行着（无头苍蝇拟合），为了避免震荡，必须将学习率设置的小一些（<strong>Internal Covariate Shift</strong>）。</li>
</ul>
<p><strong>BN的主要操作：</strong></p>
<ol>
<li>对batch数据进行标准化操作（减均值，除以方差）<ul>
<li>这一步的参数是由输入batch决定的不需要学习</li>
</ul>
</li>
<li>对标准化后的数据进行平移和放缩（修改均值，和方差）<ul>
<li>平移和放缩的量由网络学习得到，即batch分布的均值和方差由学习得到</li>
</ul>
</li>
</ol>
<p><strong>BN带来的好处：</strong></p>
<ol>
<li>所有变换均为线性变换，反向传播求导较为简单</li>
<li>可以使用更大的学习率</li>
<li>权重的大小和初始化值不再重要，bias也可以设置为0</li>
</ol>
<h3 id="CNN的应用"><a href="#CNN的应用" class="headerlink" title="CNN的应用"></a>CNN的应用</h3><p><strong>Translation</strong></p>
<p>使用cnn作为encoder，rnn作为decoder</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-8.NLP%E4%B8%AD%E7%9A%84CNN/image-20211005145436480.png" class="" title="image-20211005145436480">
<p><strong>使用cnn实现深度nlp文本分类系统</strong></p>
<h3 id="Quasi-Recurrent-Neural-Network"><a href="#Quasi-Recurrent-Neural-Network" class="headerlink" title="Quasi-Recurrent Neural Network"></a>Quasi-Recurrent Neural Network</h3><h3 id="Q-RNN-Experiments-语言模型"><a href="#Q-RNN-Experiments-语言模型" class="headerlink" title="Q-RNN Experiments: 语言模型"></a>Q-RNN Experiments: 语言模型</h3>]]></content>
      <categories>
        <category>AI理论</category>
        <category>cs224n</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>cs224n-9.子词模型</title>
    <url>/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h3 id="语言学背景-划分更小单位的词"><a href="#语言学背景-划分更小单位的词" class="headerlink" title="语言学背景-划分更小单位的词"></a>语言学背景-划分更小单位的词</h3><h4 id="语音学和音韵学"><a href="#语音学和音韵学" class="headerlink" title="语音学和音韵学"></a>语音学和音韵学</h4><p>语音学中将语音看作连续不断变化的声音流，音韵学将语音划分为不同的单位-音位（phoneme），同一个词的读法中音位的不同，对于不同群体理解可能有不同的含义。但是由于发音对于文本的理解并无意义，将此思想借鉴到单词形态分析上，形成了这种（parts of word）的思想</p>
<h4 id="形态学：部分词（part-of-word）"><a href="#形态学：部分词（part-of-word）" class="headerlink" title="形态学：部分词（part of word）"></a>形态学：部分词（part of word）</h4><p>如何对词进行拆分以更好地理解当前的单词（有点中文里的看半边猜词的味道，英文里去掉前缀后缀看词根）</p>
<ul>
<li>传统方式是将单词划分为最小语义单位</li>
<li>使用字符级n-grams对单词进行拆分</li>
</ul>
<span id="more"></span>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211009184508969.png" class="" title="image-20211009184508969">
<h4 id="不同语言词语组成形式各不相同"><a href="#不同语言词语组成形式各不相同" class="headerlink" title="不同语言词语组成形式各不相同"></a>不同语言词语组成形式各不相同</h4><img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211009185807612.png" class="" title="image-20211009185807612">
<h4 id="为什么我们需要小于词语级别的模型"><a href="#为什么我们需要小于词语级别的模型" class="headerlink" title="为什么我们需要小于词语级别的模型"></a>为什么我们需要小于词语级别的模型</h4><ul>
<li><p>部分语言的单词空间过大</p>
</li>
<li><p>音译</p>
</li>
<li><p>非正式拼写</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211009190155726.png" class="" title="image-20211009190155726">
</li>
</ul>
<h3 id="Character-level-Model-字符级别模型"><a href="#Character-level-Model-字符级别模型" class="headerlink" title="Character-level Model 字符级别模型"></a>Character-level Model 字符级别模型</h3><p>当前字符级别模型主要有两个主要方向</p>
<ol>
<li>与词语级别模型相同架构，只是将单位缩小为 “word pieces”</li>
<li>复合架构，主要采用词语模型，对于特殊情况（未知词）等使用字符级模型</li>
</ol>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010152953701.png" class="" title="image-20211010152953701">
<h4 id="纯字符级别NMT模型"><a href="#纯字符级别NMT模型" class="headerlink" title="纯字符级别NMT模型"></a>纯字符级别NMT模型</h4><p>English-Czech WMT 2015 Results</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211009191631221.png" class="" title="image-20211009191631221">
<p>Fully Character-Level Neural Machine Translation without Explicit Segmentation</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211009192226372.png" class="" title="image-20211009192226372">
<p>Stronger character results with depth in LSTM seq2seq model</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211009192454633.png" class="" title="image-20211009192454633">
<p>模型较小使用word-level，较大使用character-level</p>
<h4 id="Byte-Pair-Encoding"><a href="#Byte-Pair-Encoding" class="headerlink" title="Byte Pair Encoding"></a>Byte Pair Encoding</h4><img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010154156409.png" class="" title="image-20211010154156409">
<p>源于一种字符压缩算法，将共同出现频率较高的两个压缩成字典中不存在的新字符。</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010154131682.png" class="" title="image-20211010154131682">
<p><strong>为了解决NMT翻译中的<UNK>问题以及英语中不同后缀含义不同</strong>，使用子词单元嵌入代替词语，利用BPE思想，每次选择词库中出现频率最高的词语对（不一定长度为2），作为新词典中的一个词，不断按照上述方式选择，直到达到词典目标大小。</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010155718892.png" class="" title="image-20211010155718892">
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010155859930.png" class="" title="image-20211010155859930">
<h4 id="Wordpiece-Sentencepiece-model"><a href="#Wordpiece-Sentencepiece-model" class="headerlink" title="Wordpiece/Sentencepiece model"></a>Wordpiece/Sentencepiece model</h4><p>google在BPE的基础上形成了两种类型的模型</p>
<ol>
<li>Wordpiece  单词为整体，以字母为单位，进行划分</li>
<li>Sentencepiece  句子为整体，以单词为单位，进行划分（有点<strong>意群</strong>的味道）</li>
</ol>
<p>bert使用了一种变种wordpiece模型</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010171122417.png" class="" title="image-20211010171122417">
<h4 id="Character-level-to-build-Word-level"><a href="#Character-level-to-build-Word-level" class="headerlink" title="Character level to build Word level"></a>Character level to build Word level</h4><img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010171600247.png" class="" title="image-20211010171600247">
<p>对字符卷积获得词嵌入向量</p>
<h4 id="Character-level-based-LSTM-to-build-word-repesetation"><a href="#Character-level-based-LSTM-to-build-word-repesetation" class="headerlink" title="Character level based LSTM to build word repesetation"></a>Character level based LSTM to build word repesetation</h4><img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010171833154.png" class="" title="image-20211010171833154">
<p>不使用卷积，使用bi-LSTM输入字符，输出词语嵌入</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010171929903.png" class="" title="image-20211010171929903">
<h3 id="Hybrid-NMT"><a href="#Hybrid-NMT" class="headerlink" title="Hybrid NMT"></a>Hybrid NMT</h3><p>将两种划分方式区分开</p>
<ul>
<li>主要使用词语层级的划分</li>
<li>使用字符划分作为补充</li>
</ul>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010173107643.png" class="" title="image-20211010173107643">
<h4 id="Fast-Test-Embedding"><a href="#Fast-Test-Embedding" class="headerlink" title="Fast-Test Embedding"></a>Fast-Test Embedding</h4><img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010174250157.png" class="" title="image-20211010174250157">
<p>基于character-level 和 word2vec，改善word2vec获得的词向量对于词典外词语（oov）以及词语的各种变形不适应的情况。</p>
<ul>
<li><p>将单词拆成字符级别的n-gram表示</p>
</li>
<li><p>使用n-grams中所有子词的向量作为词语的向量表示进行嵌入训练</p>
</li>
<li><p>训练完成后，简单求和作为词语的词嵌入形式</p>
<img src="/2021/10/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/cs224n%E7%AC%94%E8%AE%B0/cs224n-9.%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/image-20211010175316928.png" class="" title="image-20211010175316928"></li>
</ul>
]]></content>
      <categories>
        <category>AI理论</category>
        <category>cs224n</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title>Subword分词:BPE&amp;word-piece</title>
    <url>/2022/03/27/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/BPE&amp;wordpiece/</url>
    <content><![CDATA[<h2 id="Subword分词-BPE-amp-word-piece"><a href="#Subword分词-BPE-amp-word-piece" class="headerlink" title="Subword分词:BPE&amp;word-piece"></a>Subword分词:BPE&amp;word-piece</h2><p>在读transforme论文时，论文中在两个NMT任务中分别使用了两种编码算法byte pair encoding和word-piece，似乎子词嵌入模型是解决OOV问题不二选择，稍微了解一下</p>
<h3 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h3><p>为了解决NMT中的OOV问题，基于子词模型（sub-word）以及Byte pair encoding思想提出了BPE算法，解决了词表大小压缩问题</p>
<h4 id="Byte-pair-encoding"><a href="#Byte-pair-encoding" class="headerlink" title="Byte pair encoding"></a>Byte pair encoding</h4><p>一种简单的数据压缩算法，寻找byte串中重复出现多次的byte对(pair of consecutive bytes)，使用串中未出现过的byte替代，直到byte串中不存在重复多次的byte串。</p>
<p>wekipedia的例子：</p>
<span id="more"></span>
<img src="/2022/03/27/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/BPE&wordpiece/image-20220315101051756.png" class="" title="image-20220315101051756">
<p>BPE算法与原Byte pair encoding算法完全一致，只是运行在character层级上，论文中思路主要为</p>
<ul>
<li>初始词表为所有单词拆分的字母+ 特殊的单词结束符（a special end-of word symbol ‘·’）</li>
<li>重复遍历使用频率最高的pair替换<ul>
<li>如  (‘A’, ‘B’) 用  ‘AB’ 替换（一个词替代两个词）</li>
<li>replace each occurrence of the most frequent pair (‘A’, ‘B’) with a new symbol ‘AB’.</li>
</ul>
</li>
<li>重复迭代直到满足要求</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re, collections</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stats</span>(<span class="params">vocab</span>):</span></span><br><span class="line">    pairs = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(symbols) - <span class="number">1</span>):</span><br><span class="line">            pairs[symbols[i], symbols[i + <span class="number">1</span>]] += freq</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_vocab</span>(<span class="params">pair, v_in</span>):</span></span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(<span class="string">&#x27; &#x27;</span>.join(pair))</span><br><span class="line">    p = re.<span class="built_in">compile</span>(<span class="string">r&#x27;(?&lt;!\S)&#x27;</span> + bigram + <span class="string">r&#x27;(?!\S)&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        w_out = p.sub(<span class="string">&#x27;&#x27;</span>.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    <span class="keyword">return</span> v_out</span><br><span class="line">vocab = &#123;<span class="string">&#x27;l o w &lt;/w&gt;&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;l o w e r &lt;/w&gt;&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">         <span class="string">&#x27;n e w e s t &lt;/w&gt;&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;w i d e s t &lt;/w&gt;&#x27;</span>: <span class="number">3</span>&#125;</span><br><span class="line">num_merges = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_merges):</span><br><span class="line">    pairs = get_stats(vocab)</span><br><span class="line">    //寻找出现次数最多的pair对</span><br><span class="line">    best = <span class="built_in">max</span>(pairs, key=pairs.get)</span><br><span class="line">    vocab = merge_vocab(best, vocab)</span><br><span class="line">    <span class="built_in">print</span>(best)</span><br></pre></td></tr></table></figure>
<h4 id="joint-BPE"><a href="#joint-BPE" class="headerlink" title="joint BPE"></a>joint BPE</h4><p>NMT需要输入源语言、输出目标语言，因此需要构建两个分别包括两种语言的词表，论文中提出了两种类型的BPE</p>
<ul>
<li>源语言与目标语言分别运行BPE算法，构建子词词表</li>
<li>源语言与目标语言合并在一起，共建一个BPE词表（joint BPE）</li>
</ul>
<p>两种方法优略</p>
<ol>
<li>第一种方法，确保两种语言词表中都只包含出现过的子词，不会存在另一种语言的子词干扰，保证了词表的大小</li>
<li>第二种方法，确保了基于统计切词在两种语言上运行的一致性，相同的词切分方式保持一致（比如说相同的人名）</li>
</ol>
<p>没细看实验</p>
<img src="/2022/03/27/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/BPE&wordpiece/image-20220315112028772.png" class="" title="image-20220315112028772">
<h3 id="wordpiece"><a href="#wordpiece" class="headerlink" title="wordpiece"></a>wordpiece</h3><p>方法来自于论文：<a href="http://ieeexplore.ieee.org/document/6289079">Japanese and Korean voice search</a>， 该方法的主要思路与BPE基本相同，只是每次不再按照频率的大小选取词对，而是选择能够使得语言模型最大似然增加（increases the likelihood），算法基本流程如下（贪心思路）：</p>
<ol>
<li>首先以字符为单位在语料集上构建词表</li>
<li>使用词表+语料训练一个语言模型</li>
<li>遍历词表，选取字符对（word unit pair）,使用字符对替换的词表+语料再训练一个语言模型，最终选择使得<strong>语言模型最大似然最大</strong>的字符对作为<strong>此轮迭代选择的字符对</strong>，更新词表</li>
<li>重2，3直到词表大小满足预期要求</li>
</ol>
<p>每次迭代都需要遍历整个词表，假设词表大小为k，能够找的 word unit pair 个数为 $k^2$, 也就需要训练 $k^2$ 个语言模型，计算复杂度多得离谱，因此原论文提出了几个加速方法</p>
<ol>
<li><p>每次选择字符对，只选择训练语料中已经存在(字面意思理解就是 在语料中相邻的 word unit)</p>
</li>
<li><p>只选择那些很有可能成为最优字符对的进行比较（不太懂如何判断最有可能）</p>
</li>
<li><p>c和d没太理解</p>
<img src="/2022/03/27/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/BPE&wordpiece/image-20220316085911584.png" class="" title="image-20220316085911584"></li>
</ol>
]]></content>
      <categories>
        <category>AI理论</category>
        <category>杂知识点</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>分词算法</tag>
      </tags>
  </entry>
  <entry>
    <title>基于统计的文档语义表示</title>
    <url>/2022/04/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E6%96%87%E6%A1%A3%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA/</url>
    <content><![CDATA[<h2 id="基于统计的文档语义表示"><a href="#基于统计的文档语义表示" class="headerlink" title="基于统计的文档语义表示"></a>基于统计的文档语义表示</h2><p>在深度学习模型到来之前，通常使用统计学方法获得表示的文档语义的特征向量，主要方法包括</p>
<ol>
<li>TF-IDF</li>
<li>基于SVD分解的LSA（潜在语义索引/分析 Latent Semantic Indexing/Analysis）</li>
<li>LDiA 隐形迪利克雷分布</li>
</ol>
<p>其中前两种分布理解较为简单，LDiA涉及比较多的概率分布知识</p>
<h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p><strong>t</strong>erm <strong>f</strong>requency–<strong>i</strong>nverse <strong>d</strong>ocument <strong>f</strong>requency，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加（TF），但同时会随着它在语料库中出现的频率成反比下降（IDF）</p>
<span id="more"></span>
<p><strong>TF</strong></p>
<p>term frequency，给定一个词，当前词在当前文件出现的频率（通常需要除文档长度），计算公式如下</p>
<script type="math/tex; mode=display">
tf_{ij}= \frac{n_{j}}{n_i}</script><p><strong>IDF</strong></p>
<p>inverse document frequency，给定一个词，IDF值为文档库中总文档数除以包含该词的文档数</p>
<script type="math/tex; mode=display">
idf = log\frac{n_{total}}{n_{contains}}</script><p>将两个值相即为当前文档中该词的TF-IDF值，计算文档在词表上所有词的TF-IDF值，即可获得<strong>相较于词袋向量更加稠密但维度相同</strong>的文档特征向量,TF-IDF作为语义特征向量还存在着一些问题</p>
<ol>
<li>无法解决不同拼写但意思相近的词，TF-IDF计算是会将其看作不同的特征</li>
<li>TF-IDF特征向量的维度还是过大，计算量大且在样本量小（维度与样本数量接近）的时候，容易出现过拟合</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 快速计算TFIDF值，问题是处理不了中文</span></span><br><span class="line">sklearn.feature_extraction.text.TfidfVectorizer</span><br><span class="line"><span class="comment"># 将词频矩阵快速转化为IDF值（中文只能用这个了）</span></span><br><span class="line">sklearn.feature_extraction.text.TfidfTransformer</span><br></pre></td></tr></table></figure>
<h3 id="LSA-潜在语义分析"><a href="#LSA-潜在语义分析" class="headerlink" title="LSA 潜在语义分析"></a>LSA 潜在语义分析</h3><p>LSA实际上就是SVD矩阵分解应用到NLP领域中换的一个名字，结合SVD和PCA对别进行理解</p>
<h4 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h4><p>常用的针对非方阵矩阵的一种分解方法</p>
<img src="/2022/04/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E6%96%87%E6%A1%A3%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA/image-20220411100221445.png" class="" title="image-20220411100221445">
<p>其中矩阵 U和V满足:</p>
<img src="/2022/04/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E6%96%87%E6%A1%A3%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA/image-20220411100348208.png" class="" title="image-20220411100348208">
<p>迁移到NLP领域中,使用截断的SVD分解（取前p大的奇异值）,定义为所谓的<strong>潜在语义主题</strong></p>
<ul>
<li>矩阵$M_{m<em>n}$为 **词 </em>文档** 的特征向量矩阵，每一列均为一篇文档的特征向量</li>
<li>矩阵 $U_{m<em>p}$ 为 **词 </em> 潜在语义主题** 的关系矩阵</li>
<li>矩阵 $V_{n<em>p}$ 为 **文档 </em> 潜在语义主题** 的关系矩阵</li>
</ul>
<p>通过矩阵分解，获得了文档的潜在语义主题表示，实现了<strong>文档特征向量的降维</strong>（从词维度降低到定义的主题维度）</p>
<h4 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a><strong>PCA</strong></h4><p>Principal Component Analysis，从降维的角度出发，发现PCA与SVD都是在做类似于矩阵分解的操作，实现降维的目的。PCA出发点为找到对$X$的一种降维方法$Y=WX$，使得损失的信息量最少，如何实现这个目标？</p>
<ol>
<li>降维后某个特征（或者说维度）的方差最大（组内多样化）</li>
<li>不同特征之间的协方差为0（组间相关性尽量小）</li>
</ol>
<p>根据降维要求和方差大小，舍弃较小方差特征，保留较大方差特征，实现降维。</p>
<ul>
<li>如何转化为数学问题求解？引出了<strong>协方差矩阵</strong>，上述两个目标即为将协方差矩阵相似对角化过程（非对角线元素（协方差）转化为0，对角线元素（方差）转化为特征值）</li>
<li>如何快速找到目标矩阵 $X$ 的协方差矩阵？当$X$不同特征内归一化处理后，$XX^T$即为协方差矩阵</li>
</ul>
<p>最终转化为求解 $XX^T$的特征向量矩阵 $W$</p>
<h4 id="PCA-VS-SVD"><a href="#PCA-VS-SVD" class="headerlink" title="PCA VS SVD"></a>PCA VS SVD</h4><p>经过分析两者均最终转化为了 $XX^T$的特征向量矩阵的求解过程，两者在计算上实际等价，不同点在于</p>
<ol>
<li>PCA目的为降维，SVD目的为分解</li>
<li>PCA只能应用于方阵，SVD均可</li>
<li>PCA需要对数据进行中心化处理</li>
</ol>
<p>sklearn中的PCA实际上就是用SVD方法求解的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 截断SVD</span></span><br><span class="line">sklearn.decomposition.TruncatedSVD([n_components, ...])</span><br><span class="line"><span class="comment"># PCA</span></span><br><span class="line">sklearn.decomposition.PCA([n_components, copy, ...])</span><br><span class="line"><span class="comment"># 增量PCA 解决数量过大内存不足问题的近似PCA</span></span><br><span class="line">sklearn.decomposition.IncrementalPCA([n_components, ...])</span><br><span class="line"><span class="comment"># 稀疏PCA</span></span><br><span class="line">sklearn.decomposition.SparsePCA</span><br></pre></td></tr></table></figure>
<h3 id="LDiA"><a href="#LDiA" class="headerlink" title="LDiA"></a>LDiA</h3><p>Latent Dirichlet allocation 隐含迪利克雷分布，即它认为一篇文档是由一组词构成的一个集合，词与词之间没有顺序以及先后的关系。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。过程涉及到两个分布关系</p>
<ol>
<li>文档的主题<strong>分布</strong></li>
<li>每个主题的词<strong>分布</strong></li>
</ol>
<p>LDiA假设这两个分布均满足多项式分布，其中多项式参数个数为主题个数和词个数，在此假设下，我们已知<strong>数据+分布</strong>，<strong>如何求出目标分布的参数</strong>？LDiA假设主题分布和词分布的<strong>先验分布均为Dirichlet分布</strong>，即</p>
<img src="/2022/04/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E6%96%87%E6%A1%A3%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA/image-20220411153017543.png" class="" title="image-20220411153017543">
<ol>
<li>文档主题多项式分布参数 $p_1,p_2,p_3….p_n$满足Dirichlet分布（$p_i$ 为某文档中 $topic_i$ 出现的概率）</li>
<li>主题词多项分布 $p_1,p_2,p_3….p_n$满足Dirichlet分布（$p_i$ 即为某主题中 $word_i$ 出现的概率）</li>
</ol>
<p>常用方法有下面两种，不太清楚是否与贝叶斯估计有关系（<strong>不深入了解了，浪费时间也没必要</strong>）</p>
<ol>
<li>EM算法</li>
<li>Gibbs Sampling算法</li>
</ol>
<h4 id="想到了上学期学的贝叶斯估计求未知参数"><a href="#想到了上学期学的贝叶斯估计求未知参数" class="headerlink" title="想到了上学期学的贝叶斯估计求未知参数"></a>想到了上学期学的贝叶斯估计求未知参数</h4><p>我想到了数理统计里的贝叶斯估计，通过先验分布和后验分布，可以得到参数关于样本的概率分布，公式如下(应用数理统计-孙荣恒)</p>
<script type="math/tex; mode=display">
h(y|x_1,...,x_n) = \frac{\pi(y)f(x_1,...,x_n|y)}{g(x1,...,x_n)}\propto\pi(y)f(x_1,...,x_n|y)</script><p>其中 $y$ 后验分布中的未知参数，$\pi(y)$ 为先验分布（LDiA中的Dirichlet分布），$f(x_1,…,x_n|y)$ 为后验分布（LDiA中的多项式分布），得到未知参数关于实验数据（输入文档）的分布后，采取特定方法即可求得未知参数的估计值</p>
<h4 id="简单总结"><a href="#简单总结" class="headerlink" title="简单总结"></a>简单总结</h4><img src="/2022/04/17/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E6%96%87%E6%A1%A3%E8%AF%AD%E4%B9%89%E8%A1%A8%E7%A4%BA/image-20220411154536747.png" class="" title="image-20220411154536747">
<p>根据上述分析，我们可以得到LDA<strong>输入有两个超参数</strong></p>
<ol>
<li><strong>文档-主题</strong>先验Dirichlet分布的参数</li>
<li><strong>主题-词</strong>先验Dirichlet分布的参数</li>
</ol>
<p>LDA实际在求解的参数为</p>
<ol>
<li><strong>文档-主题</strong>多项式分布的参数</li>
<li><strong>主题-词</strong> 多项式分布的参数</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://zh.wikipedia.org/wiki/Tf-idf">tf-idf</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3">奇异值分解</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83">隐含狄利克雷分布</a></li>
<li><a href="https://www.cnblogs.com/yifanrensheng/p/13143970.html">NLP-04 隐含狄利克雷分布(LDA)</a></li>
<li>应用数理统计-孙荣恒</li>
</ol>
]]></content>
      <categories>
        <category>AI理论</category>
        <category>杂知识点</category>
      </categories>
      <tags>
        <tag>NLP理论</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML-1.学什么</title>
    <url>/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-1.%E5%AD%A6%E4%BB%80%E4%B9%88/</url>
    <content><![CDATA[<h3 id="学什么"><a href="#学什么" class="headerlink" title="学什么"></a>学什么</h3><iframe src="//player.bilibili.com/player.html?aid=675507034&bvid=BV13U4y1N7Uo&cid=409501228&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<p>主要是从机器学习的工作流出发，每个阶段主要的工作和工业界流行的技术和概念</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-1.%E5%AD%A6%E4%BB%80%E4%B9%88/image-20220224095954652.png" class="" title="image-20220224095954652">
<p>主要包括四个学习模块：</p>
<ol>
<li><strong>数据</strong> 数据获取、存储、清洗</li>
<li><strong>模型</strong> 训练、迁移、多模态</li>
<li><strong>部署</strong></li>
<li><strong>监控</strong> 可视化</li>
</ol>
<h3 id="学习计划"><a href="#学习计划" class="headerlink" title="学习计划"></a>学习计划</h3><p>基本学习计划：</p>
<ol>
<li>数据模块<ul>
<li>一天一个视频，慢慢学</li>
</ul>
</li>
<li>模型模块<ul>
<li>机器学习、深度学习模块部分，快速过一遍</li>
<li>模型验证，调优部分慢慢学，基础较差</li>
</ul>
</li>
<li>部署 + 监控部分<ul>
<li>慢慢学</li>
</ul>
</li>
</ol>
<p>大概一共30多个视频，三周内学完（无意外的情况下），<strong>deadline 3-15</strong></p>
]]></content>
      <categories>
        <category>AI理论</category>
        <category>实用机器学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML-5.偏差与方差</title>
    <url>/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-5.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/</url>
    <content><![CDATA[<h2 id="Bias-amp-Variance"><a href="#Bias-amp-Variance" class="headerlink" title="Bias &amp; Variance"></a>Bias &amp; Variance</h2><p><strong>bias(偏差)</strong>：模型对于样本的拟合程度，模型输出结果与样本真实结果之间的差距，通过增加模型复杂程度，增加训练轮数，可以实现bias的降低，但可能会出现过拟合问题（high variance）</p>
<p><strong>variance(方差)</strong>：模型预测结果的稳定性，通过简化模型，可以实现variance的降低 计算公式为 $E[(\hat y - E(\hat y)) ^ 2]$</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-5.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/image-20220301152126220.png" class="" title="image-20220301152126220">
<span id="more"></span>
<h3 id="公式角度分析Bias-与-Variance"><a href="#公式角度分析Bias-与-Variance" class="headerlink" title="公式角度分析Bias 与 Variance"></a>公式角度分析Bias 与 Variance</h3><ul>
<li><p>以MSE误差为例，计算bias和variance</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-5.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/image-20220301153351002.png" class="" title="image-20220301153351002">
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-5.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/image-20220301153447334.png" class="" title="image-20220301153447334">
<ul>
<li><p>在样本上求误差均值，首先将f展开为    <script type="math/tex">y = f +\epsilon</script> </p>
</li>
<li><p>再在平方项内添加一个 y_hat期望 <script type="math/tex">E[\hat y]</script></p>
</li>
<li><p>由于 随机误差 $\epsilon$ 均值为0，方差为 $\sigma ^ 2$</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-5.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/image-20220301154214532.png" class="" title="image-20220301154214532">
</li>
<li><p>最终得到 Bias + Variance +  $\sigma ^ 2$ 的形式</p>
</li>
</ul>
</li>
</ul>
<h3 id="偏差与方差之间的关系"><a href="#偏差与方差之间的关系" class="headerlink" title="偏差与方差之间的关系"></a>偏差与方差之间的关系</h3><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-5.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/image-20220301154635423.png" class="" title="image-20220301154635423">
<h3 id="如何降低偏差和方差"><a href="#如何降低偏差和方差" class="headerlink" title="如何降低偏差和方差"></a>如何降低偏差和方差</h3><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML-5.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/image-20220301155342109.png" class="" title="image-20220301155342109">
<ul>
<li><strong>集成学习</strong>可以同时降低两个误差</li>
</ul>
]]></content>
      <categories>
        <category>AI理论</category>
        <category>实用机器学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML_2.数据获取</title>
    <url>/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_2.%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96/</url>
    <content><![CDATA[<h2 id="机器学习-数据"><a href="#机器学习-数据" class="headerlink" title="机器学习-数据"></a>机器学习-数据</h2><p>当使用机器学习技术解决实际问题时，最先要考虑模型输入数据问题，如何获取数据，对数据进行标注、清理以及变换等，以满足模型的输入要求。</p>
<h3 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h3><p>主要有两个获取手段</p>
<ol>
<li>寻找已有的数据集（MNIST ImageNet等等）<ul>
<li><a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research">维基百科数据集总结</a></li>
<li><a href="https://paperswithcode.com/datasets">paperwithcode datasets</a> 论文中常见的数据集</li>
<li><a href="https://www.kaggle.com/datasets">Kaggle Datasets</a> 用户上传数据集</li>
<li><a href="https://datasetsearch.research.google.com/">Google Dataset search</a> 数据集搜索引擎，聚合数据集网站内容</li>
<li><a href="https://huggingface.co/datasets">Hugging Face 数据集</a> 聚焦于文本数据</li>
</ul>
</li>
<li>根据任务，收集，形成自己的数据集</li>
<li>生成数据</li>
</ol>
<span id="more"></span>
<p>三种不同的数据集类型</p>
<ol>
<li>学术数据集<ul>
<li>定义清晰，但是局限于某个小问题</li>
</ul>
</li>
<li>竞赛数据集<ul>
<li>接近于真实的机器学习应用</li>
</ul>
</li>
<li>原始数据<ul>
<li>需要消耗大量的精力预处理</li>
</ul>
</li>
</ol>
<p>当收集到的数据来源不同，我们需要对数据进行集成（integrate）</p>
<ul>
<li><p>类似于数据库中不同表的join，寻找合并key，处理重复项、冲突以及确实</p>

</li>
</ul>
<p>当找不到可用的数据，或者收集到的数据量过小时，可通过<strong>人造(synthetic)数据</strong>增加数据量</p>
<ol>
<li>使用GANs  生成数据</li>
<li>数据增强（augmentation） <ul>
<li>图像反转、拉伸等</li>
<li>文本的反复翻译，改变语序</li>
</ul>
</li>
</ol>
<p><strong>总结</strong></p>

<h3 id="网页数据抓取（Scraping）"><a href="#网页数据抓取（Scraping）" class="headerlink" title="网页数据抓取（Scraping）"></a>网页数据抓取（Scraping）</h3><p> 网页数据抓取是获取数据的有效手段，包括ImageNet在内的很多数据集通过这种方式获取生成，其优缺点包括</p>
<ol>
<li>数据较为原始，包含的无关信息较多</li>
<li>数据量大</li>
</ol>

<p>常用的网页抓取工具</p>
<ol>
<li><p>linux的curl命令（通常会被反爬虫屏蔽）</p>
</li>
<li><p>使用模拟浏览器</p>
</li>
<li>不断更换ip，防止短时间大量相同ip访问导致的屏蔽</li>
</ol>
<h3 id="数据标注"><a href="#数据标注" class="headerlink" title="数据标注"></a>数据标注</h3><p>在完成数据收集后，需要考虑数据的标注问题，根据标签的多少，选取不同的策略</p>
<ol>
<li>标签充足-监督/半监督学习</li>
<li>标签不足-人工标注</li>
<li>标签+经费均不足-弱监督学习</li>
</ol>

<h4 id="半监督学习（Semi-supervised-learning-SSL）-简单理解"><a href="#半监督学习（Semi-supervised-learning-SSL）-简单理解" class="headerlink" title="半监督学习（Semi-supervised learning SSL）- 简单理解"></a>半监督学习（Semi-supervised learning SSL）- 简单理解</h4><blockquote>
<p>Semi-supervised learning is a class of machine learning tasks and techniques that also make use of unlabeled data for training – typically a small amount of labeled data with a large amount of unlabeled data.</p>
<p>李宏毅机器学习视频中有<a href="https://www.bilibili.com/video/BV1JE411g7XF?p=64">详细讲解</a></p>
</blockquote>
<p>半监督训练主要针对训练数据只有少部分已标注，大部分均未标注的情况，假设训练数据满足以下条件</p>
<ol>
<li>一致性假设(Continuity assumption) 具有相同特征的样本标签相同</li>
<li>聚类/簇假设(Cluster assumption) 若数据存在簇结构（即可以聚类），一个簇具有一个标签</li>
<li>流形假设(manifold assumption)  高维数据大致会分布在一个低维的流形上,邻近的样本拥有相似的输出,邻近的程度常用“相似”程度来刻画</li>
</ol>
<p><strong>半监督算法之一：自学习</strong></p>
<p>首先用带标签的数据训练一个模型，在未标注数据上进行预测，获得预测标签，也就是所谓的伪标签（Pseudo-labeled），根据置信度选择部分伪标签数据与标签数据合并训练一个新模型（自学习），不断循环训练，直到模型满足任务要求。</p>

<p><strong>半监督学习算法之二：主动学习（active learning）</strong></p>
<p>与自学训练过程类似，但是每次预测结果<strong>最典型的</strong>未标记样本，由人工标记</p>
<ul>
<li>uncertainty sampling 选取最不确定的样本作为最典型（概率为1/N）</li>
</ul>

<h4 id="Label-through-Crowdsourcing-众包"><a href="#Label-through-Crowdsourcing-众包" class="headerlink" title="Label through Crowdsourcing(众包)"></a>Label through Crowdsourcing(众包)</h4><p>花钱找人标注</p>

<h4 id="弱监督学习（Weak-Supervision）"><a href="#弱监督学习（Weak-Supervision）" class="headerlink" title="弱监督学习（Weak Supervision）"></a>弱监督学习（Weak Supervision）</h4><p>半自动生成标号，虽然比人工标注差，但是足够训练使用，常用手段 </p>
<p><strong>Data programming</strong></p>
<p>人工总结一些规律，设计一些规则（规则匹配），通过程序半自动生成标签</p>


<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4>]]></content>
      <categories>
        <category>AI理论</category>
        <category>实用机器学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML_3.提升数据质量</title>
    <url>/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/</url>
    <content><![CDATA[<h2 id="提升数据质量"><a href="#提升数据质量" class="headerlink" title="提升数据质量"></a>提升数据质量</h2><p>在数据收集完成后，数据可能还存在大量的噪声、或者模型难以使用，需要我们进一步处理提高数据质量</p>
<ol>
<li>数据噪声较多（脏数据过多） - 数据清洗</li>
<li>数据格式与模型要求输入不符 - 数据变换</li>
<li>数据难学习                                - 特征工程</li>
</ol>
<h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><p>针对数据噪声较多的问题，通过数据清洗改善数据质量，常见的Data Error由</p>
<ol>
<li>Outliers(离群值) 某些数据值远远偏离数据整体分布</li>
<li>Rule violations(违反约束) 例如某些非空字段为空</li>
<li>Pattern violation（语义语法冲突）单位是美元，数据给rmb；数据项目标类型是float，实际类型是string</li>
</ol>
<span id="more"></span>
<h4 id="离群检测-Outlier-Detection"><a href="#离群检测-Outlier-Detection" class="headerlink" title="离群检测(Outlier Detection)"></a>离群检测(Outlier Detection)</h4><h4 id="基于规则检测（Rule-based-Detection）"><a href="#基于规则检测（Rule-based-Detection）" class="headerlink" title="基于规则检测（Rule-based Detection）"></a>基于规则检测（Rule-based Detection）</h4><h4 id="基于模式检测（Pattern-based-Detection）"><a href="#基于模式检测（Pattern-based-Detection）" class="headerlink" title="基于模式检测（Pattern-based Detection）"></a>基于模式检测（Pattern-based Detection）</h4><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/image-20220228150552932.png" class="" title="image-20220228150552932">
<h3 id="数据变换（Data-Transformation）"><a href="#数据变换（Data-Transformation）" class="headerlink" title="数据变换（Data Transformation）"></a>数据变换（Data Transformation）</h3><p>实值规范化（Normalization）</p>
<ol>
<li>归一化</li>
<li>Z-score</li>
<li>十进制放缩</li>
<li>log放缩</li>
</ol>
<p>图片转换（减少图片的空间占用）</p>
<ol>
<li>下采样、裁切</li>
<li>图像白化处理（Whitening） 减少像素量</li>
</ol>
<p>视频转换 （预处理以平衡 存储，质量和加载速度 三者之间的关系）</p>
<ul>
<li>通常使用 短视频裁切（&lt;10sec），每个切片内包含单个时间</li>
</ul>
<p>文本转换</p>
<ol>
<li>Stemming and lemmatization 还原词形</li>
<li>Tokenization 切词</li>
</ol>
<h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/image-20220228155016025.png" class="" title="image-20220228155016025">
<p>如何表示特征值？</p>
<ol>
<li><p>直接使用数值表示 或者 桶划分</p>
</li>
<li><p>类别特征可采用独热向量（one-hot）</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/image-20220228155854227.png" class="" title="image-20220228155854227">
</li>
<li><p>日期 按照复合特征处理，划分为子特征</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/image-20220228155800660.png" class="" title="image-20220228155800660">
</li>
<li><p>将几个单独特征组合形成新特征</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/image-20220228160046548.png" class="" title="image-20220228160046548">
</li>
</ol>
<p><strong>文本数据</strong> 特征表示</p>
<ol>
<li><p>one hot 或者 word embedding</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/image-20220228160535728.png" class="" title="image-20220228160535728">
</li>
<li><p>预训练模型做特征提取（上下文语义嵌入 context embedding）</p>
</li>
</ol>
<p><strong>图片/视频</strong> 特征表示</p>
<ol>
<li>传统的手工特征抽取，例如SIFT</li>
<li>深度神经网络特征提取（ResNet）,类似于提取文本特征的预训练网络</li>
</ol>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_3.%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/image-20220228161500068.png" class="" title="image-20220228161500068">]]></content>
      <categories>
        <category>AI理论</category>
        <category>实用机器学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML_4.模型验证</title>
    <url>/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_4.%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/</url>
    <content><![CDATA[<h2 id="模型验证（Validation）"><a href="#模型验证（Validation）" class="headerlink" title="模型验证（Validation）"></a>模型验证（Validation）</h2><h3 id="验证（validation）集与测试（test）集"><a href="#验证（validation）集与测试（test）集" class="headerlink" title="验证（validation）集与测试（test）集"></a>验证（validation）集与测试（test）集</h3><p>验证集往往是从训练集中划分出的一部分数据，用来验证模型泛化能力，可以使用多次；测试集是单独的一系列数据，在模型训练完成后，衡量模型效果，一般只使用一次</p>
<h3 id="如何生成验证集"><a href="#如何生成验证集" class="headerlink" title="如何生成验证集"></a>如何生成验证集</h3><h4 id="随机划分"><a href="#随机划分" class="headerlink" title="随机划分"></a>随机划分</h4><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_4.%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/image-20220301142501284.png" class="" title="image-20220301142501284">
<h4 id="特殊情况"><a href="#特殊情况" class="headerlink" title="特殊情况"></a>特殊情况</h4><p>某些情况下，训练数据可能不适合采用随机划分的方式验证集合，如</p>
<ol>
<li>具有序列关系的数据-股价、房子销售<ul>
<li>验证集数据应在训练集后，避免模型训练使用到了验证集的未来信息，导致模型在验证集上的表现较好</li>
</ul>
</li>
<li>训练数据由不同组，每个组有多个样本-同一个人的多个照片<ul>
<li>以组为单位进行随即划分</li>
<li>一百组照片，选70个人训练，30个人验证</li>
</ul>
</li>
<li>类别不均衡数据<ul>
<li>对于较小类更多的采样</li>
</ul>
</li>
</ol>
<span id="more"></span>
<h4 id="K-fold-Cross-Validation-k折交叉验证"><a href="#K-fold-Cross-Validation-k折交叉验证" class="headerlink" title="K-fold Cross Validation k折交叉验证"></a>K-fold Cross Validation k折交叉验证</h4><p>将数据集划分为k个子集，每次选取一个子集作为有验证集，其余k个集合合并为训练集，重复k次，用k次平均验证误差作为验证集误差。</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_4.%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/image-20220301144205630.png" class="" title="image-20220301144205630">
<h3 id="有关验证集的常见错误（common-mistakes）"><a href="#有关验证集的常见错误（common-mistakes）" class="headerlink" title="有关验证集的常见错误（common mistakes）"></a>有关验证集的常见错误（common mistakes）</h3><ol>
<li>验证集中有来自训练集的样本<ul>
<li>数据集中有重复样本</li>
</ul>
</li>
<li>数据泄露（leaking）<ul>
<li>训练时用到了验证集未来的数据（时间序列分析）</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>AI理论</category>
        <category>实用机器学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML_6.集成学习</title>
    <url>/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="集成学习（Ensemble-Learning）"><a href="#集成学习（Ensemble-Learning）" class="headerlink" title="集成学习（Ensemble Learning）"></a>集成学习（Ensemble Learning）</h2><blockquote>
<p>In <a href="https://en.wikipedia.org/wiki/Statistics">statistics</a> and <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>, <strong>ensemble methods</strong> use multiple learning algorithms to obtain better <a href="https://en.wikipedia.org/wiki/Predictive_inference">predictive performance</a> than could be obtained from any of the constituent learning algorithms alone.</p>
</blockquote>
<h3 id="Bagging-Bootstrap-Aggregating"><a href="#Bagging-Bootstrap-Aggregating" class="headerlink" title="Bagging-Bootstrap Aggregating"></a>Bagging-Bootstrap Aggregating</h3><p>Bagging主要思路是通过结合几个模型(<strong>多个相同模型</strong>)降低总体的泛化误差（bias+viariance），实际上降低的是<strong>方差</strong></p>
<ol>
<li>同时训练多个模型（parallel）</li>
<li>输出取模型平均值（回归模型）或投票（分类模型）</li>
</ol>
<p>其中每个模型训练数据通过bootstrap sampling方式采样</p>
<ol>
<li>有放回采样，m个训练数据，进行m次又放回采样</li>
<li>大概有 $1- \frac{1}e$ 63%的数据在一次bootstrap采样中未被采样到（out of bag），可以作为模型的验证集</li>
</ol>
<span id="more"></span>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220302112353956.png" class="" title="image-20220302112353956">
<h4 id="Random-Forest-特殊的bagging"><a href="#Random-Forest-特殊的bagging" class="headerlink" title="Random Forest-特殊的bagging"></a>Random Forest-特殊的bagging</h4><ul>
<li>基学习器为决策树</li>
<li>bootstrap采样过程中，不仅对样本进行随机采样，同样对属性列进行采样，增加决策树的多样性</li>
</ul>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220302112534348.png" class="" title="image-20220302112534348">
<h4 id="Unstable-Learners"><a href="#Unstable-Learners" class="headerlink" title="Unstable Learners"></a>Unstable Learners</h4><p>对于不稳定（方差较大）模型效果提升较好</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303083634112.png" class="" title="image-20220303083634112">
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303083728465.png" class="" title="image-20220303083728465">
<ul>
<li>当 $h(x)$ 即模型预测值，方差较小，上述不等式趋向于等号</li>
<li>极端情况，当 $h(x)$ 恒相等时， $E(h(x))^2 = E(h(x)^2)$ , 使用bagging对模型效果没有提升</li>
<li>由上公式易得，bagging能够通过较少viarance，降低模型泛化误差，模型方差越大，提升效果越好</li>
</ul>
<p>决策树就是一种 unstable learner,而线性回归是稳定的</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303084813664.png" class="" title="image-20220303084813664">
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303084859931.png" class="" title="image-20220303084859931">
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>将一系列的弱模型(<strong>多个相同模型</strong>)结合组成一个强模型，以降低模型的偏差（bias）</p>
<ul>
<li>按照顺序学习n个弱模型，每次模型训练完成后，根据该模型预测错误部分对数据重新采样，训练下一个模型</li>
<li>不断的迭代，“在bias上不断地boosting”</li>
</ul>
<h4 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h4><p>每次模型训练目标为拟合残差（有点类似于梯度拟合）</p>
<ol>
<li>训练新模型 $h_t$ 时，其<strong>训练目标</strong>为 <strong>预测真实值$y_i$</strong> 与 <strong>已训练模型预测值和 $H_{t}(x_i)$</strong>的差</li>
<li>累积得到新的预测值和 $H_{t+1}(x) = H{t}(x) + \eta h_t(x)$, 其中 $\eta$ 为正则项系数，为了避免模型过度拟合</li>
</ol>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303090038631.png" class="" title="image-20220303090038631">
<p>当损失函数为MSE，即均方误差损失时，实际上残差即为负梯度，Gradient 名字的由来</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303091042627.png" class="" title="image-20220303091042627">
<h4 id="Gradient-Boosting-Decision-Tree-GBDT"><a href="#Gradient-Boosting-Decision-Tree-GBDT" class="headerlink" title="Gradient Boosting  Decision Tree(GBDT)"></a>Gradient Boosting  Decision Tree(GBDT)</h4><p>使用决策树作为弱学习模型，问题就是构建、训练时间较长</p>
<ul>
<li>使用强模型，Gradient Boosting容易出现过拟合现象</li>
<li>通过限制决策树的层数（2-3）层，控制过拟合</li>
</ul>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303091517929.png" class="" title="image-20220303091517929">
<p>Boosting VS 随机森林</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303091554319.png" class="" title="image-20220303091554319">
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p>降低 <strong>偏差 偏差 偏差 ！！！</strong></p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303091702515.png" class="" title="image-20220303091702515">
<h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><p>类似于bagging,将不同的基模型连接起来，共同预测结果以降低方差（variance）</p>
<ul>
<li><p>基模型可以是不同的类别</p>
</li>
<li><p>最后由一个线性全连接层，输入各个模型的输出，输出目标结果</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303092746542.png" class="" title="image-20220303092746542">
</li>
</ul>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303092311096.png" class="" title="image-20220303092311096">
<h4 id="Multi-layer-Stacking"><a href="#Multi-layer-Stacking" class="headerlink" title="Multi-layer Stacking"></a>Multi-layer Stacking</h4><p>多层堆叠模型，降低模型偏差（bias）</p>
<ul>
<li>容易过拟合</li>
</ul>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303094206264.png" class="" title="image-20220303094206264">
<p>如何降低过拟合问题</p>
<ol>
<li>将数据划分A和B两部分，其中A部分用来训练第一层，并用第一层预测B，预测结果作为第二层输入训练第二层（避免了原来多层堆叠出现的一个在不同层重复训练的情况）</li>
<li>重复 k-fold bagging <ul>
<li>使用k-fold训练k个模型（在多层堆叠中特指<strong>第一层的所有模型</strong>）</li>
<li>将每个模型在k-fold训练中作为验证集部分数据的输出拼接成一个份完整的第一层输出，输入到第二层训练</li>
<li><strong>进一步降低方差</strong>：重复n次k折交叉验证,每个数据均有n个输出，对n个输出做平均，获得一个完整的第一层输出，输入到第二层训练</li>
</ul>
</li>
</ol>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303095451723.png" class="" title="image-20220303095451723">
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303095533009.png" class="" title="image-20220303095533009">
<h4 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h4><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303095827469.png" class="" title="image-20220303095827469">
<h3 id="集成学习总结"><a href="#集成学习总结" class="headerlink" title="集成学习总结"></a>集成学习总结</h3><p>简单理解就是</p>
<ul>
<li>增加模型数量，学习相同内容,可以降低 Variance</li>
<li>增加模型，不断学习误差，可以降低Bias</li>
<li>K-fold multi-level stacking 通过横向增加模型数量，纵向增加层数，可以既降低Bias，又降低 Variance</li>
<li>由于多个模型容易出现过拟合现象<ul>
<li>多模型降低Variance，通过 特殊采样方法（bagging的bootstrap，staking的k-fold）避免过拟合</li>
<li>降低Bias,通过 正则项 避免过拟合</li>
</ul>
</li>
</ul>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_6.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20220303100132120.png" class="" title="image-20220303100132120">
]]></content>
      <categories>
        <category>AI理论</category>
        <category>实用机器学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML_7.模型调参</title>
    <url>/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/</url>
    <content><![CDATA[<h2 id="模型调参（Tuning）"><a href="#模型调参（Tuning）" class="headerlink" title="模型调参（Tuning）"></a>模型调参（Tuning）</h2><p>模型参数对模型效果有影响较大， 在不同的问题中，我们不仅需要选择合适的模型，也要选择合适的参数</p>
<ol>
<li><p>手动调参（Manual）</p>
<ul>
<li><p>从默认参数（工具包默认参数、论文推荐参数）开始</p>
</li>
<li><p>不断调整参数，记录调整后的结果（tensorboard, weight &amp; bias）</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220303145559540.png" class="" title="image-20220303145559540">
</li>
</ul>
</li>
<li><p>自动调参（Automated）</p>
<ul>
<li><p>计算成本在下降，人力成本在上升</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220303145731538.png" class="" title="image-20220303145731538">
</li>
</ul>
</li>
<li><p>AutoML(Automated Machine Learning)</p>
<ul>
<li>从数据清洗、特征提取到模型选择每一步均由AutoML自动完成</li>
<li>目前模型选择之前的步骤，AutoML完成效果并不好，主要能够解决的两个问题<ol>
<li>HPO（Hyperparameter optimization）选择合适的超参数</li>
<li>NAS（Neural architecture search）网络架构搜索</li>
</ol>
</li>
</ul>
</li>
</ol>
<span id="more"></span>
<h3 id="HOP-Algorithm-超参数选择"><a href="#HOP-Algorithm-超参数选择" class="headerlink" title="HOP Algorithm 超参数选择"></a>HOP Algorithm 超参数选择</h3><p>首先确定参数的搜索空间，例如</p>
<ul>
<li>避免超参数空间过大，导致搜索成本过高</li>
</ul>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304082552518.png" class="" title="image-20220304082552518">
<p>在搜索空间上采取特定的搜索算法进行搜索，主要分为两大类别</p>
<ol>
<li>Black-box 每个参数集选择进行一次完整的训练，训练完成后比较不同参数集模型效果</li>
<li>Multi-fidelity 修改训练过程，降低搜索代价<ul>
<li>在训练数据子集上训练</li>
<li>减小模型大小（减少层数，减少channel等）</li>
<li>提前终止某些效果明显较差的参数实验</li>
</ul>
</li>
</ol>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304083338601.png" class="" title="image-20220304083338601">
<h4 id="两类常见的HPO算法"><a href="#两类常见的HPO算法" class="headerlink" title="两类常见的HPO算法"></a>两类常见的HPO算法</h4><ol>
<li><p>Grid Search 网格搜索</p>
<ul>
<li>传入不同参数的多个值，搜索所有参数不同值的所有组合</li>
<li>获得 最优的参数值组合</li>
<li>随着参数数量的增加，组合数量呈指数级上升，训练成本较高</li>
</ul>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304083750975.png" class="" title="image-20220304083750975">
</li>
<li><p>Random Search</p>
<ul>
<li>与Grid Search类似，但是只随机选择n次参数组合</li>
<li>只搜索一部分参数组合空间，一定程度减少了训练成本</li>
</ul>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304083937025.png" class="" title="image-20220304083937025">
</li>
</ol>
<h4 id="Bayesian-Optimization-BO-贝叶斯优化-不深入理解"><a href="#Bayesian-Optimization-BO-贝叶斯优化-不深入理解" class="headerlink" title="Bayesian Optimization(BO) 贝叶斯优化-不深入理解"></a>Bayesian Optimization(BO) 贝叶斯优化-不深入理解</h4><p>通过不断地对数据进行采样（超参数与模型评价指标的对应关系）学习一个从<strong>超参数</strong>到<strong>模型评价指标</strong>（用该超参数训练出来的模型的评价指标）之间的函数，每次采样会根据以往的采样结果，选取采样点(online learning的感觉)。</p>
<p><strong>Surrogate model</strong></p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304090035219.png" class="" title="image-20220304090035219">
<p><strong>Acquisition function</strong></p>
<ul>
<li>每次新采样点选取 <strong>Acquisition max</strong>，即时Acquisition function最大的点</li>
<li><strong>Acquisition max</strong>的样本点 <strong>约等于</strong> <strong>置信区间较大 + 可能取得较高评价指标</strong>的点（<strong>简单理解</strong>：置信区间小我就没必要再采样了，直接使用模型预测超参数；较高指标的超参数是模型追求的目标，总结就是 <strong>既要增加可信度又要找到最优解</strong>）</li>
<li>Trade off exploration and exploitation 在 <strong>探索未知解</strong> 和 <strong>最优解附近深挖</strong> 的权衡</li>
</ul>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304090743169.png" class="" title="image-20220304090743169">
<p><strong>BO算法的局限性</strong></p>
<ul>
<li>算法的最初始阶段，近似于随机搜索</li>
</ul>
<h4 id="Successive-Halving"><a href="#Successive-Halving" class="headerlink" title="Successive Halving"></a>Successive Halving</h4><p>算法如其名，比较简单</p>
<ol>
<li>随机选择n个超参数组合，训练m轮</li>
<li>每m轮训练完毕，删除 n/2 个指标相对较差的超参数组合</li>
<li>不断重复第二步，直到只剩下一个超参数组合</li>
</ol>
<p>根据具体的预算，选取n和m</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304092244412.png" class="" title="image-20220304092244412">
<h4 id="Hyperband-实际使用比较多的算法"><a href="#Hyperband-实际使用比较多的算法" class="headerlink" title="Hyperband-实际使用比较多的算法"></a>Hyperband-实际使用比较多的算法</h4><p>在Successive Halving算法中，n和m的选取很大程度上会影响最后获得最优超参数组合</p>
<ul>
<li>资源固定的情况下，<strong>m*n = budget</strong> </li>
<li>当m选取不够大时，有些超参数组合可能在前m个epoch指标表现不好（训练不充分），导致提前被淘汰</li>
<li>当n选取不够大时，随机采样的超参数组合可能过少，无法找到最优解</li>
</ul>
<p>Hyperband算法针对Successive Halving算法的以上问题，进行了针对性改进</p>
<ol>
<li>运行多次 Successive Halving算法，每次运行降低n，增加m</li>
<li>先 exploration  再 exploitation,先 BFS 再 DFS</li>
<li>单次成本不变，多次运行，最后还是增加计算成本</li>
</ol>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304093734300.png" class="" title="image-20220304093734300">
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304093845224.png" class="" title="image-20220304093845224">
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304094115462.png" class="" title="image-20220304094115462">
<h3 id="Neural-Architecture-Search-NAS-Algorithm"><a href="#Neural-Architecture-Search-NAS-Algorithm" class="headerlink" title="Neural Architecture Search(NAS) Algorithm"></a>Neural Architecture Search(NAS) Algorithm</h3><p>超参数选择中，除了学习率、epoch、batchsize等的训练超参数，还包括模型层数、隐藏层维度等的涉及到模型结构的参数</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304110012929.png" class="" title="image-20220304110012929">
<p>NAS算法用来解决模型结构相关参数的选择</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304110121568.png" class="" title="image-20220304110121568">
<h4 id="NAS-with-Reinforcement-Learning"><a href="#NAS-with-Reinforcement-Learning" class="headerlink" title="NAS with  Reinforcement Learning"></a>NAS with  Reinforcement Learning</h4><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304110328974.png" class="" title="image-20220304110328974">
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304110853192.png" class="" title="image-20220304110853192">
<h4 id="The-One-shot-Approach（思想）"><a href="#The-One-shot-Approach（思想）" class="headerlink" title="The One-shot Approach（思想）"></a>The One-shot Approach（思想）</h4><ul>
<li><p>将网络架构与模型参数的学习连接在一起（既学网络架构，又学网络参数）</p>
</li>
<li><p>训练一个大模型，其子模型为各种各样的备选架构（先搞大杂烩，再从大杂烩里挑我要的）</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304111150887.png" class="" title="image-20220304111150887">
</li>
<li><p>问题在于 大模型资源耗费过大，如何解决？</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304111204080.png" class="" title="image-20220304111204080">
</li>
</ul>
<p><strong>Differentiable Architecture Search</strong></p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304144329266.png" class="" title="image-20220304144329266">
<ul>
<li><p>每一层有多个候选网络/结构/模型</p>
</li>
<li><p>第l层的第i个候选网络的输出为 $0_i^l$</p>
</li>
<li><p>每一层后 select模块，权重为 $a^l$ ，对该层所有候选模型输出加权求和输入到 $l + 1 $ 层</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304144536920.png" class="" title="image-20220304144536920">
</li>
<li><p>最后选择每层中 参数最大的 候选模型</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304144709664.png" class="" title="image-20220304144709664">
</li>
</ul>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304143711492.png" class="" title="image-20220304143711492">
<h4 id="Scaling-CNNs"><a href="#Scaling-CNNs" class="headerlink" title="Scaling CNNs"></a>Scaling CNNs</h4><p>对于卷积神经网络，有三种调整架构的方式</p>
<ul>
<li>更深：增加层数</li>
<li>更宽：增加channel</li>
<li>更大的输入：提高输入图像分辨率</li>
</ul>
<p>EfficientNet 提出了一种Scaling方式（调整一个，其他两个同时一起调节）</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304145127431.png" class="" title="image-20220304145127431">
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_7.%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/image-20220304145828105.png" class="" title="image-20220304145828105">]]></content>
      <categories>
        <category>AI理论</category>
        <category>实用机器学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML_8.深度神经网络</title>
    <url>/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h2 id="深度学习模型设计模式"><a href="#深度学习模型设计模式" class="headerlink" title="深度学习模型设计模式"></a>深度学习模型设计模式</h2><p>随着深度学习的不断发展，各式各样包括bert、transformer在内的深度神经网络架构层出不穷，在这些不同的网络架构中往往包含一些通用的深度网络设计模式，主要理解三种：</p>
<ol>
<li>batch/layer normalization</li>
<li>残差</li>
<li>注意力机制</li>
</ol>
<h3 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h3><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>对输入的一个batch进行标准化处理（减均值，除以方差），能够有效的降低深度网络的学习难度</p>
<ul>
<li>l从梯度角度理解，当从x-&gt;y的梯度变化 $\beta$ 变化较大时，梯度下降时沿着x点梯度移动，学习会出现偏差(斜率替代每个点的导数，如果导数变化过大，沿着斜率走就会偏离函数图像)</li>
<li>对于分布变化较大的数据要采用较小的学习率，否则就需要通过Batch Normalization对输入进行平滑化（smooth）</li>
<li>只对线性方法起作用，深度神经不起作用</li>
</ul>
<span id="more"></span>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307084121951.png" class="" title="image-20220307084121951">
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307084716730.png" class="" title="image-20220307084716730">
<p>批量归一化的主要步骤：</p>
<ol>
<li><p>首先将输入转化为2D形式数据</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307084939777.png" class="" title="image-20220307084939777">
</li>
<li><p>标准化</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307084955220.png" class="" title="image-20220307084955220">
</li>
<li><p>还原</p>
<ul>
<li>$\beta_j $ $\gamma_j$两个参数神经网络学习决定</li>
<li>形式上理解为 由模型通过学习决定是否真的需要normalization</li>
</ul>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307085101509.png" class="" title="image-20220307085101509">
</li>
<li><p>转换为输入形状，输出</p>
</li>
</ol>
<p>实现代码如下</p>
<ol>
<li><p>训练过程中 并不保留所有batch的均值和方差平均值 供预测使用</p>
</li>
<li><p>每次使用动量更新一个新的moving_var,moving_mean在预测中使用</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307090421825.png" class="" title="image-20220307090421825">
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307090430766.png" class="" title="image-20220307090430766">
</li>
</ol>
<p>完整代码：</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307090218611.png" class="" title="image-20220307090218611">
<h4 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h4><blockquote>
<p>BN在batch上做归一化，LN在样本上做归一化</p>
</blockquote>
<p>由于RNN循环计算特性，不同时间步类似于不同的batch，均值和方差变化较大，无法共享使用（若输入长度为10，需要维护10个不同时间步的均值和方差，当预测长度为20时，后10个时间步在训练过程中没有均值和方差）</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307091707388.png" class="" title="image-20220307091707388">
<p>相较于Batch Normalization，将时间步看作为batch维度，reshape维度改变，其他操作不同</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307091911670.png" class="" title="image-20220307091911670">
<h4 id="More-Normalization"><a href="#More-Normalization" class="headerlink" title="More Normalization"></a>More Normalization</h4><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_8.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20220307092724701.png" class="" title="image-20220307092724701">
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>Normalization的主要目的是为了 使目标函数平滑化，降低模型学习难度（即可以使用更大的学习率）</li>
<li>一般步骤包括<ul>
<li>reshape</li>
<li>normalization</li>
<li>recovery</li>
</ul>
</li>
</ul>
<h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p>略</p>
<h3 id="残差"><a href="#残差" class="headerlink" title="残差"></a>残差</h3><p>略</p>
]]></content>
      <categories>
        <category>AI理论</category>
        <category>实用机器学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>实用ML_9.迁移学习</title>
    <url>/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><p>由于深度网络的训练对数据要求较大，且训练成本较高，对在其他任务训练好的深度模型使用到相关任务的需求较大，主要方式有</p>
<ol>
<li>使用深度模型做特征抽取（bert，word2vec等），输入到不同模型，解决不同任务</li>
<li>在方便训练的相近的任务（可能是 训练数据充足等）上训练模型，在目标任务上重用训练好的模型</li>
<li>微调预训练模型</li>
</ol>
<h3 id="Fine-tuning（从CV方向进行讲解）"><a href="#Fine-tuning（从CV方向进行讲解）" class="headerlink" title="Fine-tuning（从CV方向进行讲解）"></a>Fine-tuning（从CV方向进行讲解）</h3><p>CV领域存在很多良好的数据集（imagenet），如何将在这些大数据集上训练好的模型（学到的知识）迁移到自己的任务上来？这就是Fine-tuning要做的任务。</p>
<h4 id="Pre-trained-Model"><a href="#Pre-trained-Model" class="headerlink" title="Pre-trained Model"></a>Pre-trained Model</h4><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220307144746729.png" class="" title="image-20220307144746729">
<span id="more"></span>
<h4 id="如何进行Fine-Tuning"><a href="#如何进行Fine-Tuning" class="headerlink" title="如何进行Fine-Tuning"></a>如何进行Fine-Tuning</h4><p>模型构建</p>
<ul>
<li>直接使用预训练模型的所有参数和架构</li>
<li>只随机初始化最后一个输出层的参数</li>
</ul>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220307145106552.png" class="" title="image-20220307145106552">
<p>模型学习</p>
<ul>
<li>使用小学习率寻找解（预训练模型已在解附近，小步探索）</li>
</ul>
<h4 id="Freeze-Bottom-Layers"><a href="#Freeze-Bottom-Layers" class="headerlink" title="Freeze Bottom Layers"></a>Freeze Bottom Layers</h4><p>深度神经网络不同层在具体任务中捕捉不同层级的特征信息，更接近输出层的网络层更加倾向于捕捉<strong>任务相关特征</strong>（task specific），底层网络更加倾向于<strong>捕捉通用的、一般的特征</strong>（图片中的曲线、边等）</p>
<p>针对预训练模型该特性以及要Fine-tuning目标任务，对底层和高层网络采用不同训练手段</p>
<ul>
<li>底层冻结或者低学习率</li>
<li>高层正常训练</li>
</ul>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220307150714650.png" class="" title="image-20220307150714650">
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220307150826148.png" class="" title="image-20220307150826148">
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220307151536617.png" class="" title="image-20220307151536617">
<h3 id="Fine-tuning-in-NLP"><a href="#Fine-tuning-in-NLP" class="headerlink" title="Fine-tuning in NLP"></a>Fine-tuning in NLP</h3><p>NLP领域面临的问题与CV不同，NLP没有大规模良好标记的数据集，只有大量未标记的文本数据（维基百科、电子书、爬取的数据等）</p>
<p>只能使用自监督学习学习的方式进行模型预训练</p>
<ul>
<li>生成伪标签，然后在伪标签上进行监督学习</li>
<li>主要的两种类型<ol>
<li>语言模型：给定序列预测下一个词</li>
<li>Masked language model：给定序列，预测序列中的某一个词</li>
</ol>
</li>
</ul>
<h4 id="常见预训练模型"><a href="#常见预训练模型" class="headerlink" title="常见预训练模型"></a>常见预训练模型</h4><ol>
<li>词嵌入模型：获得包含语义信息的词向量</li>
<li>基于transformer的预训练模型<ul>
<li>BERT：encoder  适用于文本分类等的NLU任务</li>
<li>GPT: decoder 适用于摘要、总结等NLG任务</li>
<li>T5: encoder-decoder 适用于摘要、总结等NLG任务</li>
</ul>
</li>
</ol>
<p><strong>bert微调小技巧</strong></p>
<ul>
<li><p>在将bert应用到自己任务时，可以将bert接近输出层的几层权重默认初始化</p>
</li>
<li><p>Hugging Face 获取预训练模型</p>
<img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220308085742505.png" class="" title="image-20220308085742505">
</li>
</ul>
<h4 id="应用-1"><a href="#应用-1" class="headerlink" title="应用"></a>应用</h4><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220308085949407.png" class="" title="image-20220308085949407">
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><img src="/2022/03/08/AI%E5%AD%A6%E4%B9%A0/DL%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8ML_9.%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/image-20220308090255661.png" class="" title="image-20220308090255661">]]></content>
      <categories>
        <category>AI理论</category>
        <category>实用机器学习</category>
      </categories>
      <tags>
        <tag>实用机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>搜索算法整理</title>
    <url>/2022/06/08/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h2 id="搜索相关算法"><a href="#搜索相关算法" class="headerlink" title="搜索相关算法"></a>搜索相关算法</h2><p><em>开头废话：最近这几周搞辣鸡项目花了太长时间，好久没写博客了，整理一下这几周写的搜索算法</em></p>
<p>通过枚举遍历问题的解空间，实现问题的求解，搜索作为比较基础的算法问题之一，题目数量众多，问题难度可难可易，希望通过这篇博客的整理能够加深我对目前学习到的相关搜索算法的理解。</p>
<h4 id="主要的算法"><a href="#主要的算法" class="headerlink" title="主要的算法"></a>主要的算法</h4><ol>
<li>DFS 深度优先搜索</li>
<li>BFS 广度优先搜索</li>
<li>双向搜索</li>
<li>Best First Search 最佳优先搜索</li>
<li>迭代加深搜索</li>
<li>其他</li>
</ol>
<h4 id="基础搜索算法"><a href="#基础搜索算法" class="headerlink" title="基础搜索算法"></a>基础搜索算法</h4><p>最简单也是最常用的两个搜索算法（简单总结）</p>
<ol>
<li>深度优先搜索<ul>
<li>每次递归首先尝试向更深的结点走</li>
</ul>
</li>
<li>广度优先搜索<ul>
<li>每次枚举穷尽同一层的所有结点</li>
</ul>
</li>
</ol>
<p>其中深度优先搜索更适合搜索解空间深度与目标解<strong>深度相近</strong>类型问题，广度优先遍历更适合搜索目标解深度<strong>一定程度小于</strong>目标解深度类型问题</p>
<span id="more"></span>
<h3 id="双向搜索"><a href="#双向搜索" class="headerlink" title="双向搜索"></a>双向搜索</h3><p>与普通DFS和BFS不同的点在于，双向搜索同时从两个”方向“开始搜索，这里的方向包括两种形式</p>
<ol>
<li>从初始状态正向搜索+目标状态逆向搜索</li>
<li>将问题划分为两个规模为一半的子问题，分别进行搜索（meet in middle）</li>
</ol>
<p>双向搜索能够使得遍历解空间数量<strong>在幂级上缩小一半</strong></p>
<h4 id="正-逆双向搜索（BFS）"><a href="#正-逆双向搜索（BFS）" class="headerlink" title="正/逆双向搜索（BFS）"></a>正/逆双向搜索（BFS）</h4><p>基于BFS正/逆双向搜索，问题需要满足<strong>目标解已知</strong>，否则无法进行双向搜索，以深度为N的二叉问题为例</p>
<ul>
<li><p>若使用普通的BFS，找到解的BFS深度为N，时间复杂度为O(2^N)</p>
</li>
<li><p>双向bfs每次从初始状态和目标状态遍历深度加一，相当于两个深度为N/2的BFS，时间复杂度为0(2^(N/2))</p>
</li>
</ul>
<p>伪代码模板（自己写的）如下：</p>
<ul>
<li><p>两个访问标记，两个队列</p>
</li>
<li><p>每次训练，正向bfs一层，逆向bfs一层</p>
</li>
<li><p>终止条件：正/逆向bfs过程中，新状态存在于另一个方向的访问标记中；或队列为空</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//主函数每次循环，正向和逆均搜索一层</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; !forwardQue.isEmpty() || !backwardQue.isEmpty(); i++) &#123;</span><br><span class="line">            <span class="comment">//正向搜索一层</span></span><br><span class="line">            <span class="keyword">if</span> (!forwardQue.isEmpty() &amp;&amp; bfs(forwardVisitedMap, backwardVisitedMap, forwardQue, i)) <span class="keyword">return</span>;</span><br><span class="line">            <span class="comment">//逆向搜索一层</span></span><br><span class="line">            <span class="keyword">if</span> (!backwardQue.isEmpty() &amp;&amp; bfs(backwardVisitedMap, forwardVisitedMap, backwardQue, i)) <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//修改终止条件的bfs</span></span><br><span class="line">bfs(forwardVisitedMap, backwardVisitedMap, forwardQue, <span class="keyword">int</span> curDepth) &#123;</span><br><span class="line">        <span class="keyword">int</span> tempSize = forwardQue.size();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; tempSize; j++) &#123;</span><br><span class="line">            curState = forwardQue.poll();</span><br><span class="line">            <span class="comment">//遍历所有子状态</span></span><br><span class="line">            <span class="keyword">for</span>(childState of curState)&#123;</span><br><span class="line">                <span class="comment">//若逆向的map中包含当前状态，说明双向搜索相交，找到最优解</span></span><br><span class="line">                <span class="keyword">if</span>(backwardVisitedMap.contains(childState))&#123;</span><br><span class="line">                    <span class="comment">//记录或者输出解空间</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//否则按照普通的bfs继续执行</span></span><br><span class="line">                <span class="keyword">if</span>(!forwardVisitedMap.contains(childState))&#123;</span><br><span class="line">                    forwardVisitedMap.put(childState， curDepth);</span><br><span class="line">                    forwardQue.offer(childState)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>例题：</p>
<ul>
<li><p><a href="https://www.luogu.com.cn/problem/P1032">P1032 [NOIP2002 提高组] 字串变换</a> BFS VS 双向BFS </p>
<ul>
<li>时间空间复杂度均降低</li>
</ul>
<img src="/2022/06/08/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/image-20220530202815963.png" class="" title="image-20220530202815963">
</li>
</ul>
<h4 id="Meet-in-middle"><a href="#Meet-in-middle" class="headerlink" title="Meet in middle"></a>Meet in middle</h4><p>将规模为N问题，从中间拆分为两个N/2子问题，两个子问题分别进行dfs+状态存储，最后两个状态组合求解，已N个物品的背包问题为例</p>
<ul>
<li>与双向dfs相同，时间复杂度从 O(2^N)降低到0(2^(N/2))</li>
<li>空间复杂度由于要存储中间状态，最坏情况下为0(2^(N/2))</li>
</ul>
<p>这类问题的难点其实在判断直接暴力是否超时</p>
<ul>
<li>一般刷题平台的时间限制是1秒或2秒，<strong>操作次数应该控制在 $10^{7-9} \approx 2^{23-25}$ 左右（按照这个标准进行判断）</strong></li>
</ul>
<p>伪代码模板：</p>
<ul>
<li>另一种解法时前后dfs获得的状态均存下来，最后一块求解（我觉得空间复杂度太大，没用过）</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dfs1</span><span class="params">(depth, status)</span></span>&#123;</span><br><span class="line">     <span class="comment">//到中间停止dfs</span></span><br><span class="line">     <span class="keyword">if</span>(depth == N / <span class="number">2</span>)&#123;</span><br><span class="line">         <span class="comment">//不需要state，只需要value 直接存储在列表</span></span><br><span class="line">         states.add(value)</span><br><span class="line">         <span class="comment">//需要记录状态的数组或者Map中</span></span><br><span class="line">         states[status] = value</span><br><span class="line">         <span class="keyword">return</span>;</span><br><span class="line">     &#125;</span><br><span class="line">new_status = <span class="comment">//根据规则+status确定新的status</span></span><br><span class="line">     dfs1(depth + <span class="number">1</span>, new_status);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dfs2</span><span class="params">(depth, status)</span></span>&#123;</span><br><span class="line">     <span class="comment">//到N国模</span></span><br><span class="line">     <span class="keyword">if</span>(depth == N)&#123;</span><br><span class="line">         <span class="comment">//找到states数组中需要的value+当前dfs结果组成目标解</span></span><br><span class="line">        	<span class="comment">//可能会用到：二分查找</span></span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">return</span>  dfs1(depth + <span class="number">1</span>, new_status);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">     <span class="comment">//首先第一个dfs，求前半部分状态</span></span><br><span class="line">     dfs1(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">     <span class="comment">//第二个dfs求第二部分状态，同时与第一个部分状态组合形成解</span></span><br><span class="line">     dfs2(N/<span class="number">2</span>);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>例题：</p>
<ul>
<li><a href="https://www.acwing.com/problem/content/description/173/">AcWing171. 送礼物</a> <ul>
<li>暴力：$O(2^{46})$  动态规划：$O(46*2^{31}) $  明显超时</li>
<li>双向DFS：$O(2^{23})$</li>
</ul>
</li>
<li><a href="https://www.luogu.com.cn/problem/P3067">P3067 [USACO12OPEN]Balanced Cow Subsets G</a><ul>
<li>暴力：$O(2^{40})$  动态规划：$O(40*2^{18}) $  明显超时，且动态规划方法会超内存</li>
<li>双向DFS：$O(2^{20})$</li>
</ul>
</li>
<li><a href="https://www.luogu.com.cn/problem/P3067">P3067 [USACO12OPEN]Balanced Cow Subsets G</a>（知识点比较全的一道题 <strong>重点！！！</strong>）<ul>
<li>这道题双向DFS：$O(3^{20}) \approx O(10^{9})$</li>
<li>使用二分查找搜索状态会导致时间复杂度增加一个数量级超时，<strong>map存储状态</strong></li>
</ul>
</li>
</ul>
<h3 id="Best-First-Search-最佳优先搜索"><a href="#Best-First-Search-最佳优先搜索" class="headerlink" title="Best First Search 最佳优先搜索"></a>Best First Search 最佳优先搜索</h3><p>最佳优先搜索是一种结合了贪心的搜索思想：如果任意时刻能够近似估计到达目标点的成本，每次选取最底层进行扩展搜索，以降低搜索空间。主要的搜索算法有</p>
<ol>
<li>Greedy/Beam Search<ul>
<li>最简单应用贪心思路到搜索上的算法，每次扩展选择估计成本最低的结点加入到路径中（多个就是beam search）</li>
<li>这种方法简单且暴力，但是存在贪心算法的普遍问题，即可能<strong>无法找到最优解</strong></li>
</ul>
</li>
<li>Dijkstra’s algorithm<ul>
<li>每次选择距离出发点成本最低的点进行扩展</li>
<li>能够保证找到最优解，但是搜索复杂度还是较高（剪枝效率不高）</li>
<li>采用了堆优化的算法<strong>时间复杂度</strong>为 $O(E + V logV)$，空间复杂度为  $O(logV)$（其中 $E$ 为边数，$V$为结点数量）</li>
</ul>
</li>
<li>A* algorithm<ul>
<li>A<em> 算法类似于Dijkstra，只是Dijkstra会获得从出发点到所有目的地的最短路径生成树，A</em>只会找到出发点到目标点的最短路径</li>
</ul>
</li>
</ol>
<h4 id="A-算法"><a href="#A-算法" class="headerlink" title="A*算法"></a>A*算法</h4><blockquote>
<p><a href="https://wikimili.com/en/Peter_E._Hart">Peter Hart</a>, Nils Nilsson and <a href="https://wikimili.com/en/Bertram_Raphael">Bertram Raphael</a> of Stanford Research Institute (now <a href="https://wikimili.com/en/SRI_International">SRI International</a>) first published the algorithm in 1968. <a href="https://wikimili.com/en/A*_search_algorithm#cite_note-nilsson-4">[4]</a> It can be seen as an extension of <a href="https://wikimili.com/en/Dijkstra&#39;s_algorithm">Dijkstra’s algorithm</a>. A* achieves better performance by using <a href="https://wikimili.com/en/Heuristic_(computer_science">heuristics</a>) to guide its search.</p>
</blockquote>
<p>A*算法是对Dijkstra’s algorithm的扩展，A* 定义了特殊形式的函数$f(n)$作为结点优先级的衡量标准</p>
<script type="math/tex; mode=display">
f(n) = g(n) + h(n)</script><p>其中$g(n)$ 为起点到结点n的成本（已知），$h(n)$为结点n到终点成本的估计函数（对未知的估计）-<strong>启发函数</strong></p>
<p><strong>启发函数（<a href="https://wikimili.com/en/Heuristic">heuristic</a> function）</strong></p>
<p>其实仔细看看我们会发现，其实A*算法相较于Dijkstra只是增加了一个启发函数$h(n)$，为什么要添加启发函数？</p>
<ol>
<li>“以史为鉴” $g(n)$ ：从结点到当前节点的成本我是已知的，所以每次选成本最低的可能能找到最优解（Dijkstra）</li>
<li>“最速梯度下降” $h(n)$：如果我能够近似估计从当前结点到目标结点的距离，我肯定要选择估计上更近的点，更可能找到最优解（Greedy Search）</li>
</ol>
<p>所以启发函数的选取对于A*算法极为重要</p>
<ol>
<li>一方面决定剪枝效率</li>
<li>一方面决定能够A*能否找到最优解</li>
</ol>
<p>论文中证明了只要启发函数满足以下两个性质即能满足我们的要求</p>
<ol>
<li><strong>admissible</strong>：$h(x) \leq h^<em>(x)$ 其中 $h^</em>(x)$ 为到目标结点的真实距离，即<strong>到目标结点的估计距离非严格小于实际距离</strong></li>
<li><strong>consistent/monotone</strong>：若存在边$(x, y)$，则 $h(x) ≤ d(x, y) + h(y)$，即当前节点到目标结点的估计距离，小于当前结点到临接结点的距离+临接结点到目标结点的估计距离（可以理解成 <strong>两边之和大于第三边</strong>）</li>
<li>若启发函数满足admissible，则A<em>算法一定能够找到<strong>最优解</strong>；若启发函数满足consistent，则A\</em>算法不会向优先队列中重复添加点（即<strong>单调递增</strong>，后找到点的f(n) 一定小于前找到点），其中consistent性质蕴含admissible，若启发函数具有consistent，其一定满足admissible性质</li>
</ol>
<p>根据wiki百科中的解释简单理解一下两个性质为什么导致结果成立</p>
<ol>
<li>admissible $\rightarrow$ 能够找到最优解：反证法，由于最后一次目标结点进入队列时其 $f(n_{目标}) = g(n_{目标}) + (h(n_{目标}) = 0)$, 其函数值已不包括估计值，假设我们找到非最优解，则存在一条到目标点的路径长度小于A<em>迭代退出时选择的路径，取该路径上的任意一点 $n_{rand}$ ，其启发函数一定满足$f(n_{rand}) = g(n_{rand}) + (h(n_{rand}) &lt; 经过n_{rand}实际路径长 &lt; f(n_{目标}))$，所以优先队列不可能弹出目标结点，A\</em>此次迭代优先队列不可能弹出目标节点。</li>
<li>consistent $\rightarrow$ admissible：随便选一条从出发点到目标点的路径，根据 $h(x) ≤ d(x, y) + h(y)$ 不断的转换不等式: $h(x) ≤ d(x, y) + h(y) \leq d(x, y) + d(y, z) + h(z)…$ 直到不等式转换为$h(x) ≤ d(x, n_{目标})$，推导出admissible</li>
</ol>
<p>常用的启发函数有（别人的博客）</p>
<img src="/2022/06/08/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/image-20220606101813112.png" class="" title="image-20220606101813112">
<p>所以<strong>如何选择启发函数</strong>？</p>
<ol>
<li>首先要满足admissible的性质，保证算法能够找到最优解</li>
<li>其次<strong>启发函数的估计值尽量大</strong>，</li>
<li>极端情况（即$h(x) = h^<em>(x)$）下，A</em>算法转化为 <strong>保证能够找到解的贪心搜索</strong>；若 $h(x) = 0$，A<em>算法转化为 <em>*Dijkstra</em></em>算法</li>
</ol>
<p><strong>复杂度</strong></p>
<ol>
<li>时间复杂度：看不明白证明</li>
<li>空间复杂度：看不明白证明</li>
</ol>
<p><strong>伪代码模板</strong></p>
<p>在Dijkstra改改就是A*（<del>不会写伪代码，网上抄一个</del>）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">* 初始化open_set和close_set；</span><br><span class="line">* 将起点加入open_set中，并设置优先级为<span class="number">0</span>（优先级最高）；</span><br><span class="line">* 如果open_set不为空，则从open_set中选取优先级最高的节点n：</span><br><span class="line">    * 如果节点n为终点，则：</span><br><span class="line">        * 从终点开始逐步追踪parent节点，一直达到起点；</span><br><span class="line">        * 返回找到的结果路径，算法结束；</span><br><span class="line">    * 如果节点n不是终点，则：</span><br><span class="line">        * 将节点n从open_set中删除，并加入close_set中；</span><br><span class="line">        * 遍历节点n所有的邻近节点：</span><br><span class="line">            * 如果邻近节点m在close_set中，则：</span><br><span class="line">                * 跳过，选取下一个邻近节点</span><br><span class="line">            * 如果邻近节点m也不在open_set中，则：</span><br><span class="line">                * 设置节点m的parent为节点n</span><br><span class="line">                * 计算节点m的优先级</span><br><span class="line">                * 将节点m加入open_set中</span><br></pre></td></tr></table></figure>
<h4 id="典型例题"><a href="#典型例题" class="headerlink" title="典型例题"></a>典型例题</h4><ol>
<li><a href="https://www.luogu.com.cn/problem/P1379">P1379 八数码难题</a><ul>
<li>比较了几个不同的启发函数计算方式，确实是启发函数越大，剪枝效果越好</li>
</ul>
</li>
</ol>
<h3 id="迭代加深"><a href="#迭代加深" class="headerlink" title="迭代加深"></a>迭代加深</h3><p>通过限制DFS搜索的深度实现BFS效果，本质上还是DFS搜索，只是每次DFS增加了最大递归深度。</p>
<ul>
<li>当DFS达到最大深度时，即使未找到目标解，也要返回。</li>
<li>若某次深度d的DFS没有找到解，则将迭代深度+1，重新进行DFS搜索。</li>
</ul>
<p>伪代码如下:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> maxDepth = <span class="number">1</span>;maxDepth &lt;= 最大迭代深度；maxDepth++)&#123;</span><br><span class="line">	dfs(<span class="number">0</span>, maxDepth)</span><br><span class="line">&#125;</span><br><span class="line">dfs(depth, maxDepth)&#123;</span><br><span class="line">	<span class="keyword">if</span>(depth == maxDepth)&#123;</span><br><span class="line">		<span class="keyword">return</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	dfs(depth + <span class="number">1</span>, maxDepth)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>迭代加深与BFS的区别在于</p>
<ol>
<li>迭代加深方法通过DFS实现BFS效果，避免了BFS队列对于状态的存储，降低了空间占用</li>
<li>每次增加迭代深度，会导致DFS重复遍历</li>
</ol>
<p>什么时机使用迭代加深？（玄学）</p>
<ul>
<li>当搜索树的分支比较多时，每增加一层的搜索复杂度会出现指数级爆炸式增长，这时前面重复进行的部分所带来的复杂度几乎可以忽略</li>
<li><p>为了避免过量的空间占用，采用迭代加深</p>
</li>
<li><p>题目特征：<strong>限定了搜索深度</strong>，要求找到解没有限定解的类型（<del>遇到这样的题，就嫁了吧</del>）</p>
</li>
</ul>
<h4 id="典型例题-1"><a href="#典型例题-1" class="headerlink" title="典型例题"></a>典型例题</h4><p>单纯的迭代加深方法应用的不多，与A<em>算法结合使用IDA\</em>的比较多</p>
<ol>
<li><a href="https://www.acwing.com/problem/content/172/">170. 加成序列</a><ul>
<li>BFS的<strong>最坏空间复杂度为O(N!)</strong>,当N=100时，对应复杂度能达到 $10^{158}$</li>
<li>迭代加深重复遍历相较于状态个数已经可以忽略不计</li>
</ul>
</li>
</ol>
<h3 id="IDA"><a href="#IDA" class="headerlink" title="IDA*"></a>IDA*</h3><p>迭代加深和A*结合，使用启发函数在dfs过程中减枝，从而实现算法时间复杂度的降低，伪代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> maxDepth = <span class="number">1</span>;maxDepth &lt;= 最大迭代深度；maxDepth++)&#123;</span><br><span class="line">	dfs(<span class="number">0</span>, maxDepth)</span><br><span class="line">&#125;</span><br><span class="line">dfs(depth, maxDepth)&#123;</span><br><span class="line">	<span class="keyword">if</span>(depth == maxDepth)&#123;</span><br><span class="line">		<span class="keyword">return</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//只有在启发函数+当前代价小于最大代价时才继续进行迭代</span></span><br><span class="line">	<span class="keyword">if</span>(depth + h(curStatus) &lt;= maxDepth)</span><br><span class="line">		dfs(depth + <span class="number">1</span>, maxDepth)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>观察代码其实和迭代加深算法区别不大，只是增加了减枝函数，最难的点也就在如何决定这个剪枝函数-<strong>启发函数的选取</strong>，</p>
<h4 id="典型例题-2"><a href="#典型例题-2" class="headerlink" title="典型例题"></a>典型例题</h4><ol>
<li><p><a href="P2324 [SCOI2005]骑士精神">P2324 [SCOI2005]骑士精神</a>， 简单分析一下为什么要用IDA*</p>
<ul>
<li><p>类似于八数码难题，从空位置出发，每次寻找能够移动到</p>
</li>
<li><p>空位置的棋子，该问题每次最多可能有8个位置能够移动到空位置</p>
</li>
<li><p>bfs和迭代加深算法的最坏复杂度为<strong>O(8^15)</strong>（限制了递归深度最大为15），显然超时，且BFS还会出现MLE问题，所以只能尝试IDA*</p>
<img src="/2022/06/08/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/image-20220607170129957.png" class="" title="image-20220607170129957">
</li>
</ul>
</li>
<li><p><a href="https://www.luogu.com.cn/problem/P2534">P2534 [AHOI2012]铁盘整理</a></p>
<ul>
<li>IDA*的模板题目，难点是想出来启发函数，其他倒不难</li>
</ul>
</li>
</ol>
<h3 id="Dancing-Links"><a href="#Dancing-Links" class="headerlink" title="Dancing Links"></a>Dancing Links</h3><blockquote>
<p><a href="https://oi-wiki.org/search/dlx/">OIWIKI:Dancing Links</a></p>
</blockquote>
<p>舞蹈链是为了解决精确覆盖的一种特殊数据结构，精确覆盖问题目前只能通过暴力回溯法的方式实现求解，Dancing links通过链表存储的方式实现了时间和空间复杂度一定程度的降低（难度太大找工作用不到，了解一下，不深入学习，<del>贴张别人的图表示字自己学习过了</del>）</p>
<img src="/2022/06/08/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/image-20220608094659278.png" class="" title="image-20220608094659278">
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>算是把搜索问题的主要解题思路整理学习完毕，搜索问题的难点可以总结为：</p>
<ol>
<li>剪枝：无论是普通的DFS还是A<em>，IDA\</em>算法，实现难度其实都不是很大，最难的点还是在于<strong>找到合适的剪枝函数</strong></li>
<li>时间空间复杂度分析：很多时候问题的方法显而易见，难点反而在选用什么搜索方法才能适应题目的时间空间复杂度要求</li>
<li>问题抽象：如何讲问题转化为搜索问题是比较难的点，很多问题往往即使告诉你要用搜索（比如铁盘整理和骑士精神），我也会想不明白</li>
</ol>
<p>其他没什么可总结的了，其实目前很多算法只能说懂了怎么写，真正的解决相关搜索问题的能力还欠缺，继续努力！！！</p>
<h3 id="相关参考"><a href="#相关参考" class="headerlink" title="相关参考"></a>相关参考</h3><ol>
<li><a href="https://oi-wiki.org/search/">OI WiKi的搜索专题</a></li>
<li><a href="https://www.luogu.com.cn/training/9376">luogu官方搜索题单</a></li>
<li><a href="https://en.wikipedia.org/wiki/A*_search_algorithm">wikipeida A*_search_algorithm</a></li>
<li><a href="https://www.cnblogs.com/grenet/p/3145800.html">博客：跳跃的舞者，舞蹈链（Dancing Links）算法——求解精确覆盖问题</a></li>
<li><a href="https://www.acwing.com/problem/">部分题目：acWing</a></li>
</ol>
]]></content>
      <categories>
        <category>算法相关</category>
        <category>算法整理</category>
      </categories>
      <tags>
        <tag>搜索</tag>
      </tags>
  </entry>
  <entry>
    <title>RabbitMQ入门案例</title>
    <url>/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/</url>
    <content><![CDATA[<p>RabbitMQ官网教程<a href="https://www.rabbitmq.com/getstarted.html">RabbitMQ Tutorials</a>中提供了六个入门案例，下面按照案例顺序逐个进行实践操作，了解RabbitMQ的基本应用</p>
<ol>
<li>“Hello World”：简单一对一队列</li>
<li>“Work Queues”：一对多队列，模拟任务分配</li>
<li>“Publish/Subscrible”：Fanout类型交换机，广播消息</li>
<li>“Routing”：direct类型交换机，根据RoutingKey发送到对应队列</li>
<li>“Topics”：topic类型交换机，根据RoutingKey的匹配规则发送到对应队列</li>
<li>“RPC”：使用消息队列模拟RPC调用</li>
<li>“Publish Confirms”：通过发送确认，实现发送端可靠性保证</li>
</ol>
<p>为了基于Java操作RabbitMQ，需要加入对应的Client依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/com.rabbitmq/amqp-client --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.rabbitmq<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>amqp-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h3 id="Hello-World案例"><a href="#Hello-World案例" class="headerlink" title="Hello World案例"></a>Hello World案例</h3><p>该案例为基本的生产者消费者场景，一个生产者P向MQ中写入消息，一个消费者C从MQ中读取消息</p>
<p><img src="https://www.rabbitmq.com/img/tutorials/python-one.png" alt="(P) -&gt; [|||] -&gt; (C)"></p>
<p>生产者代码如下:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sender</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> String QUEUE_NAME = <span class="string">&quot;hello&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> String MQ_HOST = <span class="string">&quot;192.168.190.100&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span><span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//初始化连接并创建队列</span></span><br><span class="line">        ConnectionFactory factory = <span class="keyword">new</span> ConnectionFactory();</span><br><span class="line">        factory.setHost(MQ_HOST);</span><br><span class="line">        factory.setUsername(<span class="string">&quot;test_user&quot;</span>);</span><br><span class="line">        factory.setPassword(<span class="string">&quot;123&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span>(Connection connection = factory.newConnection();</span><br><span class="line">            Channel channel = connection.createChannel())&#123;</span><br><span class="line">            <span class="comment">//创建队列</span></span><br><span class="line">            channel.queueDeclare(QUEUE_NAME,<span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">null</span>);</span><br><span class="line">            <span class="comment">//向默认交换器中发送消息，其中路由键等于队列名（因为默认交换器为直连型交换器）</span></span><br><span class="line">            channel.basicPublish(<span class="string">&quot;&quot;</span>,QUEUE_NAME, <span class="keyword">null</span>, <span class="string">&quot;hello world&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">            System.out.println(<span class="string">&quot;sender send message to &quot;</span> + QUEUE_NAME);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>消费者代码如下：</p>
<ul>
<li>消费者同样需要声明队列，为了避免消费者运行先于生产者</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Receiver</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> String QUEUE_NAME = <span class="string">&quot;hello&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> String MQ_HOST = <span class="string">&quot;192.168.190.100&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span><span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//初始化代码和生产者相同</span></span><br><span class="line">        <span class="comment">//省略。。。。。。</span></span><br><span class="line">        <span class="comment">//等待消费到队列内容</span></span><br><span class="line">        System.out.println(<span class="string">&quot;Receiver is waiting for message&quot;</span>);</span><br><span class="line">        <span class="comment">//定义回调函数</span></span><br><span class="line">        DeliverCallback deliverCallback = <span class="keyword">new</span> DeliverCallback() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(String consumerTag, Delivery message)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">                System.out.println(<span class="keyword">new</span> String(message.getBody()));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="comment">//开始消费（队列名，自动ACK，接收消息回调，拒绝消息回调函数）</span></span><br><span class="line">        channel.basicConsume(QUEUE_NAME, <span class="keyword">true</span>, deliverCallback, consumerTag -&gt; &#123; &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行后出现结果：</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/image-20230110095018739.png" class="" title="image-20230110095018739">
<p>遇到的问题：新创建用户不具备visualhost”/“的权限，导致无法向队列写入消息</p>
<ul>
<li><p>错误提示信息如下</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/image-20230110095214850.png" class="" title="image-20230110095214850">
</li>
<li><p>在管理页面修改用户权限即可</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/image-20230110095256385.png" class="" title="image-20230110095256385">
</li>
</ul>
<h3 id="Work-Queues案例"><a href="#Work-Queues案例" class="headerlink" title="Work Queues案例"></a>Work Queues案例</h3><p>此案例中消息的生产者相当于任务分配者，消费者相当于任务的执行者，分配者将任务发送到队列后不需要等待任务返回，执行者执行完毕后返回执行结果，相当于借助消息队列将同步操作变为异步操作，在处理耗费资源任务的场景中能够降低任务分配者阻塞的成本。</p>
<p><img src="https://www.rabbitmq.com/img/tutorials/python-two.png" alt="Producer -&gt; Queue -&gt; Consuming: Work Queue used to distribute time-consuming tasks among multiple workers."></p>
<p>生产者发送20条消息到队列中，模拟发布20个任务：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//向队列中不间断的发送20条消息</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i &lt;= <span class="number">20</span>;i ++ )&#123;</span><br><span class="line">    <span class="keyword">int</span> workTime = <span class="keyword">new</span> Random().nextInt(<span class="number">4</span>) + <span class="number">1</span>;</span><br><span class="line">    channel.basicPublish(<span class="string">&quot;&quot;</span>,QUEUE_NAME, <span class="keyword">null</span>, String.format(<span class="string">&quot;%d %d&quot;</span>, i, workTime).getBytes(StandardCharsets.UTF_8));</span><br><span class="line">    System.out.println(<span class="string">&quot;sender send message:&quot;</span>+ i+ <span class="string">&quot; to&quot;</span> + QUEUE_NAME);</span><br><span class="line">    Thread.sleep(<span class="number">200</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>消费者采用多线程方式实现，两个线程消费消息并休眠，模拟执行任务</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;worker:&quot;</span> + workerNum + <span class="string">&quot; ready to work&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//创建channel，并初始化队列</span></span><br><span class="line">        Channel channel = connection.createChannel();</span><br><span class="line">        channel.queueDeclare(QUEUE_NAME,<span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">null</span>);</span><br><span class="line">        DeliverCallback deliverCallback = <span class="keyword">new</span> DeliverCallback() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(String consumerTag, Delivery message)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">                String[] work = <span class="keyword">new</span> String(message.getBody()).split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">                System.out.println(String.format(<span class="string">&quot;[receive]worker:%d, get work %s&quot;</span>, workerNum, work[<span class="number">0</span>],work[<span class="number">1</span>]));</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    Thread.sleep(Integer.parseInt(work[<span class="number">1</span>]) * <span class="number">1000L</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        channel.basicConsume(QUEUE_NAME, <span class="keyword">true</span>, deliverCallback, consumerTag -&gt; &#123; &#125;);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到RabbitMQ采取<code>Round robin</code>的方式，两个线程轮流消费消息</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/image-20230110103701343.png" class="" title="image-20230110103701343">
<h4 id="消息的ACK问题"><a href="#消息的ACK问题" class="headerlink" title="消息的ACK问题"></a>消息的ACK问题</h4><blockquote>
<p>If a consumer dies (its channel is closed, connection is closed, or TCP connection is lost) without sending an ack, RabbitMQ will understand that a message wasn’t processed fully and will re-queue it. If there are other consumers online at the same time, it will then quickly redeliver it to another consumer.</p>
</blockquote>
<p>为了保证消息正确的被消费，RabbitMQ只有在收到消费者的ACK确认后才会将消息从队列中删除，若由于特殊原因导致未收到ACK</p>
<ol>
<li>消费者挂掉</li>
<li>超时未确认（默认30分钟）</li>
</ol>
<p>RabbitMQ会将消息重新加入到队列头，交给另外的消费者进行消费</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//关闭自动确认</span></span><br><span class="line">channel.basicConsume(QUEUE_NAME, <span class="keyword">false</span>, deliverCallback, consumerTag -&gt; &#123; &#125;);</span><br><span class="line"><span class="comment">//在回调函数中开启手动确认</span></span><br><span class="line">channel.basicAck(message.getEnvelope().getDeliveryTag(),<span class="keyword">false</span>);</span><br></pre></td></tr></table></figure>
<p>修改代码后，使用<code>rabbitmqctl</code>查看未确认消息</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/image-20230110105414275.png" class="" title="image-20230110105414275">
<h4 id="公平分配的问题"><a href="#公平分配的问题" class="headerlink" title="公平分配的问题"></a>公平分配的问题</h4><p>Rabbit对于同一个队列的多个消费者消息分配采取<code>round robin</code>的方式，虽然能够保证消息的均匀分配，但是如果消息对应任务的工作量不同，此时不同消费者的工作量就会不同，导致负载均衡失效。</p>
<p>官方教程中提供了一种方式，通过设置<code>prefetchCount = 1</code>，只有在消费者ack历史消息后，才会将新的消息分配给他，此时消息分配机制类似于先到先得,对应代码如下：</p>
<ul>
<li><code>prefetchCount</code>：指当消费者未确认的消息等于prefetchCount时，不再向消费者发送消息</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> prefetchCount = <span class="number">1</span>;</span><br><span class="line">channel.basicQos(prefetchCount);</span><br></pre></td></tr></table></figure>
<p>修改代码，可以验证上述结果:</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/image-20230110110402703.png" class="" title="image-20230110110402703">
<h3 id="Publish-Subscribe案例"><a href="#Publish-Subscribe案例" class="headerlink" title="Publish/Subscribe案例"></a>Publish/Subscribe案例</h3><p>引入了exchange（交换机）的概念，消费者并不需要了解消息发送到哪个队列，只需要发送到指定的交换机，交换机根据其类型和队列路由key确定分发对象，exchange类型分为：</p>
<ol>
<li>直连（direct）：拿到消息后根据路由键是否等于对应队列的绑定键，直接将消息路由到对应队列。</li>
<li>扇形（Fanout）：将消息发送到当前交换机绑定的所有队列（广播路由）。</li>
<li>主题（topic）：通过将消息路由键与队列绑定模式匹配，将消息转发到一个或者多个匹配成功的队列（多播路由）。</li>
<li>头（head）：将路由键从字符串变为了多值属性，多值匹配实现路由。</li>
</ol>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/image-20230110160917903.png" class="" title="image-20230110160917903">
<p>引入交换机概念后的消费者代码的逻辑为：</p>
<ol>
<li><p>若不存在，则声明交换机</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">channel.exchangeDeclare(<span class="string">&quot;exchange_name&quot;</span>, <span class="string">&quot;exchange_type&quot;</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>向交换机上生产消息</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">channel.basicPublish(<span class="string">&quot;exchange_name&quot;</span>,<span class="string">&quot;route_key&quot;</span>, 基本属性, message);</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>队列创建的逻辑：</p>
<ol>
<li><p>声明队列</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">channel.queueDeclare(queue, durable, exclusive, autoDelete,arguments);</span><br></pre></td></tr></table></figure>
</li>
<li><p>绑定交换机</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">channel.queueBind(queue, exchange, routingKe);</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>以<code>fanout</code>即广播类型exchange为例，展示发布订阅案例</p>
<ol>
<li><p>发布代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmitLog</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, TimeoutException </span>&#123;</span><br><span class="line">        Channel channel = RabbitMQUtils.getChannel();</span><br><span class="line">        channel.exchangeDeclare(<span class="string">&quot;log_exchange&quot;</span>, <span class="string">&quot;fanout&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        String curString = scanner.nextLine();</span><br><span class="line">        <span class="keyword">while</span>(!curString.startsWith(<span class="string">&quot;end&quot;</span>))&#123;</span><br><span class="line">            <span class="comment">//写入日志</span></span><br><span class="line">            channel.basicPublish(<span class="string">&quot;log_exchange&quot;</span>,<span class="string">&quot;&quot;</span>, <span class="keyword">null</span>, curString.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">            curString = scanner.nextLine();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">&quot;exit&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>订阅代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReceiveLog</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Receiver</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> receiverNum;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">Receiver</span><span class="params">(<span class="keyword">int</span> receiverNum)</span></span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.receiverNum = receiverNum;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Channel channel = RabbitMQUtils.getChannel();</span><br><span class="line"></span><br><span class="line">                <span class="comment">//生成一个临时队列</span></span><br><span class="line">                String queName = channel.queueDeclare().getQueue();</span><br><span class="line"></span><br><span class="line">                channel.queueBind(queName, <span class="string">&quot;log_exchange&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line"></span><br><span class="line">                DeliverCallback deliverCallback = <span class="keyword">new</span> DeliverCallback() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(String consumerTag, Delivery message)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">                        System.out.printf(<span class="string">&quot;receiver:%d receive log:%s\n&quot;</span>,receiverNum, <span class="keyword">new</span> String(message.getBody()));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;;</span><br><span class="line">                channel.basicConsume(queName, deliverCallback, consumerTag -&gt; &#123;&#125;);</span><br><span class="line"></span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException | TimeoutException e) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i &lt; <span class="number">3</span>;i++)&#123;</span><br><span class="line">            <span class="keyword">new</span> Receiver(i).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>结果为每发布一条消息，两个接收者均能够消费到消息</p>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/image-20230110161734746.png" class="" title="image-20230110161734746">
</li>
</ol>
<h3 id="Routing"><a href="#Routing" class="headerlink" title="Routing"></a>Routing</h3><p>前面的代码其实已经覆盖了该部分示例，在创建que和exchange的绑定关系时指定routing key，当生产者发布消息时exchange(direct 类型)根据routing key发布到对应的队列中:</p>
<ol>
<li><p>一个队列可以绑定多个routing key</p>
<p><img src="https://www.rabbitmq.com/img/tutorials/direct-exchange.png" alt="Direct exchange routing"></p>
</li>
<li><p>多个队列可以绑定相同的routing key</p>
<p><img src="https://www.rabbitmq.com/img/tutorials/direct-exchange-multiple.png" alt="Multiple Bindings"></p>
</li>
<li><p>综合示例</p>
<p><img src="https://www.rabbitmq.com/img/tutorials/python-four.png" alt="Final routing: putting it all together."></p>
</li>
</ol>
<h3 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h3><p>Topic类型交换机基于RoutingKey的匹配实现路由，其中routingkey闭幕满足<code>word.word.word</code>的格式，每个词语之间通过<code>.</code>分隔，当对应位置词语匹配时，将消息路由到对应队列，包括两个特殊类型的字符：</p>
<ol>
<li><code>*</code>：匹配一个词语</li>
<li><code>#</code>：匹配多个词语</li>
</ol>
<p><img src="https://www.rabbitmq.com/img/tutorials/python-five.png" alt="Topic Exchange illustration, which is all explained in the following text."></p>
<p>下面举几个匹配例子：</p>
<ol>
<li><p><code>*.*.rabbit</code>：可以匹配任意第三个词为<code>rabbit</code>的路由键，例如<code>quick.orange.rabbit</code></p>
</li>
<li><p><code>lazy.#</code>：可以匹配任意第一个词为<code>lazy</code>的路由键，例如</p>
<p><code>lazy.orange.rabbit</code>,<code>lazy</code>,<code>lazy.nb</code></p>
</li>
</ol>
<p>了解之前以为按照正则表达式的方式匹配，实际上只是简单的点分字符匹配。</p>
<h3 id="RPC"><a href="#RPC" class="headerlink" title="RPC"></a>RPC</h3><p>基于RabbitMQ的队列操作可以模拟实现RPC接口，基本实现逻辑如下图所示：</p>
<ol>
<li>RPC客户端将RPC请求参数写入队列，并监听返回队列</li>
<li>RPC服务端将监听请求参数队列，将结果写入返回队列</li>
</ol>
<p><img src="https://www.rabbitmq.com/img/tutorials/python-six.png" alt="Summary illustration, which is described in the following bullet points."></p>
<p>要实现RPC需要解决以下问题：</p>
<ol>
<li><p>RPC Server如何指导写入哪个返回队列？</p>
<ul>
<li><p>固定的返回队列</p>
</li>
<li><p>在参数消息中携带返回队列名（replyTo属性）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">replyTo = returnQue</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>返回队列是临时的还是固定的</p>
<ul>
<li><p>固定的方便传输，不用重复创建-&gt;区分不同请求的响应结果-&gt;使用correlationId属性解决</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">correlationId = UUID.randomUUID()</span><br></pre></td></tr></table></figure>
</li>
<li><p>每次创建一个临时队列</p>
</li>
</ul>
</li>
</ol>
<p>得到如下的案例代码</p>
<ol>
<li><p>客户端</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">byte</span>[] rpcCall(String callQue, String returnQue, <span class="keyword">byte</span>[] parameter) <span class="keyword">throws</span> IOException, ExecutionException, InterruptedException &#123;</span><br><span class="line">    <span class="keyword">if</span>(returnQue == <span class="keyword">null</span>)&#123;</span><br><span class="line">        returnQue = c.queueDeclare().getQueue();</span><br><span class="line">    &#125;</span><br><span class="line">    String opID = String.valueOf(UUID.randomUUID());</span><br><span class="line">    <span class="comment">//构建属性</span></span><br><span class="line">    BasicProperties properties = <span class="keyword">new</span> BasicProperties.Builder().correlationId(opID).replyTo(returnQue)</span><br><span class="line">        .build();</span><br><span class="line">    c.queueDeclare(callQue, <span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">null</span>);</span><br><span class="line">    <span class="comment">//写入参数到请求队列</span></span><br><span class="line">    c.basicPublish(<span class="string">&quot;&quot;</span>, callQue, properties, parameter);</span><br><span class="line">    <span class="keyword">final</span> CompletableFuture&lt;<span class="keyword">byte</span>[]&gt; response = <span class="keyword">new</span> CompletableFuture&lt;&gt;();</span><br><span class="line">    DeliverCallback deliverCallback = <span class="keyword">new</span> DeliverCallback() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(String consumerTag, Delivery message)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(opID.equals(message.getProperties().getCorrelationId()))&#123;</span><br><span class="line">                response.complete(message.getBody());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="comment">//监听返回队列</span></span><br><span class="line">    String ct = c.basicConsume(returnQue, <span class="keyword">true</span>, deliverCallback, consumerTag -&gt; &#123;&#125;);</span><br><span class="line">    <span class="keyword">byte</span>[] result = response.get();</span><br><span class="line">    c.basicCancel(ct);</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>服务端</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		Channel c = RabbitMQUtils.getChannel();</span><br><span class="line">		<span class="comment">//能者多劳</span></span><br><span class="line">		c.basicQos(<span class="number">1</span>);</span><br><span class="line">		DeliverCallback d = <span class="keyword">new</span> DeliverCallback() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(String consumerTag, Delivery message)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">                <span class="comment">//返回结果（不能忘记添加操作编号）</span></span><br><span class="line">                c.basicPublish(<span class="string">&quot;&quot;</span>, message.getProperties().getReplyTo(), <span class="keyword">new</span> AMQP.BasicProperties.Builder().correlationId(message.getProperties().getCorrelationId()).build(),</span><br><span class="line">                               String.valueOf(fbi(Integer.parseInt(<span class="keyword">new</span> String(message.getBody())))).getBytes(StandardCharsets.UTF_8));</span><br><span class="line">                c.basicAck(message.getEnvelope().getDeliveryTag(), <span class="keyword">false</span>);</span><br><span class="line">                System.out.printf(<span class="string">&quot;server:%d finish task %s\n&quot;</span>,serverNum, <span class="keyword">new</span> String(message.getBody()));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="comment">//监听请求队列</span></span><br><span class="line">        c.basicConsume(RPC_QUE, <span class="keyword">false</span>, d, consumerTag -&gt; &#123;&#125;);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException | TimeoutException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>问题：<strong>公用一个队列，如果一个客户端消费了其他的返回消息，如何解决这个问题？</strong></p>
<h3 id="Publish-Confirms"><a href="#Publish-Confirms" class="headerlink" title="Publish Confirms"></a>Publish Confirms</h3><p>发布确认为了<strong>保证生产者发送的消息到达了broker</strong>，没有因为网络或者拥塞等原因丢弃，通过发布确认、队列持久化、消息持久化三个设置能够实现消息的可靠发布。</p>
<ul>
<li>发布确认只是用来保证消息到达交换机，至于是否发送到队列，消息是否被正常消费并不在确认范围。</li>
<li>在发送消息时设置<code>mandatory=true</code>，当交换机找不到合适的队列投递消息，会将消息返回给生产者。</li>
</ul>
<p>在发送消息前，通过<code>confirmSelect</code>开启信道上的发布确认功能：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Channel channel = connection.createChannel();</span><br><span class="line">channel.confirmSelect();</span><br></pre></td></tr></table></figure>
<p>根据生产者确认是否同步，分为三种发布确认方式：</p>
<ol>
<li>同步确认</li>
<li>批量同步确认</li>
<li>异步确认</li>
</ol>
<p>这里的同步指的是生产者在等待确认时是否阻塞，并不是I/O中所谓的同步，更偏向于“阻塞”的概念</p>
<h4 id="同步确认"><a href="#同步确认" class="headerlink" title="同步确认"></a>同步确认</h4><p>生产者在发送消息后，等待消息确认后再进行下一步操作，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(!inputStr.equals(<span class="string">&quot;end&quot;</span>))&#123;</span><br><span class="line">    c.basicPublish(<span class="string">&quot;&quot;</span>, <span class="string">&quot;pc_test&quot;</span>, <span class="keyword">null</span>, inputStr.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">    c.waitForConfirmsOrDie(<span class="number">5000</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中相关的确认函数包括</p>
<ul>
<li><code>OrDie</code>：如果任意一条发送的消息返回为<code>nack</code>，直接抛出<code>IOException</code></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">waitForConfirms</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">waitForConfirms</span><span class="params">(<span class="keyword">long</span> timeout)</span> <span class="keyword">throws</span> InterruptedException, TimeoutException</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">waitForConfirmsOrDie</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">waitForConfirmsOrDie</span><span class="params">(<span class="keyword">long</span> timeout)</span> <span class="keyword">throws</span> IOException, InterruptedException, TimeoutException</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="批量确认"><a href="#批量确认" class="headerlink" title="批量确认"></a>批量确认</h4><p>生产者发送一定量消息后，再进行整体确认，代码如下：</p>
<ul>
<li>确认函数包括前面所有消息，当一条消息返回为<code>nack</code>，直接抛出<code>IOException</code></li>
<li>问题是：无法知道哪条消息发送失败，一旦失败需要重发批中所有消息</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> bachCount = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">int</span> curCount = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span>(!inputStr.equals(<span class="string">&quot;end&quot;</span>))&#123;</span><br><span class="line">    c.basicPublish(<span class="string">&quot;&quot;</span>, <span class="string">&quot;pc_test&quot;</span>, <span class="keyword">null</span>, inputStr.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">    <span class="keyword">if</span>(++curCount == bachCount)&#123;</span><br><span class="line">        c.waitForConfirmsOrDie(<span class="number">5000</span>);</span><br><span class="line">        curCount = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    inputStr = scanner.next();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span>(curCount &gt; <span class="number">0</span>)&#123;</span><br><span class="line">    c.waitForConfirmsOrDie(<span class="number">5000</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="异步确认"><a href="#异步确认" class="headerlink" title="异步确认"></a>异步确认</h4><p>基于回调函数进行消息确认,代码如下:</p>
<ul>
<li>基于消息的<code>sequence number</code>对每一条发布消息进行确认和处理</li>
<li>使用<code>NavigableMap</code>存储未确认的<code>&lt;sequence number, message&gt;</code>，采用排序map的原因是为了解决累计确认（类似于TCP协议）的问题</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ConcurrentNavigableMap&lt;Long, <span class="keyword">byte</span>[]&gt; confirmMap = <span class="keyword">new</span> ConcurrentSkipListMap&lt;&gt;();</span><br><span class="line"><span class="comment">//关键回调代码</span></span><br><span class="line">ConfirmCallback confirmCallback = <span class="keyword">new</span> ConfirmCallback() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(<span class="keyword">long</span> deliveryTag, <span class="keyword">boolean</span> multiple)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">//multi表示大于等于当前序列号的均已确认</span></span><br><span class="line">        <span class="keyword">if</span>(multiple)&#123;</span><br><span class="line">            ConcurrentNavigableMap&lt;Long, <span class="keyword">byte</span>[]&gt;  confirmed = confirmMap.headMap(deliveryTag, <span class="keyword">true</span>);</span><br><span class="line">            confirmed.clear();</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            confirmMap.remove(deliveryTag);</span><br><span class="line">            System.out.println(<span class="string">&quot;confirm message:&quot;</span> + deliveryTag);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">//两个回调函数：成功确认函数，失败确认函数</span></span><br><span class="line">c.addConfirmListener(confirmCallback, <span class="keyword">new</span> ConfirmCallback() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(<span class="keyword">long</span> deliveryTag, <span class="keyword">boolean</span> multiple)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">byte</span>[] body = confirmMap.get(deliveryTag);</span><br><span class="line">        System.err.format(</span><br><span class="line">            <span class="string">&quot;Message with body %s has been nack-ed. Sequence number: %d, multiple: %b%n&quot;</span>,</span><br><span class="line">            <span class="keyword">new</span> String(body), deliveryTag, multiple</span><br><span class="line">        );</span><br><span class="line">        confirmCallback.handle(deliveryTag, multiple);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="keyword">while</span>(!inputStr.equals(<span class="string">&quot;end&quot;</span>))&#123;</span><br><span class="line">    <span class="comment">//记录未确认消息</span></span><br><span class="line">    confirmMap.put(c.getNextPublishSeqNo(), inputStr.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">    c.basicPublish(<span class="string">&quot;&quot;</span>, <span class="string">&quot;pc_test&quot;</span>, <span class="keyword">null</span>, inputStr.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">    inputStr = scanner.next();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<ul>
<li>可见存在累计确认的情况</li>
<li>序列号顺序递增，可以使用循环缓冲区实现缓冲确认功能。</li>
</ul>
<img src="/2023/02/12/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RabbitMQ/RabbitMQ%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B/image-20230112111033979.png" class="" title="image-20230112111033979">
<h4 id="SpringBoot中使用发布确认"><a href="#SpringBoot中使用发布确认" class="headerlink" title="SpringBoot中使用发布确认"></a>SpringBoot中使用发布确认</h4><p>需要自定义回调函数<code>ConfirmCallback</code>，并注入到RedisTemplate的对应接口方法中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PCCallback</span> <span class="keyword">implements</span> <span class="title">RabbitTemplate</span>.<span class="title">ConfirmCallback</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    RabbitTemplate rabbitTemplate;</span><br><span class="line">    <span class="meta">@PostConstruct</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span></span>&#123;</span><br><span class="line">        rabbitTemplate.setConfirmCallback(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">confirm</span><span class="params">(CorrelationData correlationData, <span class="keyword">boolean</span> b, String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(b)&#123;</span><br><span class="line">            log.info(<span class="string">&quot;exchange receive message &#123;&#125; success&quot;</span>,correlationData.getId());</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            log.info(<span class="string">&quot;exchange receive message &#123;&#125; failed for &#123;&#125;&quot;</span>,correlationData.getId(), s);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同时在配置文件中开启发布确认:<code>none</code>，<code>simple</code>，<code>correlated</code>（后两者区别暂时不知道）</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">spring.rabbitmq.publisher-confirm-type</span>=<span class="string">correlated</span></span><br></pre></td></tr></table></figure>
<p>另外还可通过<code>ReturnsCallback</code>回调函数在消息投递失败后，由Exchange将消息返回给生产者，需要在配置文件开启</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">spring.rabbitmq.template.mandatory</span>=<span class="string">true</span></span><br></pre></td></tr></table></figure>
<h4 id="Consumer-Confirms"><a href="#Consumer-Confirms" class="headerlink" title="Consumer Confirms"></a>Consumer Confirms</h4><p>RabbitMQ在消费端同样包括消息确认机制，分为三种确认模式：</p>
<ol>
<li><code>basic.ack</code>：确认消息正常消费。</li>
<li><code>basic.nack</code>：消息不正常消费，可指定消息重新入队/丢弃/进入死信队列。(<a href="https://www.rabbitmq.com/confirms.html">negatively acknowledged</a>)</li>
<li><code>basic.reject</code>：nack的多条版本，拒绝小于等于指定<code>delivertag</code>的所有消息。</li>
</ol>
]]></content>
      <categories>
        <category>技术框架</category>
        <category>RabbitMQ</category>
      </categories>
  </entry>
  <entry>
    <title>MySQL事务</title>
    <url>/2022/11/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/MySQL%E4%BA%8B%E5%8A%A1/</url>
    <content><![CDATA[<h2 id="MySQL事务"><a href="#MySQL事务" class="headerlink" title="MySQL事务"></a>MySQL事务</h2><p>之前在分布式系统的学习中已经系统的整理了事务ACID以及隔离性概念，下面总结在MySQL中的事务，偏向于实现层面，主要内容来自<em>《MySQL技术内幕:InnoDB存储引擎(第2版)》</em>。</p>
<h3 id="事务分类和使用"><a href="#事务分类和使用" class="headerlink" title="事务分类和使用"></a>事务分类和使用</h3><p>MySQL 默认提交事务（<code>autocommit = 1</code>），即执行完SQL一句后立马执行COMMIT操作，也就是所谓的<strong>隐性事务</strong>。显式事务的开启，需要手动输入事务控制语句，：</p>
<ul>
<li><code>START TRANSACTION|BEGIN</code>：开启事务</li>
<li><code>COMMIT</code>：提交事务</li>
<li><code>ROLLBACK</code>：回滚事务</li>
<li><code>SAVEPOINT identifier</code>：创建事务中的保存点</li>
<li><code>REPEASE SAVEPOINT identifier</code>：删除事务中的保存点</li>
<li><code>ROLLBACK TO[SACEPOINT]indentifier</code>：回滚的指定的保存点</li>
</ul>
<p>其中包括DDL等在内的数据库管理和修改操作开启<strong>隐性事务</strong>，无法在显式事务控制中使用。</p>
<span id="more"></span>
<p>系统的隔离级别由变量<code>transaction_isolation</code>变量控制，通过以下两种方式修改：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># <span class="number">1.</span>变量修改方式</span><br><span class="line"><span class="keyword">set</span> transaction_isolation <span class="operator">=</span> <span class="string">&#x27;repeatable-read&#x27;</span>;</span><br><span class="line"># <span class="number">2.</span>控制语句形式</span><br><span class="line"><span class="keyword">SET</span> TRANSACTION ISOLATION LEVEL READ COMMITTED</span><br></pre></td></tr></table></figure>
<p>事务的分类包括：</p>
<ol>
<li>扁平事务（Flat Transactions）</li>
<li>带有保存点的扁平事务（with savepoints）</li>
<li>链事务（Chained Transactions）</li>
<li>嵌套事务（Nested Transactions）</li>
<li>分布式事务（Distributed Transactions）</li>
</ol>
<p><strong>链式事务</strong>可以理解为多个连续的事务，一个事务结束其后继事务立刻在相同的隔离级别以及访问模式中继续执行。MySQL中开启方式为</p>
<ol>
<li><p>通过全局变量设置<code>completion_type = 2</code></p>
</li>
<li><p>结束提交使用<code>COMMIT AND CHAIN</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> TRANSACTION ISOLATION LEVEL READ COMMITTED</span><br><span class="line"><span class="keyword">START</span> TRANSACTION;</span><br><span class="line">           UPDATE accounts</span><br><span class="line">              <span class="keyword">SET</span> balance <span class="operator">=</span> balance <span class="operator">-</span> <span class="number">1500</span></span><br><span class="line">            <span class="keyword">WHERE</span> id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">           UPDATE accounts</span><br><span class="line">              <span class="keyword">SET</span> balance <span class="operator">=</span> balance <span class="operator">+</span> <span class="number">1500</span></span><br><span class="line">            <span class="keyword">WHERE</span> id <span class="operator">=</span> <span class="number">2</span>;</span><br><span class="line"> <span class="keyword">COMMIT</span> <span class="keyword">AND</span> CHAIN;</span><br><span class="line">           UPDATE accounts</span><br><span class="line">              <span class="keyword">SET</span> balance <span class="operator">=</span> balance <span class="operator">-</span> <span class="number">1000</span></span><br><span class="line">            <span class="keyword">WHERE</span> id <span class="operator">=</span> <span class="number">2</span>;</span><br><span class="line">           UPDATE accounts</span><br><span class="line">              <span class="keyword">SET</span> balance <span class="operator">=</span> balance <span class="operator">+</span> <span class="number">1000</span></span><br><span class="line">            <span class="keyword">WHERE</span> id <span class="operator">=</span> <span class="number">3</span>;</span><br><span class="line"><span class="keyword">COMMIT</span>;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>嵌套事务</strong>是一个多个事务按照树状层次结构组成的复合事务</p>
<ul>
<li>上层事务控制下层事务的提交和回滚</li>
<li>MySQL并不直接支持</li>
</ul>
<h3 id="原子性-持久性实现"><a href="#原子性-持久性实现" class="headerlink" title="原子性+持久性实现"></a>原子性+持久性实现</h3><p>Mysql中事务的原子性和持久性，基于redo和undo日志实现：</p>
<ol>
<li>redo日志：记录事务操作的日志，在事务提交前首先持久化redo日志，以实现事务的持久性</li>
<li>undo日志：用于撤销事务操作，回滚数据库状态到事务开始之前，实现了事务的原子性</li>
</ol>
<h4 id="redo日志"><a href="#redo日志" class="headerlink" title="redo日志"></a>redo日志</h4><p>redo日志保证持久性的原理可以简单理解为：</p>
<ol>
<li>受限于磁盘读写性能，每次数据库的增删改操作都写入磁盘，会导致数据库性能较差</li>
<li>为了解决这一问题，引入了缓冲，先在内存缓冲中执行对应修改操作，在合适的时机写入磁盘</li>
<li>然而内存中的数据是易失的（volatile），如果内存数据写入磁盘之前丢失，则数据库的持久性遭到破坏</li>
<li>为了保证持久性，引入redo日志，记录修改操作，并持久化到磁盘</li>
</ol>
<p>相当于用<strong>操作记录的持久化成本</strong>替代<strong>操作本身持久化成本</strong>，最终在保证持久性的情况下实现性能的提升。</p>
<p>然而redo日志本身持久化也类似于数据库持久化问题，也面临着性能和持久化的权衡，InnoDB中分为内存和磁盘两部分：</p>
<ol>
<li>Log Buffer：内存中的redo log buffer，事务在执行时实时写入日志到当前位置。</li>
<li>Redo Log File：磁盘中存储redo log的文件 ，在事务提交前，会将Log Buffer中的日志持久化到Redo Log File中</li>
</ol>
<img src="/2022/11/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/MySQL%E4%BA%8B%E5%8A%A1/image-20221119093517439.png" class="" title="image-20221119093517439">
<p>InnoDB提供了<code>innodb_flush_log_at_trx_commit</code>变量控制Log Buffer持久化时机：</p>
<ul>
<li><code>0</code>：事务提交时不进行redo log持久化，此时持久化时机只剩下<code>master thread</code>每1s执行一次的fsync。</li>
<li><code>1</code>：提交时将redo log写入Redo Log File，且调用fsync，保证真正写入。</li>
<li><code>2</code>：提交时将redo log写入Redo Log File，但不调用fsync，写入操作系统缓存。</li>
</ul>
<img src="/2022/11/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/MySQL%E4%BA%8B%E5%8A%A1/image-20221119093945536.png" class="" title="image-20221119093945536">
<p>Log Buffer中的日志刷新到磁盘的时机为：</p>
<ol>
<li>事务提交时，具体刷新行为由<code>innodb_flush_log_at_trx_commit</code>控制</li>
<li>当Log Buffer一半的空间被使用完毕时</li>
<li>Log checkpoint（包括Master thread Checkpoint等，具体未深入了解）</li>
</ol>
<p>另外两个概念控制redo log的扩张收缩：</p>
<ol>
<li>LSN（Log Sequence Number）：日志序列号，记录当前redo log数量，其单位为字节数</li>
<li>Checkpoint：检查点，代表已经完成持久化数据对应的操作号，当脏页刷新到磁盘中时，其已经实现了持久化，对应的redo log也就没必要存在。</li>
</ol>
<p>最后补充一点，redo log以512byte大小的block进行存储，大小于磁盘扇区相同，因此不需要<code>doublewrite</code>避免数据损坏。</p>
<h4 id="undo日志"><a href="#undo日志" class="headerlink" title="undo日志"></a>undo日志</h4><p>undo日志可以说是redo日志的<del>反方向的钟</del>，用来实现事务的回滚操作，两者区别在于：</p>
<ol>
<li>undo日志记录逻辑操作，redo日志记录物理操作</li>
<li>undo日志类似于临时文件，当“不需要回退时”undo日志即可删除，其持久化需要通过redo日志实现</li>
</ol>
<p>undo的回退不是时光倒流，而是通过执行逆操作抵消历史操作，即delete-&gt;insert、insert-&gt;delete、update-&gt;update back，产生的时机为：</p>
<ol>
<li>用户自定义表上的<code>insert</code>,<code>update</code>,<code>delete</code>操作</li>
<li>用户定义临时表上的<code>insert</code>,<code>update</code>,<code>delete</code>操作，其中临时表上的undo日志由于不需要错误恢复，不存在对应redo日志</li>
</ol>
<p>undo日志的生命流程可以概括为：</p>
<ol>
<li>开启事务，每执行一个SQL，产生对应的undo日志（undo日志会产生对应的redo日志）</li>
<li>当事务提交时，若无其他事务依赖（insert），则undo log直接删除；若存在其他事务依赖（mvvc依赖于undo实现delete、update的隔离），等待依赖事务结束，由<code>purge</code>操作统一回收。</li>
</ol>
<p><code>purge</code>操作用来真正的执行<code>delete</code>,<code>update</code>操作，回收对应undo日志</p>
<ul>
<li>由于隔离性（或者说mvvc）要求，<code>delete</code> <code>update</code>操作并不真正的执行，而是由<code>purge</code>操作统一处理</li>
<li><code>purge</code>回收undo日志执行对应的<code>delete</code>,<code>update</code>操作，回收采用了一定的优化，缓解undolog随机读取的性能问题</li>
</ul>
<p>undo日志存储按照段形式组织，undo日志存储在undo slot中</p>
<ol>
<li><p>两个table space：根据是否为临时表分别将对应undo日志存储在undo table space和global table space</p>
</li>
<li><p>128个 rollback segment：每个table space中包括128个回滚段</p>
</li>
<li><p>1024个 undo segment：每个rollback segment根据页大小，对应不同数量undo undo segment</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(innodb_page_size / 16) * innodb_rollback_segments * number of undo tablespaces</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="隔离性实现-锁"><a href="#隔离性实现-锁" class="headerlink" title="隔离性实现-锁"></a>隔离性实现-锁</h3><p>锁是InnoDB中进行并发控制，保证隔离性的手段之一，主要的分类方式有</p>
<ol>
<li>锁的粒度：表级锁、行级锁（record lock）、页锁</li>
<li>锁的排他性：共享（shared）锁和排他(Exclusive)锁</li>
</ol>
<p>下面总结InnoDB中主要的锁类型</p>
<h4 id="Intention-Locks（意向锁）"><a href="#Intention-Locks（意向锁）" class="headerlink" title="Intention Locks（意向锁）"></a>Intention Locks（意向锁）</h4><p>为了实现<code>multiple granularity locking</code>，事务在获取任意行级锁之前，首先获取Intention Locks，意向锁为表级别锁，与普通表级别锁的区别在于：</p>
<ol>
<li>意向锁之间并不互斥</li>
<li>意向锁与其他普通表级锁满足一般的互斥性质</li>
</ol>
<p>上述机制实现了<code>多粒度</code>，访问行只获取行级锁，后续当有操作需要获取表级锁时，由于只需要判断是否与意向锁互斥，而<strong>不需要遍历整个表，判断是否存在互斥行级锁</strong>。</p>
<p>综上所述，意向锁实际上可以理解为<strong>行锁的表级别标记</strong></p>
<h4 id="Gap-Locks（间隙锁）"><a href="#Gap-Locks（间隙锁）" class="headerlink" title="Gap Locks（间隙锁）"></a>Gap Locks（间隙锁）</h4><p>结合幻读场景理解，间隙锁对index范围上锁，如下SQL所示，该查询语句对范围<code>10-20</code>区间（开区间）上锁，若要插入一条<code>c1=15</code>的记录，需要等待当前锁释放。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c1 <span class="keyword">FROM</span> t <span class="keyword">WHERE</span> c1 <span class="keyword">BETWEEN</span> <span class="number">10</span> <span class="keyword">and</span> <span class="number">20</span> <span class="keyword">FOR</span> UPDATE;</span><br></pre></td></tr></table></figure>
<p>Gap Locks存在的目的是为了阻止其他事务在当前事务涉及到对应区间数据时，向该区间内插入新的数据</p>
<ul>
<li>当数据库隔离级别为<code>READ COMMITED</code>时，Gap Locks关闭不再使用</li>
</ul>
<h4 id="Next-Key-Locks"><a href="#Next-Key-Locks" class="headerlink" title="Next-Key Locks"></a>Next-Key Locks</h4><p>行锁和间隙锁的结合，锁定一条记录以及其左区间（索引确定的范围），假设表中包含<code>index=10, 11, 13, and 20</code>四条数据，则可能的Next-Key Lock上锁位置可以有</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(negative infinity, <span class="number">10</span>]</span><br><span class="line">(<span class="number">10</span>, <span class="number">11</span>]</span><br><span class="line">(<span class="number">11</span>, <span class="number">13</span>]</span><br><span class="line">(<span class="number">13</span>, <span class="number">20</span>]</span><br><span class="line">(<span class="number">20</span>, positive infinity)</span><br></pre></td></tr></table></figure>
<p>InnoDB的<code>REPEATABLE READ</code>隔离级别中通过Next-Key Lock，实现幻读问题的解决</p>
<h4 id="Insert-Intention-Locks（插入意向锁）"><a href="#Insert-Intention-Locks（插入意向锁）" class="headerlink" title="Insert Intention Locks（插入意向锁）"></a>Insert Intention Locks（插入意向锁）</h4><p>插入意向锁为了解决多个事务插入相同位置（相同主键）的情况，由于目标插入数据项不存在，不能使用普通的行锁解决，只能通过”意向”的方式，实现互斥。案例如下：</p>
<ol>
<li><p>事务1获取对应区间的间隙锁</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">begin</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> test_select <span class="keyword">where</span> id <span class="operator">&gt;</span> <span class="number">4</span> <span class="keyword">for</span> share;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure>
</li>
<li><p>事务2此时向对应间隙插入数据，获取插入意向锁与间隙锁互斥，会等待事务1释放锁</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">begin</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> test_select <span class="keyword">values</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用<code>show engine innodb status</code>命令查看系统当前锁，可以看到事务在等待锁释放</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">insert into test_select values(5,6,7)</span><br><span class="line">------- TRX HAS BEEN WAITING 4 SEC FOR THIS LOCK TO BE GRANTED:</span><br><span class="line">RECORD LOCKS space id 68 page no 4 n bits 72 index PRIMARY of table `test_db`.`test_select` trx id 9758 lock_mode X insert intention waiting</span><br><span class="line">Record lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="AUTO-INC-Locks-自增锁"><a href="#AUTO-INC-Locks-自增锁" class="headerlink" title="AUTO-INC Locks(自增锁)"></a>AUTO-INC Locks(自增锁)</h4><p>当表具有自增（Auto Increment）字段时，为了避免并发插入导致的自增字段出现重复，插入前首先获得自增锁，该锁将并发插入变为了顺序插入，保证了自增字段逐个递增。</p>
<p>由于严格的自增锁会严重的降低并发性能，MySQL提供了<code>innodb_autoinc_lock_mode</code> 参数配置插入的不同机制（这部分内容还有问题，详细参考文档<a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-auto-increment-handling.html#innodb-auto-increment-lock-modes">AUTO_INCREMENT Handling in InnoDB</a>）</p>
<ol>
<li><code>0</code>：tradition模式，每个插入语句插入前获取AUTO-INC Lock，插入执行完毕后，释放锁</li>
<li><code>1</code>：consecutive模式，如果已知insert语句要插入的数据条数，提前分配对应数量的自增值，避免长时间占有锁</li>
<li><code>2</code>：interleaved模式：不使用表级锁，多个事务可并发插入，保证自增字段的唯一性，但不保证一个插入语句中插入的数据是连续的</li>
</ol>
<h4 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h4><p>InnoDB通过<code>死锁检测</code>手段，在发生死锁情况时主动关闭死锁依赖中最小的事务，从而破除思索。有两个参数控制：</p>
<ol>
<li><p><code>innodb_deadlock_detect</code>：默认处于开启状态，表示是否进行死锁检测</p>
</li>
<li><p><code>innodb_lock_wait_timeout</code>：等待获取锁的超时时间，当一个事务超过一定时间等待锁，InnoDB会终止该事务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> insert into test_select values(5,6,7);</span></span><br><span class="line">ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="隔离性实现-MVCC"><a href="#隔离性实现-MVCC" class="headerlink" title="隔离性实现-MVCC"></a>隔离性实现-MVCC</h3><blockquote>
<p>Multiversion concurrency control (MVCC) is a database optimization technique that creates duplicate copies of records so that data can be safely read and updated at the same time.</p>
</blockquote>
<p>MVCC(Multiversion concurrency control) 多版本并发控制是数据库优化并发性能的一种手段，通过创建读副本，在不使用锁的情况下，解决并发读写的冲突问题，整体上简单理解MVCC：</p>
<ol>
<li>每个修改（insert、update、delete等）操作都会导致产生一个新版本的数据，并不会影响老版本数据</li>
<li>读操作会从有限多个版本的数据选择合适的数据进行读取，以满足隔离性要求。</li>
</ol>
<p>MVCC虽然有效的避免了读写冲突，提升了并发性能，但存在一定问题</p>
<ol>
<li>MVCC机制难以设计和实现</li>
<li>不同版本的数据清理回收的时机难以确定</li>
</ol>
<h4 id="InnoDB-MVCC实现"><a href="#InnoDB-MVCC实现" class="headerlink" title="InnoDB MVCC实现"></a>InnoDB MVCC实现</h4><p>InnoDB的实现并没有采用多个数据版本的形式，而是采用单一数据+undo日志的实现思路</p>
<ol>
<li>所有数据行存在两个隐藏属性：<code>trx_id</code>记录最新修改的事务ID，作为并发版本时间戳；<code>roll_pointer</code>指向最新undo log</li>
<li>所有的写操作（insert，update）直接写入数据，并不是生成一个新版本的数据；删除（delete）操作并不真的删除数据项，等待删除事务提交后，MVCC无需使用时，由<code>purge</code>操作一并回收</li>
<li><strong>读操作</strong>根据构建<code>ReadView</code>，根据读取事务ID和读取数据行<code>trx_id</code>，判断是否需要基于<code>undo log</code> 回退</li>
</ol>
<p><code>ReadView</code>用于维护并发事务的时间戳信息，在读取过程中指导回退操作，主要包括：</p>
<ol>
<li><code>creator_trx_id</code>：当前事务ID</li>
<li><code>trx_ids</code>：与当前事务存在冲突的并发事务ID列表</li>
<li><code>up_limit_in</code>：最小事务ID<code>min(trx_ids)</code>，用来判断”过去”的事务</li>
<li><code>low_limit_id</code>：当前事务创建时，未分配的最小事务ID，用来判断”未来”的事务</li>
</ol>
<p>判断是否需要回退避免冲突的条件如下：</p>
<ol>
<li>可直接访问的情况（待访问数据行的<code>trx_id</code> ）<ul>
<li><code>trx_id</code> =  <code>creator_trx_id</code>（自己的修改自己能看到）；</li>
<li><code>trx_id</code> &lt;=  <code>up_limit_in</code>（历史已提交的事务能看到）;</li>
<li><code>trx_id</code> &lt;<code>low_limit_id</code> 且落在<code>trx_ids</code>外（非未来且和自己非并发的事务能看到）；</li>
</ul>
</li>
<li>需要访问<code>undo log</code>回退到历史数据的情况<ul>
<li><code>trx_id</code> 落在<code>trx_ids</code>内（和自己并发未提交的修改）</li>
<li><code>trx_id</code> &gt;=<code>low_limit_id</code> （在自己之后事务修改）</li>
</ul>
</li>
</ol>
<p>上述判断条件的本质上就是<strong>保证当前事务只能看到历史版本的数据</strong>，Mysql只在两个隔离级别中使用MVVC</p>
<ol>
<li><code>Read Committed</code>：一次事务中的每一次读操作都重新生成<code>ReadView</code></li>
<li><code>Repeatable Read</code>：一次事务只在第一次读取操作时生成<code>ReadView</code></li>
</ol>
<p>区别于MySQL，PostgreSQL写新数据时，旧数据不删除，直接插入新数据，新旧数据上保存对应的事务ID，通过新旧数据+事务ID实现MVCC机制</p>
<h4 id="一个案例展示MVCC"><a href="#一个案例展示MVCC" class="headerlink" title="一个案例展示MVCC"></a>一个案例展示MVCC</h4><p>在MySQL的<code>Repeatable Read</code>隔离级别下，并行执行两个事务：</p>
<ol>
<li><p>事务1查询表中所有数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> select * from test_select;</span></span><br><span class="line">+----+------+------+</span><br><span class="line">| id | age  | sex  |</span><br><span class="line">+----+------+------+</span><br><span class="line">|  1 |    2 |    3 |</span><br><span class="line">|  2 |    3 |    4 |</span><br><span class="line">|  3 |    4 |    5 |</span><br><span class="line">|  4 |    5 |    6 |</span><br><span class="line">+----+------+------+</span><br></pre></td></tr></table></figure>
</li>
<li><p>事务2更新<code>id=2</code>的数据，并提交</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; update test_select set age = 10 where id = 2;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br></pre></td></tr></table></figure>
</li>
<li><p>事务1查询<code>id=2</code>的数据，<strong>无变化</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> select * from test_select <span class="built_in">where</span> id = 2;</span></span><br><span class="line">+----+------+------+</span><br><span class="line">| id | age  | sex  |</span><br><span class="line">+----+------+------+</span><br><span class="line">|  2 |    3 |    4 |</span><br><span class="line">+----+------+------+</span><br></pre></td></tr></table></figure>
</li>
<li><p>事务1更新<code>id=2</code>的数据，并查询结果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> update test_select <span class="built_in">set</span> age = age + 1 <span class="built_in">where</span> id = 2;</span></span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> select * from test_select <span class="built_in">where</span> id = 2;</span></span><br><span class="line">+----+------+------+</span><br><span class="line">| id | age  | sex  |</span><br><span class="line">+----+------+------+</span><br><span class="line">|  2 |   11 |    4 |</span><br><span class="line">+----+------+------+</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>案例的结果表明，<strong>MVCC隔离读写操作，由快照读实现，所有的修改都实时的反映在最新数据上</strong>。</p>
<h3 id="补充-分布式事务"><a href="#补充-分布式事务" class="headerlink" title="补充-分布式事务"></a>补充-分布式事务</h3><p>MySQL支持分布式事务，称为XA事务，其中XA是由X/Open组织提出的<a href="https://so.csdn.net/so/search?q=分布式&amp;spm=1001.2101.3001.7020">分布式</a>事务的规范，一个XA事务由一个或者多个资源管理器（数据库）、一个事务管理器以及一个应用程序组成</p>
<ul>
<li>基于两阶段提交（two-phase commit）方式实现，事务管理器为事务协调者</li>
<li>资源管理器相当于分布式事务中参与的一个个数据库</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>从本科开始接触数据库事务这一概念，一带而过没有深入学习，经过这段时间从理论以及MySQL实现角度的学习和总结，对于事务有了一定程度的理解，更能体会并发控制的困难和事务实现设计的巧妙，希望在以后的编码过程中，能够用到事务学习中了解到的并发控制思路。</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>InnoDB</tag>
        <tag>MVCC</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL性能调优</title>
    <url>/2022/11/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/MySQL%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</url>
    <content><![CDATA[<h2 id="MySQL性能调优"><a href="#MySQL性能调优" class="headerlink" title="MySQL性能调优"></a>MySQL性能调优</h2><p>MySQL性能调优流程从低到高分别为：</p>
<ol>
<li><strong>查询及索引优化</strong>：SQL优化以及索引优化</li>
<li><strong>库表优化</strong>：设计优化以及分库分表等操作</li>
<li>MySQL参数/服务器优化：分为硬件调优和MySQL参数调优</li>
<li>分库分表横向扩展优化</li>
</ol>
<h3 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h3><p>SQL作为访问操作数据库的主要手段，采用一定的优化手段提升SQL性能，对于提高数据库整体性能具有重要意义，下面内容为对于SQL性能优化的学习和总结，主要内容来自：<a href="https://www.bilibili.com/video/BV1iq4y1u7vj?p=150&amp;vd_source=69aca4985c128187979e0f8d4604125f">尚硅谷mysql教程</a>，<a href="https://dev.mysql.com/doc/refman/8.0/en/select-optimization.html">MySQL 8.0 Reference Manual Optimizing SELECT Statements</a>。</p>
<span id="more"></span>
<h4 id="SQL性能监控"><a href="#SQL性能监控" class="headerlink" title="SQL性能监控"></a>SQL性能监控</h4><p>写出在进行数据库性能优化前首先应通过一些性能监控手段，分析MySQL目前出现性能瓶颈的原因，针对原因采用对应的调优策略，性能监控手段主要包括：</p>
<ol>
<li>通过<code>show status</code>查看一些记录在全局变量中的性能参数，例如<code>slow_queries</code>，<code>last_query_cost</code>等。</li>
<li>通过慢查询日志定位执行较慢的SQL。</li>
<li>使用<code>show profile</code>查看SQL执行成本。</li>
<li>使用<code>EXPLAIN</code>分析查询语句。</li>
<li>Performance Schema 以及 sys Schema两个参数数据库。</li>
</ol>
<p>由于性能监控脱离实际的应用场景难以加以实践以达到深入了解，下面的总结偏向理论上的了解，在以后遇到实际问题时能够有一定的准备。</p>
<h5 id="慢查询"><a href="#慢查询" class="headerlink" title="慢查询"></a>慢查询</h5><p>MySQL会将执行时间缓慢的SQL记录在日志中，即慢查询日志，慢查询日志中的SQL时分析目前系统性能瓶颈的入口之一。</p>
<p>慢查询日志默认关闭，通过修改全局变量<code>slow_query_log</code>开启:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show variables like <span class="string">&#x27;%slow_query_log&#x27;</span>;</span></span><br><span class="line">+----------------+-------+</span><br><span class="line">| Variable_name  | Value |</span><br><span class="line">+----------------+-------+</span><br><span class="line">| slow_query_log | OFF   |</span><br><span class="line">+----------------+-------+</span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show variables like <span class="string">&#x27;%slow_query_log%&#x27;</span>;</span></span><br><span class="line">+---------------------+---------------------------------+</span><br><span class="line">| Variable_name       | Value                           |</span><br><span class="line">+---------------------+---------------------------------+</span><br><span class="line">| slow_query_log      | ON                              |</span><br><span class="line">| slow_query_log_file | /var/lib/mysql/hadoop1-slow.log |</span><br><span class="line">+---------------------+---------------------------------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>
<p>慢查询SQL的阈值存储在全局变量<code>long_query_time</code>中，默认为10秒</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show variables like <span class="string">&#x27;%long_query_time%&#x27;</span>;</span></span><br><span class="line">+-----------------+-----------+</span><br><span class="line">| Variable_name   | Value     |</span><br><span class="line">+-----------------+-----------+</span><br><span class="line">| long_query_time | 10.000000 |</span><br><span class="line">+-----------------+-----------+</span><br></pre></td></tr></table></figure>
<p>通过<code>slow_queries</code>全局变量，即可获得慢查询SQL的数量：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show status like <span class="string">&#x27;slow_queries&#x27;</span>;</span></span><br><span class="line">+---------------+-------+</span><br><span class="line">| Variable_name | Value |</span><br><span class="line">+---------------+-------+</span><br><span class="line">| Slow_queries  | 2     |</span><br><span class="line">+---------------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>
<p>对于慢查询日志的分析可使用<code>mysqldumpslow</code>工具</p>
<h5 id="Show-profiling"><a href="#Show-profiling" class="headerlink" title="Show profiling"></a>Show profiling</h5><p>通过<code>show profiling</code>查看最近执行sql在不同执行阶段耗费的时间，能够较为详细的展示一条SQL的执行成本，默认关闭：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;profiling%&#x27;</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------+-------+</span></span><br><span class="line"><span class="operator">|</span> Variable_name          <span class="operator">|</span> <span class="keyword">Value</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------+-------+</span></span><br><span class="line"><span class="operator">|</span> profiling              <span class="operator">|</span> OFF   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> profiling_history_size <span class="operator">|</span> <span class="number">15</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------+-------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span>, <span class="number">1</span> warning (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure>
<p>开启后自动记录最近的几条sql，使用<code>show profiles</code>查看：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show profiles;</span></span><br><span class="line">+----------+------------+-----------------------------------------+</span><br><span class="line">| Query_ID | Duration   | Query                                   |</span><br><span class="line">+----------+------------+-----------------------------------------+</span><br><span class="line">|        1 | 0.00060375 | select * from customers                 |</span><br><span class="line">|        2 | 0.00121350 | show tables                             |</span><br><span class="line">|        3 | 0.00014175 | show create employees                   |</span><br><span class="line">|        4 | 0.01286675 | show create table employees             |</span><br><span class="line">|        5 | 0.00246975 | select distinct(company) from employees |</span><br><span class="line">+----------+------------+-----------------------------------------+</span><br><span class="line">5 rows in set, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure>
<p>使用<code>show prfoile for query x</code>查看某条sql的详细执行成本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show profile <span class="keyword">for</span> query 5;</span></span><br><span class="line">+--------------------------------+----------+</span><br><span class="line">| Status                         | Duration |</span><br><span class="line">+--------------------------------+----------+</span><br><span class="line">| starting                       | 0.000051 |</span><br><span class="line">| Executing hook on transaction  | 0.000003 |</span><br><span class="line">| starting                       | 0.000005 |</span><br><span class="line">| checking permissions           | 0.000004 |</span><br><span class="line">| Opening tables                 | 0.000026 |</span><br><span class="line">| init                           | 0.000003 |</span><br><span class="line">| System lock                    | 0.000006 |</span><br><span class="line">| optimizing                     | 0.000003 |</span><br><span class="line">| statistics                     | 0.000028 |</span><br><span class="line">| preparing                      | 0.000363 |</span><br><span class="line">| executing                      | 0.001700 |</span><br><span class="line">| end                            | 0.000005 |</span><br><span class="line">| query end                      | 0.000003 |</span><br><span class="line">| waiting for handler commit     | 0.000212 |</span><br><span class="line">| closing tables                 | 0.000012 |</span><br><span class="line">| freeing items                  | 0.000036 |</span><br><span class="line">| cleaning up                    | 0.000012 |</span><br><span class="line">+--------------------------------+----------+</span><br></pre></td></tr></table></figure>
<h5 id="EXPLAIN分析查询语句"><a href="#EXPLAIN分析查询语句" class="headerlink" title="EXPLAIN分析查询语句"></a>EXPLAIN分析查询语句</h5><p>通过<code>EXPLAIN</code>能够查看某个SQL语句的具体执行计划，该执行计划为优化器选择的最优计划，基本使用方式如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">EXPLAIN 增删改<span class="keyword">SQL</span>语句</span><br></pre></td></tr></table></figure>
<img src="/2022/11/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/MySQL%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/image-20221109202942780.png" class="" title="image-20221109202942780">
<p>输出的参数含义如下：</p>
<ul>
<li>其中较为重要的有<code>type</code>、<code>Extra</code></li>
</ul>
<img src="/2022/11/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/MySQL%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/image-20221111103403987.png" class="" title="image-20221111103403987">
<p><code>type</code>描述表连接的方式（我理解的是从表中取值的方式），常见的值有</p>
<ol>
<li><p><code>system</code>：表中只有一行数据</p>
</li>
<li><p><code>const</code>：查询结果最多有一个匹配结果，一般为主键/唯一索引的等值匹配</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> tbl_name <span class="keyword">WHERE</span> primary_key<span class="operator">=</span><span class="number">1</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>eq_ref</code>：连接时，当前表每条数据基于连接key能够确定引用表中一条数据，即连接的key为主键/唯一索引</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> ref_table,other_table</span><br><span class="line">  <span class="keyword">WHERE</span> ref_table.key_column<span class="operator">=</span>other_table.column;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>ref</code>：与<code>eq_ref</code>类似，只是连接/筛选key为普通索引，无法确定一条引用数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> ref_table <span class="keyword">WHERE</span> key_column<span class="operator">=</span>expr;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> ref_table,other_table</span><br><span class="line">  <span class="keyword">WHERE</span> ref_table.key_column<span class="operator">=</span>other_table.column;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>range</code>：索引key上的范围查询： <a href="https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html#operator_equal"><code>=</code></a>, <a href="https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html#operator_not-equal"><code>&lt;&gt;</code></a>, <a href="https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html#operator_greater-than"><code>&gt;</code></a>, <a href="https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html#operator_greater-than-or-equal"><code>&gt;=</code></a>, <a href="https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html#operator_less-than"><code>&lt;</code></a>, <a href="https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html#operator_less-than-or-equal"><code>&lt;=</code></a>, <a href="https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html#operator_is-null"><code>IS NULL</code></a>, <a href="https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html#operator_equal-to"><code>&lt;=&gt;</code></a>, <a href="https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html#operator_between"><code>BETWEEN</code></a>, <a href="https://dev.mysql.com/doc/refman/8.0/en/string-comparison-functions.html#operator_like"><code>LIKE</code></a>, or <a href="https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html#operator_in"><code>IN()</code></a> </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> tbl_name</span><br><span class="line">  <span class="keyword">WHERE</span> key_column <span class="keyword">IN</span> (<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>index</code>：遍历扫描整个索引树</p>
<ul>
<li>当索引为覆盖索引，即不涉及回表操作时，<code>Extra</code>中会提示<code>using Index</code>。</li>
</ul>
</li>
<li><p><code>ALL</code>：无索引的扫描全部数据。</p>
</li>
</ol>
<p><code>Extra</code>作为补充信息，补充说明SQL的执行信息，取值较多，下面列举几个常见的值：</p>
<ol>
<li><p><code>Using index</code>：不需要回表的，index扫描。</p>
</li>
<li><p><code>Using index condition</code>：索引下推，即首先通过索引判断是否需要扫描所有数据。</p>
<ul>
<li><p>索引定义为：<code>INDEX (zipcode, lastname, firstname)</code></p>
</li>
<li><p>SQL基上述index首先在索引上使用<code>WHERE zipcode=&#39;95054&#39; AND lastname LIKE &#39;%etrunia%&#39;</code>过滤数据，取得非过滤数据主键后，再访问真正的行，应用<code>AND address LIKE &#39;%Main Street%&#39;</code></p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> people</span><br><span class="line">  <span class="keyword">WHERE</span> zipcode<span class="operator">=</span><span class="string">&#x27;95054&#x27;</span></span><br><span class="line">  <span class="keyword">AND</span> lastname <span class="keyword">LIKE</span> <span class="string">&#x27;%etrunia%&#x27;</span></span><br><span class="line">  <span class="keyword">AND</span> address <span class="keyword">LIKE</span> <span class="string">&#x27;%Main Street%&#x27;</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>Using filesort/Using temporary</code>：使用文件排序/临时表，这是两种性能较差的SQL提示</p>
</li>
</ol>
<h5 id="Optimizer-Trace"><a href="#Optimizer-Trace" class="headerlink" title="Optimizer Trace"></a>Optimizer Trace</h5><p><code>optimizer_trace</code> 用来跟踪优化器做出的优化决策，并将追踪结果存储在<code>INFORMATION_SCHEMA .OPTIMIZER_TRACE</code>表中，通过变量<code>optimizer_trace</code>开启：</p>
<img src="/2022/11/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/MySQL%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/image-20221111111005896.png" class="" title="image-20221111111005896">
<p>其中<code>OPTIMIZER_TRACE</code>表属性列为:</p>
<ol>
<li><code>QUERY</code>：监控的SQL语句</li>
<li><code>TRACE</code>：以json形式存储的追踪数据</li>
<li><code>MISSING_BYTES_BEYOND_MAX_MEM_SIZE</code>：由于追踪信息大小存在限制，该字段存储省略的追踪信息大小</li>
<li><code>INSUFFICIENT_PRIVILEGES</code>：表明用户是否有权限查看 trace，0表示无权限，对应<code>TRACE</code>字段为空</li>
</ol>
<h5 id="Performance-Schema"><a href="#Performance-Schema" class="headerlink" title="Performance Schema"></a>Performance Schema</h5><blockquote>
<p> The Performance Schema is a tool to help a DBA do performance tuning by taking real measurements instead of “wild guesses.” </p>
</blockquote>
<p>性能数据库中的表主要用来帮助DBA定位性能问题，一般流程为：</p>
<ol>
<li>运行用例</li>
<li>查看对应的性能数据表，分析造成性能问题的原因</li>
<li>一旦确定原因，采取对应的操作解决问题<ul>
<li>修改系统参数（缓存、内存等）</li>
<li>修改查询语句</li>
<li>修改数据库关系模式(表、索引等)</li>
</ul>
</li>
<li>重复步骤直到完成修复</li>
</ol>
<p>MySQL 8.0 Reference Manual提供了如何使用Performance Scherma的文档：</p>
<ol>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/performance-schema-examples.html">Using the Performance Schema to Diagnose Problems</a></li>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/performance-schema-quick-start.html">Performance Schema Quick Start</a></li>
</ol>
<h5 id="sys-Schema"><a href="#sys-Schema" class="headerlink" title="sys Schema"></a>sys Schema</h5><blockquote>
<p>MySQL 8.0 includes the <a href="https://dev.mysql.com/doc/refman/8.0/en/sys-schema.html"><code>sys</code></a> schema, a set of objects that helps DBAs and developers interpret data collected by the Performance Schema</p>
</blockquote>
<p>用来帮助理解<code>Performance Schema</code>收集到的性能数据，具体使用查看文档：<a href="https://dev.mysql.com/doc/refman/8.0/en/sys-schema.html">Chapter 28 MySQL sys Schema</a></p>
<h4 id="Optimizer的优化策略"><a href="#Optimizer的优化策略" class="headerlink" title="Optimizer的优化策略"></a>Optimizer的优化策略</h4><h5 id="1-可用index，但是使用全表扫描"><a href="#1-可用index，但是使用全表扫描" class="headerlink" title="1.可用index，但是使用全表扫描"></a>1.可用index，但是使用全表扫描</h5><p>optimizer会根据表大小、查询行数、I/O块大小等因素，确实是否使用索引，若使用索引的成本大于全表扫描的成本，则优化器会选择全表扫描</p>
<h5 id="2-Index-Dive"><a href="#2-Index-Dive" class="headerlink" title="2. Index Dive"></a>2. Index Dive</h5><p>对于多值范围where条件，采用<code>Index Dive</code>估计实际需要访问的行数（通过估测行数确定执行计划的成本，用于后续不同查询计划的成本比较）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">where</span> col_name <span class="keyword">IN</span>(val1, ..., valN)</span><br><span class="line"><span class="keyword">where</span> col_name <span class="operator">=</span> val1 <span class="keyword">OR</span> ... <span class="keyword">OR</span> col_name <span class="operator">=</span> valN</span><br></pre></td></tr></table></figure>
<p>上述查询根据上index的不同分为两种情况</p>
<ol>
<li>当对应属性列定义了唯一索引，则对于每个值优化器估计每个值访问行数为1</li>
<li>否则使用<code>index dive</code>或者<code>index statistics</code>即index使用的统计信息，估测当前条件需要的访问行数</li>
</ol>
<p><code>index dive</code> 索引下钻，将元组值转化为区间，使用区间内行数作为访问行数的估计</p>
<ul>
<li>例如：<code>in (1, 3, 6)</code>，优化器估计的行数为：<code>(1,3),(3,6)</code>两个区间的行数和</li>
<li>相较于基于<code>index statistics</code>的行数估计更加精确，但是由于需要访问索引，引入了额外的IO成本</li>
</ul>
<p>可通过全局变量<code>eq_range_index_dive_limit</code>设置可使用<code>index dive</code> 的值数量上限</p>
<ul>
<li><code>eq_range_index_dive_limit</code>设置为0，表示关闭<code>index dive</code> </li>
</ul>
<img src="/2022/11/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/MySQL%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/image-20221113154357866.png" class="" title="image-20221113154357866">
<h5 id="3-Index-Merge-Optimization"><a href="#3-Index-Merge-Optimization" class="headerlink" title="3. Index Merge Optimization"></a>3. Index Merge Optimization</h5><p>如下语句，对于多个索引上的检索条件求<code>and,or</code>的where子句，MySQL分别在对应索引上执行查询后，对查询结果进行求<code>union,intersection</code>并、交实现查询，该操作称为索引合并<code>Index Merge</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> tbl_name <span class="keyword">WHERE</span> key1 <span class="operator">=</span> <span class="number">10</span> <span class="keyword">OR</span> key2 <span class="operator">=</span> <span class="number">20</span>;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> tbl_name</span><br><span class="line">  <span class="keyword">WHERE</span> (key1 <span class="operator">=</span> <span class="number">10</span> <span class="keyword">OR</span> key2 <span class="operator">=</span> <span class="number">20</span>) <span class="keyword">AND</span> non_key <span class="operator">=</span> <span class="number">30</span>;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> t1, t2</span><br><span class="line">  <span class="keyword">WHERE</span> (t1.key1 <span class="keyword">IN</span> (<span class="number">1</span>,<span class="number">2</span>) <span class="keyword">OR</span> t1.key2 <span class="keyword">LIKE</span> <span class="string">&#x27;value%&#x27;</span>)</span><br><span class="line">  <span class="keyword">AND</span> t2.key1 <span class="operator">=</span> t1.some_col;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> t1, t2</span><br><span class="line">  <span class="keyword">WHERE</span> t1.key1 <span class="operator">=</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">AND</span> (t2.key1 <span class="operator">=</span> t1.some_col <span class="keyword">OR</span> t2.key2 <span class="operator">=</span> t1.some_col2);</span><br></pre></td></tr></table></figure>
<p><code>Index Merge</code>适用范围限于：</p>
<ol>
<li>一张表的不同索引列</li>
<li>不适用于全文索引</li>
</ol>
<p>支持三种算法：</p>
<ol>
<li><p><code>Using intersect(...)</code></p>
</li>
<li><p><code>Using union(...)</code></p>
<ul>
<li>1，2要求索引必须为等值查询，且联合索引必须全部覆盖，只能在主键索引上存在范围，即为了保证查询结果的有序性。</li>
</ul>
</li>
<li><p><code>Using sort_union(...)</code></p>
<ul>
<li>3是对2的放宽，不要求等值查询，也不要求全部覆盖联合索引，导致返回结果可能按照索引顺序不满足逐渐顺序。由于求并集去重需要比较主键，此时要求两个集合返回数据是有序的，因为需要排序。</li>
</ul>
</li>
</ol>
<h5 id="4-Hash-Join-Optimization"><a href="#4-Hash-Join-Optimization" class="headerlink" title="4. Hash Join Optimization"></a>4. Hash Join Optimization</h5><p> MySQL (8.0.18 and later) 添加的新的连接优化方式，当表连接基于等值连接且对应值上不存在索引时，MySQL会自动采用<code>Hash Join</code>实现连接性能优化，基本步骤如下：</p>
<ol>
<li><p>首先扫描小表，构建对应字段-&gt;行数据的<code>hash table</code></p>
</li>
<li><p>遍历大表中的每一条数据，查询<code>hash table</code>获得连接行，生成结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> EXPLAIN FORMAT<span class="operator">=</span>TREE</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">FROM</span> t1</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">JOIN</span> t2</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>         <span class="keyword">ON</span> (t1.c1 <span class="operator">=</span> t2.c1 <span class="keyword">AND</span> t1.c2 <span class="operator">&lt;</span> t2.c2)</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">JOIN</span> t3</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>         <span class="keyword">ON</span> (t2.c1 <span class="operator">=</span> t3.c1)\G</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span> <span class="number">1.</span> <span class="type">row</span> <span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span></span><br><span class="line">EXPLAIN: <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">Inner</span> hash <span class="keyword">join</span> (t3.c1 <span class="operator">=</span> t1.c1)  (cost<span class="operator">=</span><span class="number">1.05</span> <span class="keyword">rows</span><span class="operator">=</span><span class="number">1</span>)</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">Table</span> scan <span class="keyword">on</span> t3  (cost<span class="operator">=</span><span class="number">0.35</span> <span class="keyword">rows</span><span class="operator">=</span><span class="number">1</span>)</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> Hash</span><br><span class="line">        <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">Filter</span>: (t1.c2 <span class="operator">&lt;</span> t2.c2)  (cost<span class="operator">=</span><span class="number">0.70</span> <span class="keyword">rows</span><span class="operator">=</span><span class="number">1</span>)</span><br><span class="line">            <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">Inner</span> hash <span class="keyword">join</span> (t2.c1 <span class="operator">=</span> t1.c1)  (cost<span class="operator">=</span><span class="number">0.70</span> <span class="keyword">rows</span><span class="operator">=</span><span class="number">1</span>)</span><br><span class="line">                <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">Table</span> scan <span class="keyword">on</span> t2  (cost<span class="operator">=</span><span class="number">0.35</span> <span class="keyword">rows</span><span class="operator">=</span><span class="number">1</span>)</span><br><span class="line">                <span class="operator">-</span><span class="operator">&gt;</span> Hash</span><br><span class="line">                    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">Table</span> scan <span class="keyword">on</span> t1  (cost<span class="operator">=</span><span class="number">0.35</span> <span class="keyword">rows</span><span class="operator">=</span><span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>相较于<code>Nested-Loop Join Algorithm</code>和<code>Block Nested-Loop Join Algorithm</code>，<code>Hash Join</code>由于事先需要构建<code>hash</code>表，对于CPU和内存的要求较高</p>
<ul>
<li>通过变量<code>join_buffer_size</code>控制<code>Hash Join</code>所能占用的缓存空间</li>
<li>当<code>Hash table size &gt; join_buffer_size</code>  时，MySQL会将使用磁盘上的文件存储溢出内容</li>
</ul>
<p>简单介绍上面提到的两外两种连接方法</p>
<ol>
<li><p><code>Nested-Loop Join Algorithm</code>：直觉上的连接方法，双层嵌套循环，外层遍历表1行，内层遍历表2行，逐个连接判断</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="keyword">each</span> <span class="type">row</span> <span class="keyword">in</span> t1 matching <span class="keyword">range</span> &#123;</span><br><span class="line">  <span class="keyword">for</span> <span class="keyword">each</span> <span class="type">row</span> <span class="keyword">in</span> t2 matching reference key &#123;</span><br><span class="line">    <span class="keyword">for</span> <span class="keyword">each</span> <span class="type">row</span> <span class="keyword">in</span> t3 &#123;</span><br><span class="line">      if <span class="type">row</span> satisfies <span class="keyword">join</span> conditions, send <span class="keyword">to</span> client</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>Block Nested-Loop Join Algorithm</code>：在方法1上的优化，外层循环按照block遍历，每个block中包含多条数据，减少了内层循环数据IO次数， MySQL 8.0.18以后默认使用<code>Hash Join</code>，MySQL 8.0.20不再使用该方法</p>
</li>
</ol>
<h5 id="5-Index-Condition-Pushdown（ICP）"><a href="#5-Index-Condition-Pushdown（ICP）" class="headerlink" title="5. Index Condition Pushdown（ICP）"></a>5. Index Condition Pushdown（ICP）</h5><p>索引条件下推优化，将索引列上的<code>where</code>检索条件<strong>下推到搜索引擎层</strong>面，减少检索涉及到的数据行数</p>
<ul>
<li>一般的查询过程：遍历数据项，根据Index<strong>获取数据项后</strong>，根据数据项内容和where条件判断是否过滤数据。</li>
<li>开启ICP后：遍历数据行，根据where条件+index判断数据项是否过滤，若满足where条件，则获取数据项返回。</li>
</ul>
<p>能够引用ICP的条件为：</p>
<ol>
<li>ICP只能用于<a href="https://dev.mysql.com/doc/refman/8.0/en/explain-output.html#jointype_range"><code>range</code></a>, <a href="https://dev.mysql.com/doc/refman/8.0/en/explain-output.html#jointype_ref"><code>ref</code></a>, <a href="https://dev.mysql.com/doc/refman/8.0/en/explain-output.html#jointype_eq_ref"><code>eq_ref</code></a>, and <a href="https://dev.mysql.com/doc/refman/8.0/en/explain-output.html#jointype_ref_or_null"><code>ref_or_null</code></a> 等访问类型</li>
<li>只针对<code>InnoDB</code>和<code>MyISAM</code>表</li>
<li>ICP目的为了减少数据行的访问，对于<code>InnoDB</code>来说，只用于次级索引，聚簇索引由于数据已经加载到内存中，没有使用ICP的意义</li>
</ol>
<p>ICP通过优化器参数参数<code>index_condition_pushdown</code>设置</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> optimizer_switch <span class="operator">=</span> <span class="string">&#x27;index_condition_pushdown=off&#x27;</span>;</span><br><span class="line"><span class="keyword">SET</span> optimizer_switch <span class="operator">=</span> <span class="string">&#x27;index_condition_pushdown=on&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h5 id="6-ORDER-BY-Optimization"><a href="#6-ORDER-BY-Optimization" class="headerlink" title="6.ORDER BY Optimization"></a>6.ORDER BY Optimization</h5><p>由于索引中的数据已经排好序，可以基于索引的顺序实现<code>ORDER BY</code>要求返回的排序顺序，如下情况所示：</p>
<ul>
<li>由于二级索引检索后，如果需要非索引上的数据，则需要进行回表操作，引入了额外的访问成本，所以即使在可以使用特定索引避免排序时，优化器如果在会全表扫描+排序的成本比较时，效果较差，不会使用该优化策略。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> t1</span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> key_part1, key_part2;</span><br></pre></td></tr></table></figure>
<p>如果无法使用上述优化测试的情况下，MySQL基于<code>filesort</code>对数据进行排序（涉及到在临时磁盘文件上的外部排序），相关控制参数为</p>
<ul>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_sort_buffer_size"><code>sort_buffer_size</code></a>：排序时可以用到的缓存大小</li>
</ul>
<h5 id="7-GROUP-BY-Optimization"><a href="#7-GROUP-BY-Optimization" class="headerlink" title="7. GROUP BY Optimization"></a>7. GROUP BY Optimization</h5><p>分组操作在没有优化的情况下的执行过程为：扫描整个表，创建一个同组数据相邻的临时表，在此临时表上进行分组聚集操作，其中临时表可能会占用大量的内存，因此提出了基于index的优化手段</p>
<ol>
<li><p>Loose Index Scan：借助索引的顺序，避免临时表的创建，直接在索引上进行<code>Group By</code>操作，必须满足的条件为：</p>
<ul>
<li><p>检索一个表的数据</p>
</li>
<li><p><code>Group by</code> 属性列满足索引最左前缀匹配原则</p>
</li>
<li><p>检所涉及到的值要被索引覆盖，且非<code>Group by</code> 属性列必须为常量</p>
</li>
<li><p>聚集函数只能包括<code>Min()</code>,<code>MAX()</code>，且如果同时应用两个聚集函数，必须指向同一个列</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c1, c2 <span class="keyword">FROM</span> t1 <span class="keyword">WHERE</span> c3 <span class="operator">=</span> const <span class="keyword">GROUP</span> <span class="keyword">BY</span> c1, c2;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Tight Index Scan：放宽了最左前缀原则，<code>Where key = const</code>+  <code>Group key</code> 满足最左前缀匹配即可</p>
<ul>
<li>类似于索引下推，首先使用where条件过滤后，再在剩余数据上进行<code>Group By</code></li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># C2 <span class="operator">=</span> <span class="string">&#x27;a&#x27;</span> 填补了不满足最左前缀匹配</span><br><span class="line"><span class="keyword">SELECT</span> c1, c2, c3 <span class="keyword">FROM</span> t1 <span class="keyword">WHERE</span> c2 <span class="operator">=</span> <span class="string">&#x27;a&#x27;</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> c1, c3;</span><br><span class="line"><span class="keyword">SELECT</span> c1, c2, c3 <span class="keyword">FROM</span> t1 <span class="keyword">WHERE</span> c1 <span class="operator">=</span> <span class="string">&#x27;a&#x27;</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> c2, c3;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>另外<code>DISTINCT</code>由于通常与<code>GROUP BY</code>，其优化手段与<code>GROUP BY</code>相同，无<code>GROUP BY</code>的<code>DISTINCT</code>相当于一个组</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> c1, c2, c3 <span class="keyword">FROM</span> t1</span><br><span class="line"><span class="keyword">WHERE</span> c1 <span class="operator">&gt;</span> const;</span><br></pre></td></tr></table></figure>
<h5 id="8-LIMIT-Query-Optimization"><a href="#8-LIMIT-Query-Optimization" class="headerlink" title="8. LIMIT Query Optimization"></a>8. LIMIT Query Optimization</h5><p>如下SQL语句，在没有优化情况下，访问了200万条数据，却只返回了10条数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> stuent <span class="keyword">where</span> nation <span class="operator">=</span> <span class="string">&#x27;cn&#x27;</span> limit <span class="number">2000000</span>,<span class="number">10</span>;</span><br></pre></td></tr></table></figure>
<p>针对LIMIT查询，MySQL有以下几条优化方案：</p>
<ol>
<li>对于<code>LIMIT</code>限定数据条数较少的情况，查询基于聚簇索引而不是全表扫描</li>
<li>对于需要<code>file sort</code>的<code>LIMIT</code>查询语句，一旦满足<code>LIMIT</code>数量，MySQL不会继续排序</li>
<li>LIMIT 0直接返回空数据集，不会执行</li>
</ol>
<h3 id="库表优化"><a href="#库表优化" class="headerlink" title="库表优化"></a>库表优化</h3><p>库表优化可以分为三种类型：</p>
<ol>
<li>在数据库设计阶段，根据ER建模和范式，设计出合理的数据库表，选择合适的表字段类型</li>
<li>在数据库使用过程中，根据实际需求进行反范式化操作，例如为了方便查询，增加冗余中间表</li>
<li>当表数据量扩张到一定程度，考虑进行表拆分满足性能需求</li>
</ol>
<h4 id="范式化vs反范式化（私货总结）"><a href="#范式化vs反范式化（私货总结）" class="headerlink" title="范式化vs反范式化（私货总结）"></a>范式化vs反范式化（<del>私货总结</del>）</h4><p><strong>范式化</strong>本质上是通过拆分表中具有依赖关系的数据列到不同的表中，从而减少<strong>数据冗余</strong>，降低表空间占用。但是由于查询往往涉及到不同表的关联，一张表的冗余越少，查询涉及到的关联次数越多，导致查询成本较高，因此产生了对应的概念-<strong>反范式化</strong>，通过在表中增加冗余列，减少查询需要的关联次数。</p>
<ul>
<li>在设计时根据范式化规则，确定数据库表</li>
<li>在数据库使用中，根据查询的实际需求，通过反范式化优化数据库查询性能</li>
</ul>
<h4 id="字段类型选择优化"><a href="#字段类型选择优化" class="headerlink" title="字段类型选择优化"></a>字段类型选择优化</h4><p>在选择表字段数据类型时，应选择满足需求的空间占用最小的数据类型，下面列举我了解到的常见的选择策略：</p>
<ol>
<li>对于既能使用文本类型，又能使用数字类型的字段，使用数字类型，降低存储和比较成本<ul>
<li>IP地址以数字形式存储，MySQL提供转化数字和ip地址的方法：<code>inet_aton</code>和<code>inet_ntoa</code></li>
</ul>
</li>
<li>能使用<code>TIMESTAMP</code>，就不使用<code>DATATIME</code>，前者以数字形式存储（4byte），后者以字符串形式存储（8byte）</li>
<li>当存储非负数时，可使用<code>UNSIGNED</code>整数</li>
<li>涉及到浮点数的精确计算时，使用<code>DECIMAL</code>类型</li>
</ol>
<h4 id="表拆分"><a href="#表拆分" class="headerlink" title="表拆分"></a>表拆分</h4><ol>
<li>读写分离</li>
<li>垂直拆分：分库/垂直分表</li>
<li>水平拆分：水平拆分（客户端绑定：Sharding-JDBC，中间件：MyCat）</li>
</ol>
<h3 id="主从复制"><a href="#主从复制" class="headerlink" title="主从复制"></a>主从复制</h3><p>通过主从复制操作实现主服务器提供读写操作、从服务器只读，通过分散读请求到不同服务器，降低主服务器性能瓶颈，从而提升整体性能。</p>
<p>下面使用两台服务器建立一个简单的主从架构，熟悉主从架构构建过程：</p>
<ol>
<li><p>修改配置文件，标识主服务器和从服务器（以及一些其他的配置信息）</p>
<ul>
<li><p>主配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">#[必选]主服务器唯一ID</span><br><span class="line">server-id=1</span><br><span class="line">#[必选] binlog位置</span><br><span class="line">log-bin=/var/lib/mysql/my_binlog</span><br><span class="line">#[可选] 可选的参数</span><br><span class="line">read_only=0</span><br><span class="line"># binlog_expire_logs_seconds</span><br><span class="line"># 指定同步的数据库，默认为所有</span><br><span class="line">binlog-do-db= test_sync_db</span><br><span class="line"># 设置binlog格式</span><br><span class="line">binlog_format=STATEMENT</span><br></pre></td></tr></table></figure>
</li>
<li><p>从配置，server-id不同即可（<del>没有把主服务器IP配置进来有点反直觉</del>）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 从服务器ID</span><br><span class="line">server-id=2</span><br><span class="line">read-only=1</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>主服务器上创建从服务器连接使用的用户，并分配对应权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> create user <span class="string">&#x27;readonly_slave&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified by <span class="string">&#x27;123456&#x27;</span>;</span></span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> grant replication slave on *.* to <span class="string">&#x27;readonly_slave&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span></span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br></pre></td></tr></table></figure>
</li>
<li><p>从服务器配置master信息</p>
<ul>
<li><p>配置IP，同步binlog起始点，连接用户名密码等信息</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">CHANGE MASTER <span class="keyword">TO</span> MASTER_HOST<span class="operator">=</span><span class="string">&#x27;192.168.190.100&#x27;</span>, MASTER_PORT<span class="operator">=</span><span class="number">3306</span>, MASTER_USER<span class="operator">=</span><span class="string">&#x27;readonly_slave&#x27;</span>, MASTER_PASSWORD<span class="operator">=</span><span class="string">&#x27;123456&#x27;</span>, MASTER_LOG_FILE<span class="operator">=</span><span class="string">&#x27;my_binlog.000001&#x27;</span>, MASTER_LOG_POS<span class="operator">=</span><span class="number">701</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>通过<code>show master status</code>展示获取主服务器此时的binlog信息</p>
<img src="/2022/11/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%90%86%E8%AE%BA/Mysql/MySQL%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/image-20221127191630198.png" class="" title="image-20221127191630198">
</li>
</ul>
</li>
<li><p>刷新权限，并重启服务器，即可看到同步成功</p>
<ul>
<li><p>在从服务器上调用<code>show slave status</code>查看是否同步成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show slave status\G</span></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">               Slave_IO_State: Waiting for source to send event</span><br><span class="line">                  Master_Host: 192.168.190.100</span><br><span class="line">                  Master_User: readonly_slave</span><br><span class="line">                  Master_Port: 3306</span><br><span class="line">                Connect_Retry: 60</span><br><span class="line">              Master_Log_File: my_binlog.000002</span><br><span class="line">          Read_Master_Log_Pos: 891</span><br><span class="line">               Relay_Log_File: localhost-relay-bin.000005</span><br><span class="line">                Relay_Log_Pos: 326</span><br><span class="line">        Relay_Master_Log_File: my_binlog.000002</span><br><span class="line">             Slave_IO_Running: Yes</span><br><span class="line">            Slave_SQL_Running: Yes</span><br><span class="line">            	  ...........(省略)</span><br><span class="line">                  Master_UUID: 60f3b167-4f9e-11ed-9e94-000c29502aa1</span><br><span class="line">             Master_Info_File: mysql.slave_master_info</span><br><span class="line">                    SQL_Delay: 0</span><br><span class="line">          SQL_Remaining_Delay: NULL</span><br><span class="line">      Slave_SQL_Running_State: Replica has read all relay log; waiting for more updates</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<p>在配置同步的过程中遇到了不少的问题，下面是简单的记录</p>
<ol>
<li><p>普通密码无法通过<code>caching_sha2_password</code>验证的问题</p>
<ul>
<li><p>错误提示如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show slave status\G</span></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">               Slave_IO_State: Connecting to source</span><br><span class="line">            	  ...........(省略)</span><br><span class="line">                Last_IO_Error: error connecting to master &#x27;readonly_slave@192.168.190.100:3306&#x27; - retry-time: 60 retries: 2 message: Authenon plugin &#x27;caching_sha2_password&#x27; reported error: Authentication requires secure connection.</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>原因</strong>：MySQL8.0 将默认身份验证的插件从<code>mysql_native_password</code>换为了<code>caching_sha2_password</code></p>
</li>
<li><p><strong>解决方案</strong>：修改密码类型</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> alter user <span class="string">&#x27;readonly_slave&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified with sha256_password by <span class="string">&#x27;123456&#x27;</span>;</span></span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>出现问题</p>
<ul>
<li><p>错误提示如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show slave status\G</span></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">               Slave_IO_State: Waiting for source to send event</span><br><span class="line">               ...........(省略)</span><br><span class="line">               Last_Error: Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 faxecuting transaction &#x27;ANONYMOUS&#x27; at master log my_binlog.000002, end_log_pos 891. See error log and/or performance_schema.replication_applitus_by_worker table for more details about this failure or others, if any.</span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> select * from performance_schema.replication_applier_status_by_worker\G</span></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">               ...........(省略)</span><br><span class="line">		LAST_ERROR_MESSAGE: Worker 1 failed executing transaction &#x27;ANONYMOUS&#x27; at master log my_binlog.000002, g_pos 891; Error &#x27;Table &#x27;test_table&#x27; already exists&#x27; on query. Default database: &#x27;test_sync_db&#x27;. Query: &#x27;create table test_table(id int)&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>原因：我之前已经在从库中创建了对应的table，同步执行<code>create table test_table</code>时，创建失败导致无法失败</p>
</li>
<li><p>解决方案</p>
<ul>
<li><p>简单方式：删除从库中的对应表</p>
</li>
<li><p>复杂方式：重新设置同步，将同步binlog位置设置到binlog末尾位置，跳过创建表步骤</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 相关命令</span><br><span class="line">stop slave;</span><br><span class="line">reset slave;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>简单<strong>总结</strong>：</p>
<ol>
<li>主从同步对于主服务器来说相当于增加了一个特殊的客户端（从库），该客户端访问的是binlog</li>
<li>对于<code>statement</code>格式的binlog，主从同步就是简单的SQL重放，因此在从库上执行写入操作会导致主从不一致，但不一定会导致同步失败，只有在重放失败的情况（遇到的问题2）下才会导致同步失败</li>
</ol>
<h4 id="binlog补充"><a href="#binlog补充" class="headerlink" title="binlog补充"></a>binlog补充</h4><p>binlog用来记录数据库中的修改操作，例如修改数据项，创建表等操作，因此并不关心例如<code>select</code>，<code>show</code>等不会修改数据的检索语句，按照记录信息的不同分为：</p>
<ol>
<li><code>--binlog-format=STATEMENT</code>：<em>statement-based logging</em>，基于SQL语句的binary log，主从复制在涉及到非确定性SQL语句（例如：<code>now()</code>，<code>uuid()</code>）时，可能出现数据不一致问题。</li>
<li><code>--binlog-format=ROW</code>（默认）： <em>row-based logging</em>，日志记录每一行数据如何被修改，相较于<code>STATEMENT</code>，粒度更细，规避了不一致的问题，但是会导致日志量较大。</li>
<li><code>--binlog-format=MIXED</code>：混合模式，上述两种模式的组合，当MySQL能够保证操作的确定性时，采用<em>statement-based logging</em>，无法保证时则会采用<em>row-based logging</em>。</li>
</ol>
<p>binlog有两个主要的应用场景：</p>
<ol>
<li>主从复制</li>
<li>数据恢复</li>
</ol>
<p>类似于redolog，binlog产生伴随着事务中的语句执行，不断的写入binlog cache，直到事务提交，binlog是否刷入磁盘由<code>sync_binlog</code>参数控制。</p>
<p>由于redo log控制主服务器的持久性，binlog 控制从服务器与主服务器的同步性，当redo log提交成功，binlog提交失败或者相反时，就会导致主从服务器之间的不一致问题，对此解决方案为-两阶段提交：</p>
<ul>
<li>在事务提交前，首先将redo log写入磁盘，此时为预提交状态</li>
<li>将binlog写入磁盘，写入完成后，提交事务</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>综上所述，MySQL主从同步原理并不复杂，反而是binlog的实现值得深入研究，因为需要保证主从之间的一致性。</p>
]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Raft的ShardedKV数据库实现</title>
    <url>/2022/09/06/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/%E5%9F%BA%E4%BA%8ERaft%E7%9A%84ShardedKV%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h2 id="基于Raft的Sharded-KV-数据库实现"><a href="#基于Raft的Sharded-KV-数据库实现" class="headerlink" title="基于Raft的Sharded KV 数据库实现"></a>基于Raft的Sharded KV 数据库实现</h2><blockquote>
<p>项目来自于<a href="https://pdos.csail.mit.edu/6.824/index.html">MIT6.824分布式系统</a>的结课大作业，实现代码已上传 <a href="https://github.com/fuhaifei/ProjectShardKV">github仓库</a>，该博客为项目的总体框架总结，省略了大量的实现细节和代码，细节总结可参考 <a href="https://fuhaifei.github.io/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/">MIT6.824实验总结</a></p>
</blockquote>
<p>该项目实现目标为一个分布式容错的简单KV数据库，系统主要的功能点可以总结为：</p>
<ol>
<li>提供包括<code>put(key, value), append(key, value), get(key)</code>的基本kv数据库功能</li>
<li>基于Raft共识算法的多服务器备份，实现一致性备份存储，实现了系统容错功能</li>
<li>基于Raft日志的WAL机制以及系统快照机制，允许系统在失效后通过日志重新执行、加载快照等，快速恢复数据</li>
<li>通过数据分片和多复制服务器组存储方式，实现了系统的高并发访问性能</li>
<li>支持存储服务器的动态配置，即可以动态的增加删除存储服务器</li>
</ol>
<span id="more"></span>
<h3 id="基本实现架构"><a href="#基本实现架构" class="headerlink" title="基本实现架构"></a>基本实现架构</h3><p>如下图所示，系统按照标准的CS架构实现，其中Server端包括一个配置管理集群（Shard Manager）以及多个数据片存储管理集群（KV Server Group）；Client端包括两种类型身份的Client：一种为发送KV数据操作请求的客户端（KV Client）,一种为管理分片信息以及数据片存储集群的客户端（Shard Manage Client）</p>
<ul>
<li>Shard Manager Server负责kv server group、数据分片以及分片到kv server映射信息等系统元数据的管理（类似于HDFS Master）</li>
<li>KV Server Group负责按照分片配置存储对应分片数据以及执行和响应KV客户端操作</li>
</ul>
<img src="/2022/09/06/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/%E5%9F%BA%E4%BA%8ERaft%E7%9A%84ShardedKV%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E7%8E%B0/image-20220904152037470.png" class="" title="image-20220905145204954">
<p>其中<strong>Shard Manager</strong>和每个<strong>KV Server Group</strong>，通过多服务器备份的方式实现数据的可靠性，具体实现架构如下图所示（以KV Server Group为例）：</p>
<ul>
<li>每个KV Server Group以及ShardManager内包括三个服务器实例，互为备份服务器</li>
<li>通过基于Raft日志的WAL机制，保证不同副本之间的状态一致性以及错误恢复（KV Server中状态机为存储数据片、Shard Manager中状态机为系统shard元数据）</li>
</ul>
<img src="/2022/09/06/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/%E5%9F%BA%E4%BA%8ERaft%E7%9A%84ShardedKV%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E7%8E%B0/image-20220904161454555.png" class="" title="image-20220905145204954">
<h3 id="分片以及分片分配-sharding"><a href="#分片以及分片分配-sharding" class="headerlink" title="分片以及分片分配(sharding)"></a>分片以及分片分配(sharding)</h3><p>kv数据库中的每个key对相当于关系数据库中表中的条目，且value为单值，区别于关系型数据库条目由多个属性组成，采用Horiziontal Partitioning策略，基于hash的方法对数据进行分片，分片方式如下：</p>
<ul>
<li>shardNum为配置的固定分片数量，确定后即不会改变</li>
</ul>
<script type="math/tex; mode=display">
shardId := hash(key) \% shardNum</script><ul>
<li>确定分片后，根据KV Server Group数量将shard<strong>均匀分配</strong>到KV Server Gruop上，由Shard Manager维护映射关系<script type="math/tex; mode=display">
shadId->GruopId</script></li>
</ul>
<p>上述方法可以总结为：<strong>固定分片策略，动态分配方法</strong>相，优缺点为：</p>
<ul>
<li>优点：当KV Server Group配置改变时，涉及到数据迁移时，以shard为单位进行迁移，较于以key直接映射KV Serve Group(如下公式)，降低了涉及到的数据迁移通信量</li>
<li>缺点：根据key分布进行划分，当key分布不均有或者某些热点key访问量较高时，无法保证不同KV Serve Group之间的负载均衡</li>
</ul>
<script type="math/tex; mode=display">
shardId := hash(key) \% Server Group Num</script><p>未来可以优化的点：</p>
<ul>
<li>采用复合划分Composite partitioning策略即：首先基于哈希方法划分，在根据key的分布规律和请求访问，进行基于列表划分（List Partitionning）的二次细粒度划分</li>
<li>可以基于一致性哈希实现shard-&gt;KV Serve Group的映射管理，降低由于KV Serve Group的增加或者减少shard重分配导致的数据迁移</li>
</ul>
<h4 id="分片分配机制"><a href="#分片分配机制" class="headerlink" title="分片分配机制"></a>分片分配机制</h4><p>Shard Manager作为<script type="math/tex">key->kvServerGroup</script>的配置管理查询服务，支持动态增加/删除存储服务器，相关接口如下</p>
<ul>
<li><code>Join(servers)</code>：批量增加存储服务器组</li>
<li><code>Leave(gruopIds)</code>：批量删除存储服务器组</li>
</ul>
<p>当kvServerGroup配置发生改变时，Shard Manager会重新在剩余可用Gruop进行shard重分配，基本原则如下：</p>
<ul>
<li>保证shard在所有server Group上的<strong>均匀分配</strong>：一部分存储平均数个shard，一部分存储平均数+1个shard</li>
<li>尽量<strong>减少shard迁移</strong>：重新计算平均数，存储shard大于平均数的group移动到小于平均数的group</li>
</ul>
<p>重分配关键代码如下所示：</p>
<ul>
<li>根据group数量，计算平均值<code>averagerShard</code>和余数<code>remindShard</code>，<strong>最终分配结果为</strong>：<code>remindShard</code>个server存储<code>averagerShard + 1</code>个shard，其余group存储<code>averagerShard</code>个shard</li>
<li>分为统计重分配shard和重新分配两个步骤，两步骤思路基本相同</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">//首先统计当前gruop数量</span></span><br><span class="line"><span class="keyword">for</span> key, _ := <span class="keyword">range</span> newConfig.Groups &#123;</span><br><span class="line">	groupShardNumMap[key] = <span class="number">0</span></span><br><span class="line">	allGroupId = <span class="built_in">append</span>(allGroupId, key)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//将shard超过average的日志重新分配</span></span><br><span class="line">reallocateShards := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">0</span>)</span><br><span class="line">averageShard := <span class="built_in">len</span>(newConfig.Shards) / <span class="built_in">len</span>(allGroupId)</span><br><span class="line">remindShard := <span class="built_in">len</span>(newConfig.Shards) % <span class="built_in">len</span>(allGroupId)</span><br><span class="line"><span class="keyword">for</span> shard, group := <span class="keyword">range</span> newConfig.Shards &#123;</span><br><span class="line">	_, ok := newConfig.Groups[group]</span><br><span class="line">	<span class="keyword">if</span> !ok &#123;</span><br><span class="line">		<span class="comment">//shard所属gruop被删除的情况</span></span><br><span class="line">		reallocateShards = <span class="built_in">append</span>(reallocateShards, shard)</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		numShard := groupShardNumMap[group]</span><br><span class="line">		<span class="comment">//如何判断一个group是否超量（关键）：管理shard数量 &gt; averageShard 或者 管理shard数量 == averageShard 且此时可以管理averageShard + 1个shard的机会已经用尽</span></span><br><span class="line">		<span class="keyword">if</span> numShard &gt; averageShard || (numShard == averageShard &amp;&amp; remindShard == <span class="number">0</span>) &#123;</span><br><span class="line">			reallocateShards = <span class="built_in">append</span>(reallocateShards, shard)</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="comment">//如果当前group管理shard数量为averageShard，再分配一个shard，当前group管理了averageShard + 1个shard，则需要占用一个管理averageShard + 1的名额</span></span><br><span class="line">			<span class="keyword">if</span> numShard == averageShard &#123;</span><br><span class="line">				remindShard--</span><br><span class="line">			&#125;</span><br><span class="line">			groupShardNumMap[group] += <span class="number">1</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//开始重新分配</span></span><br><span class="line"><span class="comment">//。。。。。。省略重新分配代码，和上部分差异不大</span></span><br></pre></td></tr></table></figure>
<h3 id="分片迁移实现"><a href="#分片迁移实现" class="headerlink" title="分片迁移实现"></a>分片迁移实现</h3><p>当系统发生Server Gruop的增加或者删除时，会触发shard在不同server之间的变更，虽然整体上思考较为复杂，但是从单个shard迁移的角度考虑，可以将变更过程定义为：多个同时进行的<strong>shard从一个server group 到 另一个server group</strong>的过程，如下图所示：</p>
<ul>
<li>对于每个server group来说，在每个配置变更期，要么有一定量移出的shard，要么有一定量等待移入的shard，要么shard不变</li>
</ul>
<img src="/2022/09/06/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/%E5%9F%BA%E4%BA%8ERaft%E7%9A%84ShardedKV%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E7%8E%B0/image-20220905112323707.png" class="" title="image-20220905145204954">
<p>在shard迁移过程中，我们必须保证一下几点：</p>
<ol>
<li>shard不能丢失：需要迁出shard的server group只有在确保对方成功接收对应shard后，才能安全删除</li>
<li>shard一旦迁出，不能再提供服务：server group在迁出shard后，不能再服务shard上的数据操作（client端可能为获取到最新配置，导致该问题的出现）</li>
<li>在配置切换过程中，系统能够继续提供服务</li>
</ol>
<p>针对上述问题，为shard定义以下几个状态，迁入迁出过程可以通过状态变更实现：</p>
<ul>
<li><code>Normal</code>：默认正常状态（<strong>正常访问操作</strong>）</li>
<li><code>WaitIn</code>：等待迁入状态（无法访问操作）</li>
<li><code>In</code>：已经迁入，但还向发送端确认状态（<strong>正常访问操作</strong>）</li>
<li><code>out</code>：等待迁出状态（无法访问操作）</li>
<li><code>Delelte</code>：迁出完毕状态，可以进行垃圾回收（无法访问操作）</li>
</ul>
<p>如下图以一个shard迁移过程中的状态变更为例，分析变更流程：</p>
<ul>
<li>shard接收端读取到新配置后，创建空shard，并设置状态为 <code>WaitIn</code></li>
<li>不断地向发送端，拉取shard(<strong>由发送端推也一样，只是选择实现了拉</strong>)，不断重试，直到获取到shard，修改状态： <code>WaitIn -&gt; In</code></li>
<li>完成状态转换后，向发动端发送确认收到RPC，发动端修改shard状态： <code>Out -&gt; Delete</code></li>
<li>接收端不断地发送确认收到RPC，直到RPC请求返回，携带有发送端已经将对应shard状态变更为<code>Delete</code>或者删除的信息后，停止发送，修改状态：<code>In -&gt; Normal</code></li>
<li>当server Group的所有shard状态变为<code>Normal</code>或者<code>Delete</code>时，<strong>完成配置变更</strong></li>
<li>上述所有状态变更操作均首先提交到raft，在<strong>操作日志成功提交后</strong>，执行对应状态变更</li>
</ul>
<img src="/2022/09/06/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/%E5%9F%BA%E4%BA%8ERaft%E7%9A%84ShardedKV%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E7%8E%B0/image-20220905145204954.png" class="" title="image-20220905145204954">
<p>上述设计思路的原因：</p>
<ol>
<li><p>为什么需要确认消息，才能将对应shard状态变更<code>Out -&gt; Delete</code></p>
<ul>
<li>由于发送端是被拉取方，在接收端发送确认收到消息后，发送端才能保证shard已经发送到接收端且成功存储可以<strong>删除</strong></li>
</ul>
</li>
<li><p>为什么确认收到消息，返回需要携带发动端是否将对应shard状态变更为delete</p>
<ul>
<li>接受端发送确认消息的目的是为了通知发送端自己确认收到shard，接收端需要<strong>保证</strong>发送端收到并且成功记录的自己的确认消息，当发送端shard状态变为delete时，接收端可以确定自己的确认消息成功执行</li>
<li><p>若不按照上述方式执行，可能存在第一次消息确认成功返回，接收端停止发送，但是发送端由于leader切换等，消息确认操作log未成功提交，导致发送端<strong>无限期等待接收端的确认消息</strong></p>
</li>
<li><p>上述设计来源于假设：即使请求成功返回，对应操作不一定成功执行，只有操作结果出现（raft保证操作结果不丢失），才能保证操作执行</p>
</li>
</ul>
</li>
</ol>
<ul>
<li><strong>简单总结</strong>：发送端需要保证接收端接收到才能删除shard-&gt;接收端需要通知发送端自己收到了-&gt;接收端需要保证发送端知道了自己成功接收，才能停止通知</li>
</ul>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>在具体设计实现过程中，并未采用状态变更与通信绑定的操作，即一个线程执行了状态变更后，进行对应发送请求，具体考虑如下：</p>
<ul>
<li>通信可能失败，需要不断重试，状态变更线程不应等待通信，应该继续执行其他操作</li>
<li>状态变更线程由于互斥需要，往往需要持有锁，由于通信的不确定性（延迟、失败），持有锁时进行RPC通信，可能导致系统性能大幅下降</li>
</ul>
<p>综合考虑上述设计问题，采用了状态变更线程+周期性状态检测线程的思路</p>
<ul>
<li>状态变更线程：负责读取raft日志，根据日志中操作变更shard状态</li>
<li>周期性状态检测线程：周期性遍历shard，根据shard状态按照上述交互图，发送消息</li>
</ul>
<p><strong>状态变更线程</strong>代码如下所示：</p>
<ul>
<li>只负责根据日志操作进行状态变更，不负责状态变更后的操作</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">processConfigOp</span><span class="params">(commandInterface <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">	<span class="keyword">switch</span> opCommand := commandInterface.(<span class="keyword">type</span>) &#123;</span><br><span class="line">	<span class="keyword">case</span> ConfigOp:</span><br><span class="line">		<span class="comment">//过滤重复的修改配置操作（因为从写入日志到日志提交存在时间差，可能重复提交日志）</span></span><br><span class="line">		<span class="keyword">if</span> opCommand.Config.Num &gt; kv.config.Num &#123;</span><br><span class="line">            <span class="comment">//省略根据配置信息修改shard状态代码</span></span><br><span class="line">		&#125;</span><br><span class="line">	<span class="keyword">case</span> InShardOp:</span><br><span class="line">		<span class="keyword">if</span> opCommand.ConfigNum == kv.config.Num &#123;</span><br><span class="line">			<span class="comment">//省略添加shard(去重)</span></span><br><span class="line">		&#125;</span><br><span class="line">	<span class="keyword">case</span> DelShardOp:</span><br><span class="line">		<span class="keyword">if</span> opCommand.ConfigNum == kv.config.Num &#123;</span><br><span class="line">			<span class="comment">//省略修改shard状态为out-&gt;delete(需要去重)</span></span><br><span class="line">		&#125;</span><br><span class="line">	<span class="keyword">case</span> AckShardOp:</span><br><span class="line">		<span class="keyword">if</span> opCommand.ConfigNum == kv.config.Num &#123;</span><br><span class="line">			<span class="comment">//省略从in状态修改为normal状态(需要去重)</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>周期性检测线程</strong>代码如下所示：</p>
<ul>
<li>遍历所有shard，启动单独线程负责通信，主线程等待所有通信线程退出</li>
<li>所有通信线程退出后，主线程遍历所有shard，判断是否退出配置切换状态</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">updateShardState</span><span class="params">(updateFunc <span class="keyword">func</span>(<span class="keyword">int</span>, <span class="keyword">int</span>, []<span class="keyword">string</span>)</span>, <span class="title">chaeckStatus</span> <span class="title">string</span>)</span> &#123;</span><br><span class="line">	kv.mu.RLock()</span><br><span class="line">	_, isLeader := kv.rf.GetState()</span><br><span class="line">	<span class="comment">//如果在配置</span></span><br><span class="line">	<span class="keyword">if</span> kv.isConfiging() &amp;&amp; isLeader &#123;</span><br><span class="line">		<span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line">        <span class="comment">//遍历所有shard，根据状态执行对应操作（如：WaitIn状态发起拉取RPC请求，In状态发起确认RPC）</span></span><br><span class="line">		<span class="keyword">for</span> shardId, shard := <span class="keyword">range</span> kv.allShards &#123;</span><br><span class="line">			<span class="keyword">if</span> shard.ShardStatus == chaeckStatus &#123;</span><br><span class="line">				wg.Add(<span class="number">1</span>)</span><br><span class="line">				<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(shardId <span class="keyword">int</span>, configNum <span class="keyword">int</span>, allServers []<span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">					<span class="keyword">defer</span> wg.Done()</span><br><span class="line">					updateFunc(shardId, configNum, allServers)</span><br><span class="line">				&#125;(shardId, kv.config.Num, kv.preConfig.Groups[kv.preConfig.Shards[shardId]])</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//释放锁，并等待</span></span><br><span class="line">		kv.mu.RUnlock()</span><br><span class="line">		wg.Wait()</span><br><span class="line">        <span class="comment">//上锁，判断当前状态是否可以退出配置状态</span></span><br><span class="line">		kv.mu.RLock()</span><br><span class="line">		completeFlag := <span class="literal">true</span></span><br><span class="line">		<span class="keyword">for</span> _, shard := <span class="keyword">range</span> kv.allShards &#123;</span><br><span class="line">			<span class="keyword">if</span> shard.ShardStatus != ShardNormal &amp;&amp; shard.ShardStatus != ShardDelete &#123;</span><br><span class="line">				completeFlag = <span class="literal">false</span></span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		kv.mu.RUnlock()</span><br><span class="line">		<span class="keyword">if</span> completeFlag &#123;</span><br><span class="line">			kv.changeConfigState(<span class="literal">false</span>)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		kv.mu.RUnlock()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于后台存在多个周期性运行函数（状态检测、垃圾回收），抽取一个<strong>公用的周期循环</strong>方法：</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">backRoutine</span><span class="params">(operation <span class="keyword">func</span>()</span>, <span class="title">interval</span> <span class="title">int</span>)</span> &#123;</span><br><span class="line">	<span class="keyword">for</span> !kv.killed() &#123;</span><br><span class="line">		<span class="comment">//执行具体操作</span></span><br><span class="line">		operation()</span><br><span class="line">		time.Sleep(time.Duration(interval) * time.Millisecond)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//传入需要周期运行的方法</span></span><br><span class="line"><span class="keyword">go</span> kv.backRoutine(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123; kv.updateShardState(kv.callMigrateShard, ShardWaitIn) &#125;, UpdateShardInterval)</span><br><span class="line"><span class="comment">//启动确认收到对应shard的线程</span></span><br><span class="line"><span class="keyword">go</span> kv.backRoutine(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123; kv.updateShardState(kv.callMigrateShardAck, ShardIn) &#125;, UpdateShardInterval)</span><br></pre></td></tr></table></figure>
<h4 id="垃圾回收实现"><a href="#垃圾回收实现" class="headerlink" title="垃圾回收实现"></a>垃圾回收实现</h4><p>根据分片迁移实现部分逻辑，仅仅需要回收状态为delete状态的shard，实现逻辑较为简单，采用周期性回收线程的方式，关键代码如下：</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">garbageCollect</span><span class="params">()</span></span> &#123;</span><br><span class="line">	kv.mu.Lock()</span><br><span class="line">	<span class="keyword">for</span> shardId, shard := <span class="keyword">range</span> kv.allShards &#123;</span><br><span class="line">		<span class="keyword">if</span> shard.ShardStatus == ShardDelete &#123;</span><br><span class="line">			<span class="comment">//删除对应状态的shard</span></span><br><span class="line">			<span class="built_in">delete</span>(kv.allShards, shardId)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	kv.mu.Unlock()</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//启动垃圾回收线程</span></span><br><span class="line"><span class="keyword">go</span> kv.backRoutine(kv.garbageCollect, GCInterval)</span><br></pre></td></tr></table></figure>
<h3 id="遇到的实现问题"><a href="#遇到的实现问题" class="headerlink" title="遇到的实现问题"></a>遇到的实现问题</h3><h5 id="1-同一个类型中的RPC请求-结构体中，由于条件不同请求参数不同，导致大量无用参数"><a href="#1-同一个类型中的RPC请求-结构体中，由于条件不同请求参数不同，导致大量无用参数" class="headerlink" title="1.同一个类型中的RPC请求/结构体中，由于条件不同请求参数不同，导致大量无用参数"></a>1.同一个类型中的RPC请求/结构体中，由于条件不同请求参数不同，导致大量无用参数</h5><p>在实现过程中遇到了一个操作请求对应多种不同操作的情况，不同操作需要携带不同的操作参数，如下代码所示：</p>
<ul>
<li>一种操作对应四种类型的操作，传输其他操作时需要占用其他三种操作参数的空间</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> ShardOp <span class="keyword">struct</span> &#123;</span><br><span class="line">	<span class="comment">//公共参数</span></span><br><span class="line">	OpType    <span class="keyword">string</span></span><br><span class="line">	ConfigNum <span class="keyword">int</span></span><br><span class="line">	<span class="comment">//修改配置操作参数</span></span><br><span class="line">	Config shardctrler.Config</span><br><span class="line">	<span class="comment">//添加shard操作参数</span></span><br><span class="line">	AddShard <span class="keyword">int</span></span><br><span class="line">	Shard    ShardData</span><br><span class="line">	<span class="comment">//删除操作参数</span></span><br><span class="line">	DelShard <span class="keyword">int</span></span><br><span class="line">	<span class="comment">//确认shard操作参数</span></span><br><span class="line">	AckShard <span class="keyword">int</span> <span class="comment">//迁移进入shard的参数</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>针对以上情况，想出了三种解决方案</p>
<ol>
<li><p>发送不特殊处理，接收端根据opType进行处理（不做处理），缺点是：多余参数占用空间</p>
</li>
<li><p>修改结构体，使用byte[]存储编码后的参数，发送端编码，接收端根据OpType进行解码，缺点：编解码浪费时间，发送接收端需要确定编码顺序</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> ShardOp <span class="keyword">struct</span> &#123;</span><br><span class="line">	<span class="comment">//公共参数</span></span><br><span class="line">	OpType    <span class="keyword">string</span></span><br><span class="line">	ConfigNum <span class="keyword">int</span></span><br><span class="line">	<span class="comment">//修改配置操作参数</span></span><br><span class="line">	parameters []<span class="keyword">byte</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将结构体拆分，传输不同的结构体，在接收端基于golang反射进行操作，缺点：反射的运行效率较低，影响系统运行效率</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> DelShardOp <span class="keyword">struct</span> &#123;</span><br><span class="line">	ConfigNum <span class="keyword">int</span> <span class="comment">//非切换配置需要使用的去重参数</span></span><br><span class="line">	DelShard  <span class="keyword">int</span> <span class="comment">//迁移出删除shard的参数</span></span><br><span class="line"></span><br><span class="line">	AddShard <span class="keyword">int</span>       <span class="comment">//迁移进入shard的参数</span></span><br><span class="line">	Shard    ShardData <span class="comment">//迁移进入shard的参数</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">type</span> InShardOp <span class="keyword">struct</span> &#123;</span><br><span class="line">	ConfigNum <span class="keyword">int</span>       <span class="comment">//非切换配置需要使用的去重参数</span></span><br><span class="line">	AddShard  <span class="keyword">int</span>       <span class="comment">//迁移进入shard的参数</span></span><br><span class="line">	Shard     ShardData <span class="comment">//迁移进入shard的参数</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">type</span> AckShardOp <span class="keyword">struct</span> &#123;</span><br><span class="line">	ConfigNum <span class="keyword">int</span> <span class="comment">//非切换配置需要使用的去重参数</span></span><br><span class="line">	AckShard  <span class="keyword">int</span> <span class="comment">//迁移进入shard的参数</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//接收端执行操作</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">processConfigOp</span><span class="params">(commandInterface <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">	<span class="keyword">switch</span> opCommand := commandInterface.(<span class="keyword">type</span>) &#123;</span><br><span class="line">	<span class="keyword">case</span> ConfigOp:</span><br><span class="line">	<span class="keyword">case</span> InShardOp:</span><br><span class="line">	<span class="keyword">case</span> DelShardOp:</span><br><span class="line">	<span class="keyword">case</span> AckShardOp:</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>最后综合考虑<strong>采用第三种方法</strong>，虽然执行效率低，但是实现逻辑上更加清晰，相较于第一种方式减少了空间浪费，降低了网络通信代价</p>
<h3 id="测试与总结"><a href="#测试与总结" class="headerlink" title="测试与总结"></a>测试与总结</h3><p>测试过程按照以下方式进行：</p>
<ul>
<li><p>执行测试脚本，测试200次，每次输出结果写入到文件中</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for ((i = 0; i &lt; 200; i++)); do echo $i; (go test) &gt; ./res/$i; grep -nr &quot;FAIL.*&quot; res;  done</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行完毕，统计通过数量</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line">grep -nr <span class="string">&quot;PASS&quot;</span> res |wc -l</span><br></pre></td></tr></table></figure>
</li>
<li><p>重复执行三轮，共计测试600次</p>
</li>
</ul>
<p>测试结果为：</p>
<ul>
<li>测试所有轮次均通过</li>
</ul>
<img src="/2022/09/06/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/%E5%9F%BA%E4%BA%8ERaft%E7%9A%84ShardedKV%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E7%8E%B0/image-20220905161251235.png" class="" title="image-20220905161251235">
<ul>
<li>其中一次的测试输出为：</li>
</ul>
<img src="/2022/09/06/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/%E5%9F%BA%E4%BA%8ERaft%E7%9A%84ShardedKV%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E7%8E%B0/image-20220905161743796.png" class="" title="image-20220905161743796">
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>终于经过了一个多月的视频学习和接近一个月的实验实现，终于完成MIT6.824的学习，现在回看自己的收获可以总结为以下几点：</p>
<ol>
<li>对于分布式系统概念以及涉及到的知识点，有了广泛但不一定深入的了解</li>
<li>掌握了基本golang开发和调试的能力，对于golang的特性和语法有了一定程度的理解</li>
<li>对于并发编程，RPC通信，线程和进程有了更深的理解</li>
</ol>
]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>MIT6.824</tag>
      </tags>
  </entry>
  <entry>
    <title>RPC框架整体理解</title>
    <url>/2023/07/11/%E6%8A%80%E6%9C%AF%E6%A1%86%E6%9E%B6/RPC%E7%9B%B8%E5%85%B3/RPC%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h1 id="RPC框架整体理解"><a href="#RPC框架整体理解" class="headerlink" title="RPC框架整体理解"></a>RPC框架整体理解</h1><blockquote>
<p>In <a href="https://en.wikipedia.org/wiki/Distributed_computing">distributed computing</a>, a <strong>remote procedure call</strong> (<strong>RPC</strong>) is when a computer program causes a procedure (subroutine) to execute in a different <a href="https://en.wikipedia.org/wiki/Address_space">address space</a> (commonly on another computer on a shared network), which is written as if it were a normal (local) procedure call, without the programmer explicitly writing the details for the remote interaction. </p>
<p>— wikipedia</p>
</blockquote>
<p>RPC（remote procedure call）远程过程调用，通过封装底层网络通信细节，向跨网络的服务提供如同本地调用一般的远程调用接口</p>
<ul>
<li>RPC来源于微服务架构的发展，越来越多业务系统被拆解为多个相互协作的微服务，借助RPC工具能够方便实现不同服务之间相互调用。</li>
<li>常见的RPC框架有：Dubbo、Thrift、GRPC、Spring Cloud（部分）</li>
</ul>
<span id="more"></span>
<p>根据RPC在微服务架构中所处的位置和作用，容易知道如果要实现一个RPC框架所需要解决的问题包括：</p>
<ol>
<li>如何使得远程调用用起来像“本地调用”？<strong>动态代理</strong></li>
<li>如何找到被调用的服务在哪里？ —— <strong>服务注册与发现</strong></li>
<li>微服务高级功能如何支持？—— <strong>负载均衡/流量控制/调用链路监控</strong></li>
<li>内存数据如何转化为网络传输的二进制数据? —— <strong>序列化方法</strong></li>
<li>调用双方如何通信？—— <strong>通信协议设计</strong></li>
<li>底层通信逻辑如何设计？ —— <strong>网络IO设计</strong></li>
</ol>
<p>综上所属，一个典型的RPC框架的架构，如下图所示：</p>
<ul>
<li>接入层：通过动态代理将本地调用转化为远程调用，基于过滤器链进行调用</li>
<li>服务治理层：负责包括注册发现，负载均衡，路由/鉴权等RPC服务治理功能，或者说辅助rpc功能</li>
<li>协议层：主要负责实现从内存对象到网络传输数据的相互转换，并按照约定的通信协议进行封装/拆封</li>
<li>传输层：基于特定IO模型和底层通信协议实现调用数据的传输。</li>
</ul>

<p>一个RPC框架的主要业务逻辑包括：</p>
<ol>
<li>服务注册和上线</li>
<li>RPC调用执行</li>
</ol>
<h2 id="常用RPC框架"><a href="#常用RPC框架" class="headerlink" title="常用RPC框架"></a>常用RPC框架</h2><p>常用的RPC框架包括Dubbo、Thrift、GRPC、Spring Cloud等，下面简单总结本人对于不同RPC框架的学习</p>
<h3 id="Thrift"><a href="#Thrift" class="headerlink" title="Thrift"></a>Thrift</h3><blockquote>
<p>The Apache Thrift software framework, for scalable cross-language services development, combines a software stack with a code generation engine to build services that work efficiently</p>
<p>本部分的大部分内容来自：<a href="https://diwakergupta.github.io/thrift-missing-guide/">Thrift: The Missing Guide</a></p>
</blockquote>
<p>Thrift 作为Apache旗下的顶级项目，具备支持多种语言，多种消息格式，同步异步通信等特点，其基本架构为：</p>
<ul>
<li>Server层：以Client-Server模式定义RPC通信逻辑，支持单/多线程，阻塞非阻塞通信模式</li>
<li>Processor层：封装从输入流读取/写入输出流的操作，是协议和流之间的转化层。</li>
<li>Protocol层：定义传输协议（内存对象到传输数据的转化）+ 序列化，其中序列化方式包括Binary 协议/Compact 协议/json</li>
<li>Transport层：对网络读写提供了一个简单的抽象接口，不负责消息的序列化/反序列化等操作(TCP/HTTP)。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-------------------------------------------+</span><br><span class="line">| Server                                    |</span><br><span class="line">| (single-threaded, event-driven etc)       |</span><br><span class="line">+-------------------------------------------+</span><br><span class="line">| Processor                                 |</span><br><span class="line">| (compiler generated)                      |</span><br><span class="line">+-------------------------------------------+</span><br><span class="line">| Protocol                                  |</span><br><span class="line">| (JSON, compact etc)                       |</span><br><span class="line">+-------------------------------------------+</span><br><span class="line">| Transport                                 |</span><br><span class="line">| (raw TCP, HTTP etc)                       |</span><br><span class="line">+-------------------------------------------+</span><br></pre></td></tr></table></figure>
<p>Thrift的使用也比较简单，参考官方文档即可。</p>
<h3 id="gRPC"><a href="#gRPC" class="headerlink" title="gRPC"></a>gRPC</h3><p>gRPC 一开始由 google 开发，是一款语言中立、平台中立、开源的远程过程调用(RPC)系统</p>
<ul>
<li>基于Protocol Buffer协议实现序列化功能</li>
<li>底层通信基于HTTP2.0实现</li>
</ul>
<p>其他基本和thrift大差不差，不再赘述，使用也比较简单，参考<a href="https://grpc.io/docs/languages/java/quickstart/">官方文档</a>即可</p>
<h3 id="Dubbo"><a href="#Dubbo" class="headerlink" title="Dubbo"></a>Dubbo</h3><p>Apache Dubbo 是一款 RPC 服务开发框架，用于解决微服务架构下的服务治理与通信问题，相较于上述两个RPC框架，Dubbo最大的区别在于其涵盖了更多的服务治理功能：</p>
<ul>
<li><strong>服务开发框架</strong> ：定义了一套微服务定义，服务间调用模型，服务发现，负载均衡策略，流量路由和管控的范式，其中一部分功能由Dubbo自身实现（服务定义，调用模型，负载均衡），另一部分功能可与其他组件配合实现（地址发现，链路追踪，认证鉴权等）。</li>
<li><strong>RPC 通信协议实现</strong> ：Dubbo 从设计上不绑定任何一款特定通信协议，HTTP/2、REST、gRPC、JsonRPC、Thrift、Hessian2 等几乎所有主流的通信协议。</li>
</ul>
<p>与上述两个rpc框架的区别在于，Dubbo整体上是一个微服务治理框架，rpc只是微服务治理中的一个重要问题。</p>
<h2 id="实现一个RPC框架？"><a href="#实现一个RPC框架？" class="headerlink" title="实现一个RPC框架？"></a>实现一个RPC框架？</h2><p>根据上文中对于RPC架构的总结，我们可以类比计网中OSI七层协议的学习思路，自底向上分析每一层会遇到的问题以及如何实现。</p>
<h3 id="网络通信模型"><a href="#网络通信模型" class="headerlink" title="网络通信模型"></a>网络通信模型</h3><p>RPC网络通信实际上就是Cilent与Server之间的网络IO，《UNIX 网络编程卷 I》中根据同/异步，阻塞/非阻塞定义了五种IO通信模型：</p>
<ol>
<li>阻塞IO模型：发起IO端（客户端/应用程序）在发出IO请求后，阻塞等待当前请求返回后，再向下执行</li>
<li>非阻塞IO：发起IO端（客户端/应用程序）在发出IO请求后，继续执行，在合适的时机询问IO是否执行完毕。</li>
<li>异步IO：发起IO端（客户端/应用程序）在发出IO请求后，等待IO结束，通知自己数据准备完毕。</li>
<li>IO复用：select/poll/epoll。</li>
<li>信号驱动IO：数据在内核中准备完毕后，以事件的形式通知用户程序</li>
</ol>
<p>Java 在上述网络模型的基础上定义了自身的IO模型：</p>
<ol>
<li>BIO(Blocking IO)，同步阻塞IO，每当服务器接收到一个客户端连接请求，创建新线程处理</li>
<li>NIO(Non Blocking IO)，同步非阻塞IO，底层基于LInux内核函数的多路复用函数(select，poll，epoll)</li>
<li>AIO(Asynchronous IO)，异步非阻塞IO</li>
</ol>
<p>另外Netty基于 <a href="http://hxz.ink/2021/09/11/reactor-pattern/">Reactor模型 </a>实现了更高性能的网络通信，是目前常用的网络IO框架</p>
<h4 id="Thrift实现"><a href="#Thrift实现" class="headerlink" title="Thrift实现"></a>Thrift实现</h4><p>Thrift基于java原声socket模型实现了底层的io通信机制，服务端包含四种通信模式的server:</p>
<ul>
<li>TSimpleServer：单线程服务器端，阻塞IO</li>
<li>TThreadPoolServer：多线程服务器端，阻塞IO<ul>
<li>持有一个线程池，scoket接收到连接后交给线程池处理</li>
</ul>
</li>
<li>TNonblockingServer：单线程服务器端，使用非阻塞式I/O<ul>
<li>底层基于java NIO实现，单线程Reactor模型，处理连接/读取/写入等均由单一线程执行</li>
</ul>
</li>
<li>THsHaServer：半同步半异步服务器端，基于非阻塞式IO读写和多线程工作任务处理<ul>
<li>在TNonblockingServer的基础上增加了线程池处理具体的任务逻辑，主线程处理连接建立/读写请求，即多线程Reactor模型</li>
</ul>
</li>
<li>TThreadedSelectorServer：多线程选择器服务器端，对THsHaServer在异步IO模型上进行增强<ul>
<li>将主线程的任务进一步拆分，主Reactor线程负责建立连接等，从Reactor线程负责读写，其余交由线程池处理，即主从Reactor模型</li>
</ul>
</li>
</ul>
<p>客户端支持两种通信模式的server：</p>
<ul>
<li>TSocket：阻塞IO</li>
<li>TNonblockingSocket：非阻塞IO</li>
</ul>
<p>另外 TFramedTransport 包装类支持数据以帧进行传输，具体传输IO模式依赖内部包装的类，如Tsocket。</p>
<h4 id="gRPC实现"><a href="#gRPC实现" class="headerlink" title="gRPC实现"></a>gRPC实现</h4><p>gRPC基于基于Netty4.1 的 HTTP/2 协议栈框架构建，在以往传统的RPC调用方式上，额外支持了基于HTTP/2.0的stream调用方式。其中服务端Server 基于Reactor模型实现：</p>
<ul>
<li>主线程监听指定的 port，来等待Client连接请求, 分给 worker 线程池处理. </li>
<li>HTTP/2请求消息的请求和响应发送都由Netty负责(NioEventLoop)</li>
<li>gRPC 负责消息的序列化和反序列化、以及应用服务接口的调用</li>
</ul>
<p>客户端线程模型分为：</p>
<ol>
<li>同步阻塞服务调用：普通的请求响应模型</li>
<li>同步非阻塞服务调用：基于Future机制实现</li>
<li>异步非阻塞调用：基于回调函数</li>
<li>基于HTTP/2.0的stream调用（协议层详细了解）</li>
</ol>
<h4 id="Dubbo中的实现"><a href="#Dubbo中的实现" class="headerlink" title="Dubbo中的实现"></a>Dubbo中的实现</h4><blockquote>
<p>内容来自，我只是简化总结强化记忆：<a href="https://cn.dubbo.apache.org/zh-cn/overview/mannual/java-sdk/advanced-features-and-usage/performance/threading-model/provider/">Dubbo服务端线程模型</a></p>
</blockquote>
<p>Dubbo 底层 IO 框架基于Netty实现，其中服务端根据  通信的不同阶段是否在IO线程执行 分为了五种通信线程模型：</p>
<ol>
<li>All：在 IO 线程（Netty handler主线程）上执行sent/序列化response 操作，在Dubbo线程池中执行其他操作和反序列化</li>
<li>Direct：所有操作均在IO线程执行</li>
<li>Execution：IO 线程执行 sent/connected/disconnected/caught和序列化 response 操作，Dubbo 线程池中执行r eceived 和反序列化request</li>
<li>Message Only</li>
<li>Connection Ordered</li>
</ol>
<p>上述线程模型在配置文件中配置，Dubbo通过SPI机制实现动态设置。</p>
<p>客户端线程模型为：</p>
<ol>
<li>业务线程发出请求，拿到一个 Future 实例。</li>
<li>在调用 future.get() 之前，先调用 ThreadlessExecutor.wait()，wait 会使业务线程在一个阻塞队列上等待，直到队列中被加入元素。</li>
<li>当业务数据返回后，生成一个 Runnable Task 并放入 ThreadlessExecutor 队列</li>
<li>业务线程将 Task 取出并在本线程中执行：反序列化业务数据并 set 到 Future。</li>
<li>业务线程拿到结果直接返回</li>
</ol>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>结合上文对于不同框架中网络IO模型的分析，我们很容易得到如下结论：</p>
<ol>
<li>服务端IO基于Reactor模型实现，结合线程池实现高性能网络IO，常用Netty实现底层IO</li>
<li>客户端往往支持阻塞/非阻塞/异步三种通信模式</li>
</ol>
<p>不同RPC框架的底层IO原理没有很大的区别，区别在于多线程和线程池的使用。</p>
<h3 id="通信协议"><a href="#通信协议" class="headerlink" title="通信协议"></a>通信协议</h3><p>在进行通信前通信双方必须事先约定好通信的方式，才能理解从网络上传送来的二进制数据，这就是传输协议的作用，RPC框架位于应用层，需要基于网络层的TCP/UDP协议实现通信，因此在通信时会遇到一下通信单位与应用层传输单位的不对应问题。</p>
<p>例如：一次RPC调用对应 TCP流的一段数据 或 UDP的一到多个数据包，如何确定数据的终点（TCP拆包问题），不同调用之间的分隔（TCP粘包问题），所以通信协议实际上用来解决以下问题：</p>
<ul>
<li>确定一次RPC通信传输数据长度和边界。</li>
<li>确定通信传输数据的存储和解析方式。</li>
</ul>
<p>参考IP数据报的设计方式，我们不难想到通信协议包括。</p>
<ul>
<li>协议头：整体长度，数据长度，以及其他如版本，消息ID等的控制信息。</li>
<li>协议体：携带的数据本身。</li>
</ul>
<h4 id="TCP拆包粘包"><a href="#TCP拆包粘包" class="headerlink" title="TCP拆包粘包"></a>TCP拆包粘包</h4><p>TCP是面向流，没有边界，而操作系统在发送TCP数据时，会通过缓冲区来进行优化，例如缓冲区为1024个字节大小</p>
<ul>
<li>粘包：如果一次请求发送的数据量比较小，没达到缓冲区大小，TCP则会将多个请求合并为同一个请求进行发送</li>
<li>拆包：如果一次请求发送的数据量比较大，超过了缓冲区大小，TCP就会将其拆分为多次发送，这就是拆包</li>
</ul>
<p>常见的解决方法包括：</p>
<ul>
<li>以固定长度形式传输。每次通信传输固定长度的数据，不足则补0</li>
</ul>
<ul>
<li>发送端在每个包的末尾使用固定的分隔符。<strong>FTP命令</strong>的一般报文格式是：<code>命令 选项参数 \r\n</code>，<strong>FTP应答</strong>的一般报文格式为：<code>状态码 报文选项 \r\n</code></li>
<li>将消息分为头部和消息体，头部中保存整个消息的长度，只有读取到足够长度的消息之后才算是读到了一个完整的消息；</li>
<li>通过自定义协议进行粘包和拆包的处理。</li>
</ul>
<h4 id="现有RPC框架中的通信协议"><a href="#现有RPC框架中的通信协议" class="headerlink" title="现有RPC框架中的通信协议"></a>现有RPC框架中的通信协议</h4><p><strong>Thrift</strong></p>
<blockquote>
<p>内容来自 <a href="https://github.com/apache/thrift/blob/master/doc/specs/thrift-binary-protocol.md">Thrift Binary protocol encoding</a>等文档</p>
</blockquote>
<p>thrift支持多种通信协议，包括：</p>
<ul>
<li><code>TBinaryProtocol</code>：二进制协议</li>
<li><code>TCompactProtocl</code>：带压缩的二进制协议</li>
</ul>
<p>整体协议分为head和body两部分，具体格式如下：</p>

<p>其中head包括：</p>
<ol>
<li>magic(4byte)：包含版本号，消息类型等，严格模式首位取1，非严格模式首位取0。消息类型包括：<ul>
<li>CALL = 1 调用消息，如<code>0x80010001</code></li>
<li>REPLY = 2 应答消息，如<code>0x80010002</code></li>
<li>EXCEPTION = 3 异常消息，如<code>0x80010003</code></li>
<li>ONEWAY = 4 单向消息，属于调用消息，但是不需要应答，如<code>0x80010004</code></li>
</ul>
</li>
<li>mehod name length(4byte)：调用方法名长度</li>
<li>mehod name (Nbyte)：调用方法名</li>
<li>seqid(4byte)：序列号</li>
</ol>
<p>Body对应方法参数/返回值，对应一个<code>struct</code>类型，<code>struct</code>针对常用数据类型分别定义序列化方式，如string类型占4+N字节，格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--------------------</span><br><span class="line">| size |  content  |</span><br><span class="line">|   4  |     N     |</span><br><span class="line">--------------------</span><br></pre></td></tr></table></figure>
<p><strong>grpc</strong></p>
<p>gropc基于http2.0协议实现，http2.0通过引入二进制分帧在兼容http1.x的基础上实现了性能提升，其新特性包括：</p>
<ol>
<li>二进制分帧。在应用层和网络层之间添加了二进制分帧操作，对于http1.x报文划分为头部帧和数据帧分别发送</li>
<li>首部压缩+首部缓存。通信双方缓存首部帧，传输时只需要发送变化的头部和数据帧即可。另外基于HPACK算法（传输索引+存储参数表）对首部进行压缩。</li>
<li>多路复用：引入“流”概念将一个逻辑上的http2请求拆分为多个流复用，提供了通过单一的http/2 连接发起多重的请求的能力。</li>
<li>请求优先级。分帧后可根据帧携带数据内容调整传输顺序提升通信效率，每个流都可以带有一个31比特的优先值，根据流优先级进行帧传输</li>
<li>服务端推送。服务器可以对一个客户端请求发送多个响应，服务器向客户端推送资源无需客户端明确地请求。</li>
</ol>
<p>其余内容参考<a href="https://zhuanlan.zhihu.com/p/161577635">gRPC系列(三) 如何借助HTTP2实现传输</a>，<del>我写也是cv不写了</del>，其中不同数据存储在不同帧：</p>
<ul>
<li>请求的Method在header中传递</li>
<li>参数用DATA帧</li>
<li>返回状态用HEADER帧</li>
<li>返回数据用DATA帧</li>
</ul>
<p>通信过程：client发送header帧携带method信息，server返回header帧后，通过data帧交换数据</p>
<p><strong>Dubbo</strong></p>
<p>Dubbo除去提供Triple，Dubbo2两种通信协议外，同时支持任意第三方通信协议，如官方支持的 gRPC、Thrift、REST、JsonRPC、Hessian2 等。</p>
<ul>
<li>Triple协议同时支持基于http1（unary），http2（stream）的请求协议，其中http2在实现上与标准gRPC协议基本一致。</li>
<li>Dubbo2协议类似于常规的通信协议，如下图所示，基本属性包括版本号（magic number），请求响应表示（res/requst），序列化标识（serilization ID），响应状态（Status id），请求ID（request id），Variable Part（请求中携带方法名，服务名，参数列表等，响应携带返回值和异常）</li>
</ul>
<p><img src="https://cn.dubbo.apache.org/imgs/dev/dubbo_protocol_header.png" alt="/dev-guide/images/dubbo_protocol_header.jpg"></p>
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p>从上面对现有协议的总结，我们能够大体知道目前rpc协议的主要方式包括：</p>
<ol>
<li>基于TCP自定义通信协议，一般结构为：协议头+数据</li>
<li>基于http/http2协议实现通信，方法名/服务名等数据存放在请求头，参数/返回等数据存放在请求体</li>
</ol>
<p>自定义数据格式通常由一下字段组成：</p>
<ol>
<li>魔术位：表明这是什么协议，协议的版本等，为了兼容不同通信协议</li>
<li>长度信息：无论是整体/数据长度，解决粘包拆包问题</li>
<li>消息ID，消息类型：定位此次消息，以及消息是什么类型（响应/请求）</li>
<li>序列化方式：标识携带数据序列化方式，兼容不同的序列化方式。</li>
</ol>
<h3 id="内存对象-gt-网络传输"><a href="#内存对象-gt-网络传输" class="headerlink" title="内存对象-&gt;网络传输"></a>内存对象-&gt;网络传输</h3><p>在进行网络传输前，由于调用参数/返回参数均为内存中的对象，需要转化为可在网络上传输数据形式后才能按照协议封装传输，序列化方法解决了内存到传输数据形式的转化，目前主要的序列化方法包括：</p>
<ol>
<li>JDK原生</li>
<li>JSON：将对象的属性名-属性值以kv形式存储为json文件形式<ul>
<li>以纯文本json文件形式传输，文本形式空间开销较大，不适合大量rpc调用情况.</li>
<li>JSON 没有类型，但像 Java 这种强类型语言，需要通过反射统一解决。</li>
</ul>
</li>
<li>Thrift：上文中简单提及了Thrift序列化方式，与Hessian属于一流派</li>
<li>Hessian</li>
<li>Kryo</li>
<li>Protobuf</li>
</ol>
<h4 id="JDK原生"><a href="#JDK原生" class="headerlink" title="JDK原生"></a>JDK原生</h4><p>JDK原生提供了java对象的序列化，通过<code>ObejctOutputStream</code> 和 <code>ObejctInputStream</code>提供的方法接口可以序列化/反序列化实现了<code>Serializable</code>的接口，主要的注意事项有：</p>
<ol>
<li>一旦变量被transient修饰，变量将不再是对象持久化的一部分。</li>
<li>只会记录第一次序列化的编号，不会重复序列化，这会导致最新变量变更不会体现在序列化文件中。</li>
<li>实现 <code>Externalizable</code> 接口可自定义序列化和反序列化方法。</li>
</ol>
<p>其序列化格式为类似于上文中的协议格式。</p>
<p>JDK原生序列化协议存在空间利用效率较低，无法跨平台使用等问题</p>
<h4 id="Hessian"><a href="#Hessian" class="headerlink" title="Hessian"></a>Hessian</h4><blockquote>
<p>来自<a href="https://cn.dubbo.apache.org/zh-cn/overview/mannual/java-sdk/reference-manual/serialization/hessian/">Dubbo Hessian介绍</a></p>
</blockquote>
<p>Hessian is a dynamically-typed, binary serialization and Web Services protocol designed for object-oriented transmission.</p>
<ol>
<li>自描述序列化类型。不依赖外部描述文件或者接口定义，将所有类字段信息都放入序列化字节数组中，直接利用字节数组进行反序列化</li>
<li>把复杂对象的所有属性存储在一个Map中进行序列化。所以在父类、子类存在同名成员变量的情况下，Hessian序列化时，先序列化子类，然后序列化父类，因此反序列化结果会导致子类同名成员变量被父类的值覆盖</li>
<li>兼容字段增、减，序列化和反序列化；不支持一部分java类型序列化：Linked 系列，LinkedHashMap、LinkedHashSet 等；Locale 类，可以通过扩展 ContextSerializerFactory 类修复Byte/Short 反序列化的时候变成 Integer。</li>
</ol>
<h4 id="Kryo"><a href="#Kryo" class="headerlink" title="Kryo"></a>Kryo</h4><p>Kryo 是一个快速高效的 Java 二进制对象图序列化框架，该项目的目标是高速、小尺寸和易于使用的 API。</p>
<ul>
<li>使用变长的int和long保证这种基本数据类型序列化后尽量小</li>
<li>Kryo对Class的序列化只需要化Class的全路径名，在反序列化时根据Class通过类加载进行加载</li>
<li>不是线程安全的，要通过ThreadLocal或者创建Kryo线程池来保证线程安全</li>
<li>不需要实现Serializable接口</li>
<li>字段增、减，序列化和反序列化时无法兼容</li>
<li>必须拥有无参构造函数</li>
</ul>
<h4 id="Protobuf"><a href="#Protobuf" class="headerlink" title="Protobuf"></a>Protobuf</h4><p>Protocol Buffers 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。目前提供了 C++、Java、Python 三种语言的 API。</p>
<ul>
<li>语言无关、平台无关。即 ProtoBuf 支持 Java、C++、Python 等多种语言，支持多个平台</li>
<li>高效。即比 XML 更小（3 ~ 10倍）、更快（20 ~ 100倍）、更为简单</li>
<li>扩展性、兼容性好。你可以更新数据结构，而不影响和破坏原有的旧程序</li>
</ul>
<p>使用Protobuf协议需要先定义IDL（Interface description language），根据IDL内容生成对应的序列化反序列化工具</p>
<h4 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h4><p>该部分没有深入的去学具体的序列化协议内部原理，只是简单了解了主要的几种序列化方法（偷了一部分网上的八股内容）。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Kryo</td>
<td>速度快，序列化后体积小</td>
<td>跨语言支持较复杂</td>
</tr>
<tr>
<td>Hessian</td>
<td>默认支持跨语言</td>
<td>较慢</td>
</tr>
<tr>
<td>Protostuff</td>
<td>速度快，基于protobuf</td>
<td>需静态编译</td>
</tr>
<tr>
<td>Protostuff-Runtime</td>
<td>无需静态编译，但序列化前需预先传入schema</td>
<td>不支持无默认构造函数的类，反序列化时需用户自己初始化序列化后的对象，其只负责将该对象进行赋值</td>
</tr>
<tr>
<td>Java</td>
<td>使用方便，可序列化所有类</td>
<td>速度慢，占空间</td>
</tr>
</tbody>
</table>
</div>
<h3 id="如何找到服务？"><a href="#如何找到服务？" class="headerlink" title="如何找到服务？"></a>如何找到服务？</h3><p>为了解决服务之间的可见性，rpc框架往往需要基于第三方组件提供服务的注册发现功能，目前了解过的服务注册中心有：</p>
<ol>
<li>ZooKeeper：服务注册为Zookeeper中的ZNode，并给予Watcher机制实现服务发现。<ul>
<li><a href="https://juejin.cn/post/6864384493374242830">理解Zookeeper的Watch机制</a></li>
<li>另外zookeeper基于强一致性的zab协议，保证CP性质</li>
</ul>
</li>
<li>Nacos：阿里提供的开源服务注册发现组件，提供了基于Http和gRPC机制服务注册发现接口<ul>
<li>Nacos将服务分为持久服务和临时服务，临时服务基于gRPC连接发送心跳信息保活，持久服务由注册中心发送心跳保活</li>
<li>调用方订阅服务时会在服务本地和注册中心分别维护一个订阅列表，订阅信息的维护分为推逻辑和拉逻辑<ul>
<li>服务端推送逻辑：当订阅服务发生变更（上下线/元信息变更）是，注册中心会主动向订阅客户端推送变更服务信息（只告诉客户端变了，让他自己拉取）</li>
<li>客户端拉取逻辑：客户端会周期性向服务端请求最新服务信息，若服务发生变更，则拉取变更服务信息</li>
</ul>
</li>
<li>nacos同时提供保证强一致性Raft协议实现AP，和弱一致性的Distro实现CP。对于持久化实例，采用raft保证AP。对于配置中心和服务注册发现中的临时实例场景，采用DIstro协议实现CP</li>
</ul>
</li>
</ol>
<h3 id="保证可靠的调用？"><a href="#保证可靠的调用？" class="headerlink" title="保证可靠的调用？"></a><strong>保证</strong>可靠的调用？</h3><h4 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h4><p>常见的负载均衡策略（Niginx）包括：</p>
<ol>
<li>轮询（round robin）：按照顺序逐个调用服务节点</li>
<li>加权平滑轮询：指定权重轮询，每次调用后按照一定规则修改权重值，避免所有请求打在权重高的服务节点上<ul>
<li>每个节点初始化一个当前值和权重值</li>
<li>每次选择当前值+权重值最大的节点，选择该节点后，修改该节点当前值=当前值-总权重</li>
<li>重复这个过程，能够保证不会一直选择权重最大的节点</li>
</ul>
</li>
<li>ip_hash：根据ip hash值分配到对应服务器，解决session不共享问题</li>
</ol>
<p>Dubbo中提供的负载均衡策略包括：</p>
<ul>
<li>加权随机（Weighted Random）</li>
<li>加权轮询（round robin）：借鉴自niginx</li>
<li>最少活跃优先（LeastActive）：活跃数越低，越优先调用，相同活跃数的进行加权随机。活跃数=请求发送数 - 响应返回数</li>
<li>最短响应优先：在最近一个滑动窗口中，响应时间越短（时间窗口内的平均数），越优先调用。相同响应时间的进行加权随机。</li>
<li>一致性哈希：相同参数的请求总是发到同一提供者（特殊需求）</li>
</ul>
<p>从上述负载均衡算法我们容易看到，负载均衡的目标实际上就是将调用按照处理能力均匀的分配到对应服务节点上，分为两种派别：</p>
<ol>
<li>无权/固定加权 轮询/随机：随机/轮询不一定很好，但是一定不会太差</li>
<li>根据参数动态确定权重：根据服务节点相关指标计算权重<ul>
<li>调用方：请求-响应次数，响应时间</li>
<li>被调用方发送给调用方：cpu负载，内存占用，CPU核数、内存大小、请求处理的耗时指标（如请求平均耗时、TP99、TP999）、服务节点的状态指标（如正常、亚健康）等指标，例如<a href="https://cn.dubbo.apache.org/zh-cn/overview/reference/proposals/heuristic-flow-control/#adaptive%E7%AE%97%E6%B3%95">Dubbo对应负载策略</a></li>
</ul>
</li>
</ol>
<h4 id="调用异常处理"><a href="#调用异常处理" class="headerlink" title="调用异常处理"></a>调用异常处理</h4><p>RPC调用由于需要经过网络传输，相较于普通本地调用会出现由网络导致的异常等问题，这些也是RPC框架需要解决的问题，下面研究一下Dubbo中的异常定义和处理方法，以对RPC调用的异常处理有更加清晰的认知。</p>
<p><strong>Dubbo异常处理</strong></p>
<p>Dubbo将RPC过程的异常定义为RPCException，其枚举状态包括:网络相关异常/业务异常/权限异常/路由异常等</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> UNKNOWN_EXCEPTION = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> NETWORK_EXCEPTION = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TIMEOUT_EXCEPTION = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> BIZ_EXCEPTION = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> FORBIDDEN_EXCEPTION = <span class="number">4</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SERIALIZATION_EXCEPTION = <span class="number">5</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> NO_INVOKER_AVAILABLE_AFTER_FILTER = <span class="number">6</span>;</span><br><span class="line">。。。。省略</span><br></pre></td></tr></table></figure>
<p>Dubbo在<strong>provider端</strong>定义了ExceptionFilter处理服务端方法抛出的异常（<a href="https://github.com/apache/dubbo/blob/e026b5bb596bbc48fa89bbf756cba504d52bed1d/dubbo-rpc/dubbo-rpc-api/src/main/java/org/apache/dubbo/rpc/filter/ExceptionFilter.java#L50">代码</a>），其处理逻辑为：</p>
<ul>
<li>对于客户端可识别的异常直接抛出。什么是可识别异常，即Dubbo认为客户端知道的异常，包括RuntimeException，方法名上声明的异常，JDK本身异常，异常和api定义在一个jar包的异常，dubbo异常（RPCException）</li>
<li>对于客户端不可识别的异常，包装为RuntimeException包装返回给客户端。<ul>
<li>可参考<a href="https://segmentfault.com/a/1190000040337469">Dubbo异常处理源码探究及其最佳实践</a></li>
</ul>
</li>
</ul>
<p>客户端根据不同类型的异常进行不同处理：</p>
<ol>
<li><p>服务端异常：在根据返回结果中的异常类型进行异常重放。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">recreate</span><span class="params">()</span> <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (exception != <span class="keyword">null</span>) &#123;</span><br><span class="line">          <span class="comment">// fix issue#619</span></span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">              Object stackTrace = exception.getStackTrace();</span><br><span class="line">              <span class="keyword">if</span> (stackTrace == <span class="keyword">null</span>) &#123;</span><br><span class="line">                  exception.setStackTrace(<span class="keyword">new</span> StackTraceElement[<span class="number">0</span>]);</span><br><span class="line">              &#125;</span><br><span class="line">          &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">              <span class="comment">// ignore</span></span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">if</span> ((exception <span class="keyword">instanceof</span> RpcException) &amp;&amp; !(exception <span class="keyword">instanceof</span> com.alibaba.dubbo.rpc.RpcException)) &#123;</span><br><span class="line">              com.alibaba.dubbo.rpc.RpcException recreated =</span><br><span class="line">                  <span class="keyword">new</span> com.alibaba.dubbo.rpc.RpcException(((RpcException) exception).getCode(),</span><br><span class="line">                      exception.getMessage(), exception.getCause());</span><br><span class="line">              recreated.setStackTrace(exception.getStackTrace());</span><br><span class="line">              <span class="keyword">throw</span> recreated;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">throw</span> exception;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>客户端调用异常：在invoke方法中处理（这里展示的默认invoker）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="comment">//**远程调用代码</span></span><br><span class="line">&#125; <span class="keyword">catch</span> (TimeoutException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RpcException(RpcException.TIMEOUT_EXCEPTION, <span class="string">&quot;Invoke remote method timeout. method: &quot;</span> + RpcUtils.getMethodName(invocation) + <span class="string">&quot;, provider: &quot;</span> + getUrl() + <span class="string">&quot;, cause: &quot;</span> + e.getMessage(), e);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (RemotingException e) &#123;</span><br><span class="line">            String remoteExpMsg = <span class="string">&quot;Failed to invoke remote method: &quot;</span> + RpcUtils.getMethodName(invocation) + <span class="string">&quot;, provider: &quot;</span> + getUrl() + <span class="string">&quot;, cause: &quot;</span> + e.getMessage();</span><br><span class="line">            <span class="keyword">if</span> (e.getCause() <span class="keyword">instanceof</span> IOException &amp;&amp; e.getCause().getCause() <span class="keyword">instanceof</span> SerializationException) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> RpcException(RpcException.SERIALIZATION_EXCEPTION, remoteExpMsg, e);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> RpcException(RpcException.NETWORK_EXCEPTION, remoteExpMsg, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>Dubbo 服务在尝试调用一次之后，如出现非业务异常(服务突然不可用、超时等)，Dubbo 默认会进行额外的最多2次重试，其实现位于<code>FailbackClusterInvoker.java</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> Result <span class="title">doInvoke</span><span class="params">(Invocation invocation, List&lt;Invoker&lt;T&gt;&gt; invokers, LoadBalance loadbalance)</span> <span class="keyword">throws</span> RpcException </span>&#123;</span><br><span class="line">    Invoker&lt;T&gt; invoker = <span class="keyword">null</span>;</span><br><span class="line">    URL consumerUrl = RpcContext.getServiceContext().getConsumerUrl();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        invoker = select(loadbalance, invocation, invokers, <span class="keyword">null</span>);</span><br><span class="line">        <span class="comment">// Asynchronous call method must be used here, because failback will retry in the background.</span></span><br><span class="line">        <span class="comment">// Then the serviceContext will be cleared after the call is completed.</span></span><br><span class="line">        <span class="keyword">return</span> invokeWithContextAsync(invoker, invocation, consumerUrl);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">        logger.error(CLUSTER_FAILED_INVOKE_SERVICE,<span class="string">&quot;Failback to invoke method and start to retries&quot;</span>,</span><br><span class="line">            <span class="string">&quot;&quot;</span>,<span class="string">&quot;Failback to invoke method &quot;</span> + RpcUtils.getMethodName(invocation) +</span><br><span class="line">                <span class="string">&quot;, wait for retry in background. Ignored exception: &quot;</span></span><br><span class="line">            + e.getMessage() + <span class="string">&quot;, &quot;</span>,e);</span><br><span class="line">        <span class="keyword">if</span> (retries &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            addFailed(loadbalance, invocation, invokers, invoker, consumerUrl);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> AsyncRpcResult.newDefaultAsyncResult(<span class="keyword">null</span>, <span class="keyword">null</span>, invocation); <span class="comment">// ignore</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="降级-熔断-限流"><a href="#降级-熔断-限流" class="headerlink" title="降级/熔断/限流"></a>降级/熔断/限流</h4><blockquote>
<p> 降级是指当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心业务正常运作或高效运作。说白了，就是尽可能的把系统资源让给优先级高的服务。</p>
<p> 熔断是指在固定时间窗口内，接口调用超时比率达到一个阈值，会开启熔断。进入熔断状态后，后续对该服务接口的调用不再经过网络，直接执行本地的默认方法，达到服务降级的效果。</p>
<p> <a href="https://zhuanlan.zhihu.com/p/419102097">服务降级和服务熔断万字讲解，从0到1，边学边实战</a></p>
</blockquote>
<p>当服务面临段时间大量请求调用系统资源耗尽无法有效响应请求时，服务需要采取一定的机制保证自身的高可用性，降级和熔断是从两个不同角度保护服务的机制：</p>
<ol>
<li>降级（服务方）：当我自身的服务能力跟不上请求来的速度，我是不是应该“降低我服务标准/数量”</li>
<li>熔断（请求方）：如果我调用的服务响应很慢/总是报错等，我是不是应该停一会再调用</li>
<li>限流：限制单位时间调用服务的请求数量，可以理解为降级的一种方法</li>
</ol>
<p>博客介绍的比较全面，不再赘述，简单总结一下主要的限流算法</p>
<ol>
<li>令牌桶<ul>
<li>算法描述：为系统会以一个恒定的速度往桶里放入令牌，而如果请求需要被处理，则需要先从桶里获取一个令牌，当桶里没有令牌可取时，则拒绝服务。</li>
<li>算法特点：流量均匀，桶中令牌能够一定程度上应对突发流量</li>
</ul>
</li>
<li>漏桶<ul>
<li>算法描述：水（对应请求）从进水口进入到漏桶里，漏桶以一定的速度出水（请求放行），当水流入速度过大，桶内的总水量大于桶容量会直接溢出，请求被拒绝</li>
<li>算法特点：流量均匀，但是无法应对突发流量</li>
</ul>
</li>
<li>固定/滑动窗口</li>
</ol>
<p>具体实现有：</p>
<ol>
<li><a href="https://www.wdbyte.com/java/rate-limiter/">代码实现</a></li>
<li><a href="https://juejin.cn/post/6889285100128305160">使用Guava RateLimiter限流入门到深入</a></li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://diwakergupta.github.io/thrift-missing-guide/">Thrift: The Missing Guide</a></p>
<p><a href="https://grpc.io/docs/languages/java/quickstart/">gRPC官方文档</a></p>
<p><a href="jianshu.com/p/d0a3611819b1">gRPC基本原理</a></p>
<p><a href="http://hxz.ink/2021/09/11/reactor-pattern/">Reactor 模型｜为啥 Redis 单线程模型也能效率这么高？ </a></p>
<p><a href="https://juejin.cn/post/6968641908314751012"> Apache thrift 之网络模型</a></p>
<p><a href="https://wikimore.github.io/2016/04/04/thrift-protocol/">Thrift协议介绍</a></p>
<p><a href="面试题：聊聊TCP的粘包、拆包以及解决方案">面试题：聊聊TCP的粘包、拆包以及解决方案</a></p>
<p><a href="https://juejin.cn/post/6844903984524705800">深入理解http2.0协议，看这篇就够了！</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/161577635">gRPC系列(三) 如何借助HTTP2实现传输</a></p>
]]></content>
      <categories>
        <category>技术框架</category>
        <category>RPC</category>
      </categories>
  </entry>
  <entry>
    <title>MIT6.824实验总结</title>
    <url>/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>MIT6.824中共设计了四个实验，主要内容是一个分布式kv数据库。</p>
<ol>
<li>mapreduce分布式实现</li>
<li>raft实现</li>
<li>基于raft的kv数据库实现</li>
<li>Sharded KV数据库实现</li>
</ol>
<span id="more"></span>
<h2 id="Lab1-MapReduce分布式实现"><a href="#Lab1-MapReduce分布式实现" class="headerlink" title="Lab1:MapReduce分布式实现"></a>Lab1:MapReduce分布式实现</h2><p>实现架构按照论文中描述的实现方式：</p>
<ol>
<li>coordinator（master）：一个协作服务器，负责任务的分配+任务运行状态的监控</li>
<li>worker：多个worker，负责map和reduce任务的执行</li>
</ol>
<img src="/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/image-20220801085644561.png" class="" title="image-20220801085644561">
<p>整个系统的实现逻辑为（worker主动发送请求，coordinator被动响应请求）：</p>
<ol>
<li>coordinator启动，初始化任务信息；同时多个worker启动，开始向coordinator发送rpc请求，请求分配任务</li>
<li>coordinator根据“FIFO”原则，将map和任务分配给请求地worker，并记录任务状态和分配worker</li>
<li>coordinator每收到一个worker完成任务的rpc请求，修改待完成任务数量，当map任务完成进入reduce阶段，当reduce任务完成，结束执行</li>
</ol>
<p>coordinator主要实现逻辑即为map和reduce任务的分配，以map任务分配的<strong>关键代码</strong>为例：</p>
<ol>
<li><p>首先判断是否存在未分配的map任务，若存在则进行任务分配</p>
</li>
<li><p>传入workerId，分配任务（记录workerId，修改任务数量，修改任务状态）</p>
</li>
<li><p>启动10秒的监控线程，休眠十秒后，如果此时任务状态不为已完成，认为worker执行任务出现问题，将任务状态重新修改为空闲待处理</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> c.numMap != <span class="number">0</span> &#123;</span><br><span class="line">	allocateNumber, allocateFile := c.AllocateMapJob(args.WorkerId)</span><br><span class="line">	<span class="keyword">if</span> allocateNumber != <span class="number">-1</span> &#123;</span><br><span class="line">		reply.JobType = JOBTYPEMAP</span><br><span class="line">		reply.FileList = []<span class="keyword">string</span>&#123;allocateFile&#125;</span><br><span class="line">		reply.JobNumber = allocateNumber</span><br><span class="line">		<span class="comment">//启动线程，10秒种后若结果没有返回,将任务重置为未分配状态</span></span><br><span class="line">		<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">			time.Sleep(<span class="number">10</span> * time.Second)</span><br><span class="line">			<span class="keyword">if</span> c.mapJobStatus[allocateFile][<span class="number">1</span>] != JOBCOMPLETED &#123;</span><br><span class="line">				c.mapJobLocks[allocateFile].Lock()</span><br><span class="line">				<span class="keyword">defer</span> c.mapJobLocks[allocateFile].Unlock()</span><br><span class="line">				<span class="keyword">if</span> c.mapJobStatus[allocateFile][<span class="number">1</span>] != JOBCOMPLETED &#123;</span><br><span class="line">					c.mapJobStatus[allocateFile][<span class="number">1</span>] = JOBIDLE</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;()</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>worker的主要实现逻辑为不断地向coordinator申请任务，<strong>关键代码</strong>为：</p>
<ol>
<li><p>启动时首先向coordinator发送注册rpc请求，获得workerid</p>
</li>
<li><p>循环发送申请任务请求，终止条件为：收到任务结束标志或rpc请求失败</p>
<ul>
<li>根据获得任务类型（map/reduce），调用对应处理方法，返回处理结果</li>
<li>当返回失败时（执行超时/rpc失败），删除任务输出结果</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">func Worker(mapf func(string, string) []KeyValue,</span><br><span class="line">	reducef func(string, []string) string) &#123;</span><br><span class="line">	ok, workerId, reduceNumber := ReigsiterWorker()</span><br><span class="line">	if ok &#123;</span><br><span class="line">		for taskEndFlag := false; !taskEndFlag; &#123;</span><br><span class="line">			ok, workInfo := CallForJob(workerId)</span><br><span class="line">			if ok &#123;</span><br><span class="line">				if workInfo.IsOver &#123;</span><br><span class="line">					taskEndFlag = true</span><br><span class="line">				&#125; else &#123;</span><br><span class="line">					switch workInfo.JobType &#123;</span><br><span class="line">					case JOBTYPEMAP:</span><br><span class="line">						ok, intermediate_file_names := doMapWork(workInfo.JobNumber, workInfo.FileList[0], reduceNumber, workerId, mapf)</span><br><span class="line">						if ok &#123;</span><br><span class="line">							ok = CallForMapJobAccomplished(workInfo.FileList[0], workerId, intermediate_file_names)</span><br><span class="line">						&#125;</span><br><span class="line">						if !ok &#123;</span><br><span class="line">							RemoveAllFiles(intermediate_file_names)</span><br><span class="line">						&#125;</span><br><span class="line">					case JOBTYPEREDUCE:</span><br><span class="line">					//省略...................</span><br><span class="line">					default:</span><br><span class="line">						log.Printf(&quot;none job recieved, waiting to next call\n&quot;)</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125; else &#123;</span><br><span class="line">				log.Printf(&quot;failed to request a task, retry 2 seconds later&quot;)</span><br><span class="line">			&#125;</span><br><span class="line">			time.Sleep(2 * time.Second)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>测试用例完全通过</strong></p>
<img src="/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/image-20220801100606914.png" class="" title="image-20220801100606914">
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Lab1实现还是比较简单的，主要涉及的技术包括：</p>
<ol>
<li>通过锁控制共享变量的访问</li>
<li>通过gorutine实现多线程并发编程</li>
<li>通过RPC进行进程间相互通信</li>
</ol>
<h2 id="Lab2-Raft共识算法实现"><a href="#Lab2-Raft共识算法实现" class="headerlink" title="Lab2:Raft共识算法实现"></a>Lab2:Raft共识算法实现</h2><p>实验二主要任务是实现一个不包括成员切换功能的Raft共识算法，主要实现的功能部分如下</p>
<ol>
<li>Leader Election(选主)：实现raft算法的选主功能</li>
<li>Log Replication(日志复制)：实现日志添加和多副本备份功能</li>
<li>Persistence(持久化)：按照raft论文中持久化要求，实现对应参数的持久化</li>
<li>Snapshot/Log Compaction（快照）：raft层实现日志压缩，从而实现上层应用的快照需求</li>
</ol>
<h3 id="功能实现"><a href="#功能实现" class="headerlink" title="功能实现"></a>功能实现</h3><p>上述四个功能的实现主要参照论文中的figure2以及课程的相关资料，还有一部分存在疑问的地方也参考了其他人的实现思路，涉及到的参考均在文章末尾列出。</p>
<h4 id="选主"><a href="#选主" class="headerlink" title="选主"></a>选主</h4><p>选主首先要解决是raft结点的状态变更问题（即<strong>何时进入选举</strong>），按照论文中的思路如下（<strong>关键代码为ticker()函数</strong>）：</p>
<ul>
<li><p>为避免选主的争抢问题，随机设置超时时间为250-400ms</p>
</li>
<li><p>采用sleep的方式实现超时检测，而不是timer+事件处理的方式</p>
</li>
<li>更新超时时间的时机为收到有效的“RPC”消息：<ol>
<li>为某个 <strong>RequestVoteRPC</strong>(投票请求) 投出一票</li>
<li>收到 <strong>AppendEntryRPC</strong>(添加日志请求) 且该rpc是有效的</li>
<li>收到主节点的 <strong>InstallSnapshotRPC</strong>(更新快照请求)</li>
</ol>
</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">ticker</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">for</span> !rf.killed() &#123;</span><br><span class="line">		time.Sleep(time.Duration(rf.election_timeout-time.Now().UnixMilli()) * time.Millisecond)</span><br><span class="line">		rf.mu.Lock()</span><br><span class="line">		<span class="keyword">if</span> !rf.killed() &amp;&amp; rf.peer_status != STATUS_LEADER &amp;&amp; rf.election_timeout&lt;=time.Now().UnixMilli() &#123;</span><br><span class="line">			<span class="keyword">if</span> rf.election_timeout &lt;= time.Now().UnixMilli() &#123;</span><br><span class="line">                <span class="comment">//省略：修改状态，发起选举</span></span><br><span class="line">			&#125;</span><br><span class="line">            <span class="comment">//省略。。。。。。。</span></span><br><span class="line">		&#125;</span><br><span class="line">		rf.mu.Unlock()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第二个要解决的是leader端的选举判断逻辑（即<strong>如何进行选举</strong>），基本思路如下（<strong>关键代码为startElection()函数</strong>）：</p>
<ol>
<li>主进程为每个其他的raft结点启动一个发送线程，发送请求投票请求</li>
<li>主进程启动完毕之后，等待到“条件成熟”(主线程使用自旋锁，不断判断)，判断是否成功选为leader</li>
<li>选为leader执行初始化操作，否则进入下一轮选举</li>
</ol>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">startElection</span><span class="params">(election_timeout <span class="keyword">int64</span>)</span></span> &#123;</span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="built_in">len</span>(rf.peers); i++ &#123;</span><br><span class="line">		<span class="keyword">if</span> i != rf.me &#123;</span><br><span class="line">			<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(aimServer <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">				<span class="comment">//省略：发送投票请求，判断是否同意票</span></span><br><span class="line">			&#125;(i)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//主线程不断判断是否满足条件（自旋锁）</span></span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">        <span class="comment">//所有发送线程退出/投票数达到要求</span></span><br><span class="line">		<span class="keyword">if</span> <span class="keyword">int</span>(finished_number) == <span class="built_in">len</span>(rf.peers) || <span class="keyword">int</span>(vote_number) &gt; <span class="built_in">len</span>(rf.peers)/<span class="number">2</span> &#123;</span><br><span class="line">			<span class="keyword">break</span></span><br><span class="line">		&#125;</span><br><span class="line">		rf.mu.Lock()</span><br><span class="line">		<span class="comment">//超时或者状态变更</span></span><br><span class="line">		<span class="keyword">if</span> time.Now().UnixMilli() &gt;= election_timeout || rf.currentTerm != vote_requst.CandidateTerm &#123;</span><br><span class="line">			rf.mu.Unlock()</span><br><span class="line">			<span class="keyword">break</span></span><br><span class="line">		&#125;</span><br><span class="line">		rf.mu.Unlock()</span><br><span class="line">		<span class="comment">//休息10毫秒</span></span><br><span class="line">		time.Sleep(time.Millisecond * <span class="number">10</span>)</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//省略：判断是否能够成为leader，能够成为即转化状态，否则退出，等待下一轮选举</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h5><p>主要涉及到的思路包括以下四点：</p>
<ol>
<li>超时状态转换：采用sleep()+election_timeout的机制，不断有”事件“更新election_timeout时间点，检测线程不断的休眠到这个时间点，直到某次“起晚了”</li>
<li>选举判断：采用<strong>主线程判断+多个从线程（对应结点）发送</strong>的模式，主线程自旋等待条件满足，从线程执行完发送判断即退出</li>
<li>维持选主状态：通过定时发送heartbeat消息实现（与下部分重叠，在下部分阐述）</li>
<li>接收端判断投票逻辑：完全按照论文中实现</li>
</ol>
<h4 id="日志复制"><a href="#日志复制" class="headerlink" title="日志复制"></a>日志复制</h4><h5 id="日志发送"><a href="#日志发送" class="headerlink" title="日志发送"></a>日志发送</h5><p>日志复制主要为leader结点的日志发送+follower结点的日志接收，实现之前我想到了两种思路：</p>
<ol>
<li>leader为每个follower设置一个发送线程</li>
<li>leader采用广播形式采用单个发送线程同时向多个follower发送</li>
</ol>
<p>逻辑上日志是采用广播形式，即leader每次发送日志会发送到所有的follower结点，另外heatbeart在此逻辑下同样是采用广播形式，然而论文中的如下描述与广播的逻辑相悖（单个结点发送失败不需要重新给其他结点法）</p>
<blockquote>
<p>If followers crash or run slowly, or if network packets are lost, the leader retries Append- Entries RPCs indefinitely (even after it has responded to the client) until all followers eventually store all log entries.</p>
<p>当followers失效或者运行缓慢，导致发送失败，leader应该无限重试直到所有follower存储所有日志</p>
</blockquote>
<p>仔细思考所谓的”retries Append- Entries RPCs indefinitely“会发现存在以下问题：</p>
<ol>
<li>若在重试过程中leader需要发送另外一个日志，重试是否应该携带新日志，或者停止重试，重新发送，如果不停止一个，就会出现<strong>新旧消息同时发送</strong>的情况，然而如何停止无法实现</li>
<li>无限的重试导致heartbeat需要针对每个结点单独判断，单独发送</li>
</ol>
<p>经过以上思考，最终确定采用广播的形式实现日志发送和heartbeat，基本思路如下：</p>
<ol>
<li>统一发送日志和heartbeat使用一个广播接口，发送日志调用广播接口，heartbeat周期性调用广播接口</li>
<li>广播时每个follower根据nextIndex发送需要的log(不断地广播相当于实现了<strong>无限重试</strong>，只是将重试的逻辑转移到了下一次广播)</li>
<li>由于heartbeat的周期性发送，即使没有外部日志发送请求，其效果也相当于”无限重试”的效果</li>
</ol>
<p>广播关键代码如下（<strong>broadcastAppendEntry()</strong>）：</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">broadcastAppendEntry</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="comment">//省略：判断状态函数</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="built_in">len</span>(rf.peers); i++ &#123;</span><br><span class="line">		<span class="keyword">if</span> i != rf.me &#123;</span><br><span class="line">			rf.mu.Lock()</span><br><span class="line">			<span class="comment">//判断perlogIndex是否已经存储在日志中（小于伪头entryindex）</span></span><br><span class="line">			<span class="keyword">if</span> rf.nextIndex[i]<span class="number">-1</span> &lt; rf.log[<span class="number">0</span>].Index &#123;</span><br><span class="line">				<span class="comment">//省略：发送快照方法</span></span><br><span class="line">			&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">//根据结点缺失的日志情况，发送日志</span></span><br><span class="line">				args := AppendEntryArgs&#123;LeaderId: rf.me, Entries: rf.log[rf.nextIndex[i]-rf.log[<span class="number">0</span>].Index:], Term: rf.currentTerm,</span><br><span class="line">					PreLogIndex: rf.nextIndex[i] - <span class="number">1</span>, PreLogTerm: rf.log[rf.nextIndex[i]-rf.log[<span class="number">0</span>].Index<span class="number">-1</span>].Term, LeaderCommit: rf.commitIndex + rf.log[<span class="number">0</span>].Index&#125;</span><br><span class="line">				reply := AppendEntryReply&#123;&#125;</span><br><span class="line">				<span class="keyword">go</span> rf.sendAppendEntry(i, &amp;args, &amp;reply)</span><br><span class="line">			&#125;</span><br><span class="line">			rf.mu.Unlock()</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//更新发送时间</span></span><br><span class="line">	rf.lastSendTime = time.Now().UnixMilli()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>调用broadcastAppendEntry的时机有以下两种：</p>
<ul>
<li><p>每当leader接收到一个添加日志请求时，调用broadcastAppendEntry() </p>
</li>
<li><p>heartbeat采用类似election_timout的机制实现周期性调用broadcastAppendEntry() </p>
</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">Start</span><span class="params">(command <span class="keyword">interface</span>&#123;&#125;)</span> <span class="params">(<span class="keyword">int</span>, <span class="keyword">int</span>, <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line">	<span class="comment">//省略。。。。。</span></span><br><span class="line">	<span class="keyword">if</span> rf.peer_status == STATUS_LEADER &#123;</span><br><span class="line">	<span class="comment">//省略。。。。。</span></span><br><span class="line">		<span class="keyword">go</span> rf.broadcastAppendEntry(<span class="literal">false</span>)</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//省略。。。。。</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">heartsbeats</span><span class="params">()</span></span> &#123;</span><br><span class="line">	rf.lastSendTime = time.Now().UnixMilli() - IDLE_INTERVAL_TIME</span><br><span class="line">	<span class="keyword">for</span> !rf.killed() &#123;</span><br><span class="line">		<span class="comment">//省略验证状态代码</span></span><br><span class="line">		<span class="keyword">if</span> time.Now().UnixMilli()-rf.lastSendTime &gt;= IDLE_INTERVAL_TIME &#123;</span><br><span class="line">			rf.broadcastAppendEntry()</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//休眠到超时（未触发，休息到触发，否则休息一个interval）</span></span><br><span class="line">		time.Sleep(time.Duration(math.Min(IDLE_INTERVAL_TIME, <span class="keyword">float64</span>(IDLE_INTERVAL_TIME+rf.lastSendTime-time.Now().UnixMilli()))) * time.Millisecond)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="日志提交"><a href="#日志提交" class="headerlink" title="日志提交"></a>日志提交</h5><p>日志提交逻辑按照论文中的逻辑，其中实现思路为：</p>
<ol>
<li>提交日志到应用：每个raft结点启动时，启动applyEntry线程，等待lastapplied &lt; commitIndex，进行提交</li>
<li>推进commitIndex：当leader选举成功时，leader启动checkCommit线程，不断推进commitIndex</li>
</ol>
<ul>
<li>两个同步方式均采用golang的条件变量：rf.applyCond 和 rf.leaderCond</li>
</ul>
<p>以<strong>关键代码如下</strong>为例：</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1.启动raft进程时，启动applyEntry（）进程</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Make</span><span class="params">(peers []*labrpc.ClientEnd, me <span class="keyword">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	persister *Persister, applyCh <span class="keyword">chan</span> ApplyMsg)</span> *<span class="title">Raft</span></span> &#123;</span><br><span class="line">	<span class="comment">//省略。。。。。。。。。。。。。。。。。。。。。</span></span><br><span class="line">	<span class="keyword">go</span> rf.checkCommit()</span><br><span class="line">	<span class="keyword">go</span> rf.applyEntry()</span><br><span class="line">	<span class="comment">//省略。。。。。。。。。。。。。。。。。。。。。</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//2.applyEntry等待条件变量</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">applyEntry</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">for</span> !rf.killed() &#123;</span><br><span class="line">		rf.applyCond.L.Lock()</span><br><span class="line">		rf.applyCond.Wait()</span><br><span class="line">		rf.applyCond.L.Unlock()</span><br><span class="line">        <span class="comment">//省略：提交代码</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//3.当commitIndex修改时（），唤醒条件变量</span></span><br><span class="line"><span class="comment">//	1. follower收到appenEntries时，有可能修改commitIndex</span></span><br><span class="line"><span class="comment">//	2. leader checkCommit时，有可能修改commitIndex</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">AppendEntries</span><span class="params">(args *AppendEntryArgs, reply *AppendEntryReply)</span></span> &#123;</span><br><span class="line">	<span class="comment">//省略。。。。。。。。。。。。。。。。。。。。。。。。。</span></span><br><span class="line">	<span class="keyword">if</span> rf.lastApplied &lt; rf.commitIndex &#123;</span><br><span class="line">		rf.applyCond.L.Lock()</span><br><span class="line">		rf.applyCond.Signal()</span><br><span class="line">		rf.applyCond.L.Unlock()</span><br><span class="line">	&#125;</span><br><span class="line"><span class="comment">//checkCommit方法不需要等待条件变量，周期性检查</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">checkCommit</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">for</span> !rf.killed() &#123;</span><br><span class="line">        <span class="comment">//等待成为leader</span></span><br><span class="line">		rf.leaderCond.L.Lock()</span><br><span class="line">		rf.leaderCond.Wait()</span><br><span class="line">		rf.leaderCond.L.Unlock()</span><br><span class="line">		<span class="comment">//开始周期性检查，是否能增加commitIndex</span></span><br><span class="line">		<span class="keyword">for</span> !rf.killed() &#123;</span><br><span class="line">			time.Sleep(LEADER_COMMIT_CHECK_INTERVAL * time.Millisecond)</span><br><span class="line">			<span class="keyword">if</span> rf.lastApplied &lt; rf.commitIndex &#123;</span><br><span class="line">				rf.applyCond.L.Lock()</span><br><span class="line">				rf.applyCond.Signal()</span><br><span class="line">				rf.applyCond.L.Unlock()</span><br><span class="line">			&#125;</span><br><span class="line">			rf.mu.Unlock()</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h5><p>日志复制部分要实现的逻辑比较多，也比较复杂，难点主要在于设计好整个发送接收以及提交框架，具体的日志验证、nextIndex维护按照论文中的描述即可</p>
<ol>
<li>日志发送+heartbeat：统一广播接口，heartbeat周期性调用，日志发送响应外部请求调用</li>
<li>日志提交：leader的checkCommit线程推进commitIndex，所有raft结点的applyEntry推进lastapplied</li>
<li>其他实现逻辑：严格按照论文逻辑实现</li>
</ol>
<h4 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h4><p>持久化思路比较简单，在任何修改涉及到持久化属性时，调用持久化方法即可：</p>
<ol>
<li><strong>修改term</strong>:任意RPC请求收到Term大于自己的Term响应时;收到任意RPC请求Term大于自己的Term时</li>
<li><strong>修改log</strong>:AppenEntriesRPC涉及到修改自身log时;Leader被调用start()方法，添加日志时；快照、接收到installsnapshot时</li>
<li><strong>修改voteFor</strong>:收到RequestVoteRPC，并同意投票时;超时切换为Candidate状态时</li>
</ol>
<h4 id="快照"><a href="#快照" class="headerlink" title="快照"></a>快照</h4><p>快照的难点不在于快照本身，而是在于快照导致的<strong>log的index不等于log在LogEntries中的index</strong>，如下图所示，经过快照日志的日志压缩后，raft结点的LogEntries长度从7变为3，导致index为5、6、7的三个日志项在LogEntriesz中的index为1、2、3</p>
<img src="/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/image-20220811144251911.png" class="" title="image-20220811144251911">
<p>针对以上问题以及raft的性质，进行以下设计:</p>
<ol>
<li><strong>本地用伪index</strong>，包括：leader维护的nextIndex和matchIndex、commitIndex和lastApplied</li>
<li><strong>传输转化为真Index</strong>，包括：appenEntry请求和返回index，requestVote请求和返回Index，installSnpshot请求和返回Index</li>
</ol>
<p>确定以上index设计思路后，修改部分原始代码：</p>
<ol>
<li>AppendEntries()中接收到leader的commitIndex（真index）确定commitIndex时,需要转化为logEntries中的index</li>
<li>Snapshot()中接收到leader的commitIndex和lastApplied减去日志压缩的数量</li>
<li>InstallSnapshot()中接收快照<ul>
<li>若commitIndex/lastApplied小于快照的LastIncludedIndex，应直接将commitIndex和lastApplied设置为0</li>
</ul>
</li>
</ol>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1. 情况1</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">AppendEntries</span><span class="params">(args *AppendEntryArgs, reply *AppendEntryReply)</span></span> &#123;</span><br><span class="line">	<span class="comment">//省略。。。。。。。。</span></span><br><span class="line">    <span class="comment">//更新commitIndex(涉及到index转换)</span></span><br><span class="line">	<span class="keyword">if</span> args.LeaderCommit-rf.log[<span class="number">0</span>].Index &gt; rf.commitIndex &#123;</span><br><span class="line">		rf.commitIndex = args.LeaderCommit - rf.log[<span class="number">0</span>].Index</span><br><span class="line">		<span class="keyword">if</span> rf.commitIndex &gt; <span class="built_in">len</span>(rf.log)<span class="number">-1</span> &#123;</span><br><span class="line">			rf.commitIndex = <span class="built_in">len</span>(rf.log) - <span class="number">1</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">    <span class="comment">//省略。。。。。。。。</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//2.情况2</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">Snapshot</span><span class="params">(index <span class="keyword">int</span>, snapshot []<span class="keyword">byte</span>)</span></span> &#123;</span><br><span class="line">	<span class="comment">//省略。。。。。。。。</span></span><br><span class="line">	rf.lastApplied -= index - rf.log[<span class="number">0</span>].Index</span><br><span class="line">	rf.commitIndex -= index - rf.log[<span class="number">0</span>].Index</span><br><span class="line">	<span class="comment">//省略。。。。。。。。</span></span><br><span class="line">  	<span class="comment">//若为leader</span></span><br><span class="line">	<span class="keyword">if</span> rf.peer_status == STATUS_LEADER &#123;</span><br><span class="line">		<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="built_in">len</span>(rf.peers); i++ &#123;</span><br><span class="line">			<span class="keyword">if</span> i != rf.me &#123;</span><br><span class="line">                <span class="comment">//nextIndex移动</span></span><br><span class="line">				rf.nextIndex[i] -= index - rf.log[<span class="number">0</span>].Index</span><br><span class="line">				rf.mathchIndex[i] = rf.nextIndex[i] - <span class="number">1</span></span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">InstallSnapShot</span><span class="params">(args *InstallSnapshotArgs, reply *InstallSnapshotReply)</span></span> &#123;&#123;</span><br><span class="line">	<span class="comment">//省略。。。。。。。。</span></span><br><span class="line">	<span class="comment">//如果lastApplied 或者 commitIndex 小于 args.LastIncludedIndex</span></span><br><span class="line">	<span class="keyword">if</span> rf.commitIndex+rf.log[<span class="number">0</span>].Index &lt; args.LastIncludedIndex </span><br><span class="line">		rf.commitIndex = <span class="number">0</span></span><br><span class="line">		rf.lastApplied = <span class="number">0</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> rf.lastApplied+rf.log[<span class="number">0</span>].Index &lt; args.LastIncludedIndex &#123;</span><br><span class="line">		rf.lastApplied = <span class="number">0</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//省略。。。。。。。。</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>快照相关的方法在确定index的转化后，实现难度并不大，关键代码如下：</p>
<ul>
<li>在broadcastAppendEntry中增加判断,当prelogIndex指向日志在主节点中不存在时，发送InstallSnapshotRPC</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">broadcastAppendEntry</span><span class="params">(isHeartbeat <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line">	<span class="comment">//省略。。。。。。。。</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="built_in">len</span>(rf.peers); i++ &#123;</span><br><span class="line">		<span class="keyword">if</span> i != rf.me &#123;</span><br><span class="line">			<span class="comment">//省略。。。。。。。。</span></span><br><span class="line">			<span class="comment">//判断perlogIndex是否已经存储在日志中（小于伪头entryindex）</span></span><br><span class="line">			<span class="keyword">if</span> rf.nextIndex[i] &lt;= <span class="number">0</span> &#123;</span><br><span class="line">				<span class="comment">//调用发送快照接口</span></span><br><span class="line">				args := InstallSnapshotArgs&#123;Term: rf.currentTerm, LeaderId: rf.me,</span><br><span class="line">					LastIncludedIndex: rf.log[<span class="number">0</span>].Index, LastIncludedTerm: rf.log[<span class="number">0</span>].Term, Data: rf.persister.snapshot&#125;</span><br><span class="line">				reply := InstallSnapshotReply&#123;&#125;</span><br><span class="line">				<span class="keyword">go</span> rf.sendSnapShot(i, &amp;args, &amp;reply)</span><br><span class="line">				</span><br><span class="line">			&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">				<span class="comment">//省略发送日志代码</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//更新发送时间</span></span><br><span class="line">	rf.lastSendTime = time.Now().UnixMilli()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h5><p>快照和持久化类似，代码的实现量并不大，关键在于修改历史代码使得兼容当前操作，这几个index的关系和转化折磨了我很久的时间，有时候debug很久才发现，不是逻辑问题，只是index没有考虑到的问题</p>
<h3 id="遇到的实现问题"><a href="#遇到的实现问题" class="headerlink" title="遇到的实现问题"></a>遇到的实现问题</h3><h5 id="1-不要在占有锁的时候进行通信"><a href="#1-不要在占有锁的时候进行通信" class="headerlink" title="1. 不要在占有锁的时候进行通信"></a>1. 不要在占有锁的时候进行通信</h5><p>通信（RPC,管道等）前应该首先释放锁，因为通信是不可靠的，可能存在延迟返回导致长时间占有锁，系统停顿的问题，两种解决方案</p>
<ol>
<li>先释放锁，再进行通信，或者通信完成再获取锁</li>
<li>启动单独的线程机型通信，主线程继续执行</li>
</ol>
<p>情况1应用较为广泛，如下应用日志的关键代码：</p>
<ul>
<li>修改之前遇到了死锁bug：当前线程占有锁，向管道(applyCh)中写入，但是由于管道已满导致阻塞，测试代码中管道的消费者消费上一条消息调用Snapshot方法要获取锁，两者构成了占有且等待的条件，构成死锁（<del>这个bug折磨死我了，测试几十次出现一次</del>）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">func (rf *Raft) applyEntry() &#123;</span><br><span class="line">	//省略：。。。。。。。。。。。。。。。</span><br><span class="line">	for !rf.killed() &#123;</span><br><span class="line">		rf.mu.Lock()</span><br><span class="line">		//省略：获取发送所需资源</span><br><span class="line">		rf.mu.Unlock()</span><br><span class="line">		//log.Printf(&quot;peer:%v try to applyEntry，释放锁&quot;, rf.me)</span><br><span class="line">		//再发送信息</span><br><span class="line">		for i := 0; i &lt; len(applyEntries); i++ &#123;</span><br><span class="line">			rf.applyCh &lt;- ApplyMsg&#123;CommandValid: true, SnapshotValid: false, Command: applyEntries[i].Command, CommandIndex: applyEntries[i].Index&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		//log.Printf(&quot;peer:%v try to applyEntry，完成发送&quot;, rf.me)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="2-在收到比自己更新（Term更大）的请求-响应时，应立即修改状态，返回请求"><a href="#2-在收到比自己更新（Term更大）的请求-响应时，应立即修改状态，返回请求" class="headerlink" title="2. 在收到比自己更新（Term更大）的请求/响应时，应立即修改状态，返回请求"></a>2. 在收到比自己更新（Term更大）的请求/响应时，应立即修改状态，返回请求</h5><p>上述机制保证了过期的raft结点不会落后太多，如果在某些地方少考虑了这一要求，会出现意想不到的bug</p>
<h5 id="3-Leader只能提交自己任期内的日志（重点）"><a href="#3-Leader只能提交自己任期内的日志（重点）" class="headerlink" title="3. Leader只能提交自己任期内的日志（重点）"></a>3. Leader只能提交自己任期内的日志（重点）</h5><p>这一点在看论文的时候有点难理解，导致在实现过程中容易忘记这一点，如果不按照这一点实现，测试时会出现日志不一致的情况</p>
<h5 id="4-nextIndex会回退，matchIndex不会回退"><a href="#4-nextIndex会回退，matchIndex不会回退" class="headerlink" title="4. nextIndex会回退，matchIndex不会回退"></a>4. nextIndex会回退，matchIndex不会回退</h5><p>这一点结合课程guidance思考了很久才想到，导致了之前一直存在的bug<strong>appendEntries发送端当prelogIndex冲突时只用改nextIndex，不用改matchIndex</strong></p>
<ul>
<li>matchIndex指向已经成功写入log，nextIndex回退不可能小于等于matchIndex</li>
<li>发送appendEnties是之所以会发生prelogIndex冲突，是由于Leader初始化时将nextIndex设置为自己的日志长度</li>
</ul>
<h5 id="5-日志复制请求由于网络问题会存在先发送后到达的情况"><a href="#5-日志复制请求由于网络问题会存在先发送后到达的情况" class="headerlink" title="5.日志复制请求由于网络问题会存在先发送后到达的情况"></a>5.日志复制请求由于网络问题会存在先发送后到达的情况</h5><p>在做实验3时测试发现了这个bug，Leader向follower先发送的日志复制请求反而后到，导致nextIndex不是因为日志冲突回退，而是因为历史请求延迟返回错误回退，之前没有考虑到这个情况</p>
<ul>
<li>例如：连续发送两个prelogIndex=12的appendEntry请求,第一个请求返回成功推进，第二个请求延迟，此时Leader发送prelogIndex=19的appendEntry请求，成功将nextIndex推进到29后，第二个请求返回，又将nextIndex回退到19</li>
<li>上述情况在不不使用快照的情况下不会影响正确性，只会影响系统性能；使用快照后，client在收到prelogIndex=19的appendEntry后第二个情况返回之前可能会进行快照，结果导致<strong>client认为日志提交到了29以后，将29之前的日志均快照压缩，Leader由于延迟请求将nextIndex回退到了19，下次发送perlogIndex=19的日志复制请求时，client端已经不包含perlogIndex=19的日志</strong>。</li>
</ul>
<p>针对以上<strong>历史请求定义为</strong>：存在比当前请求后发送，但是先被接收到/返回的请求。对此解决方案为:</p>
<ol>
<li><p>在follower接收端过滤历史请求</p>
<ul>
<li>无法全部过滤，因为follower无法判断请求是否为历史的，只能通过第一个index过滤一部分</li>
<li>执行历史请求在follower端是不影响正确性的，所以没有采用commitIndex等的更加复杂的判断逻辑</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">//判断是否过时，如果过时直接返回</span></span><br><span class="line"><span class="keyword">if</span> args.Term &lt; rf.currentTerm || args.PreLogIndex &lt; rf.log[<span class="number">0</span>].Index &#123;</span><br><span class="line">	<span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在leader端发送请求返回处理时，过滤历史请求</p>
<ul>
<li>判断term和nextIndex是否为发送时的值，term不同说明发生了重新选举，nextIndex不同说明当前请求返回前，有其他后发出的请求返回，两种情况都应丢弃当前返回结果</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> rf.currentTerm == args.Term &amp;&amp; rf.nextIndex[server]<span class="number">-1</span> &gt;= <span class="number">0</span> &amp;&amp; args.PreLogIndex == rf.log[rf.nextIndex[server]<span class="number">-1</span>].Index &#123;</span><br><span class="line">    <span class="comment">//......</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li>能够直接过滤历史请求的原因，在于新请求的成功执行代表了老请求+新信息的共同成功执行，即<strong>新请求包括了老请求的所有信息</strong></li>
</ul>
<h3 id="测试和总结"><a href="#测试和总结" class="headerlink" title="测试和总结"></a>测试和总结</h3><p>实现不保证不存在bug，在完成代码之后，运行了400次测试用例，全部pass，可以证明整体上的逻辑没有大问题，实验中我得到的收获有以下几点：</p>
<ol>
<li>并发控制很好玩，就是死锁太磨人</li>
<li>解决问题的成就感是遭受折磨的最大回报（<del>如果实验室的项目能给我带来这种成就感，我可能就不会骂他垃圾了，或者说我的水平不足以坚持到能够给我提供成就感的时候</del>）</li>
</ol>
<img src="/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/image-20220821150706818.png" class="" title="image-20220821150706818">
<h2 id="Lab3-基于Raft的KV数据库实现"><a href="#Lab3-基于Raft的KV数据库实现" class="headerlink" title="Lab3:基于Raft的KV数据库实现"></a>Lab3:基于Raft的KV数据库实现</h2><p>实验三的任务是实现一个基于Raft的多副本kv数据库，在能够保证Raft实现正确性的情况下，实现KV数据库不算太难，在实际的调试中，大部分问题来自于Raft之前实现的小bug，说明ab2的测试还是不够完善，经过修改和重新测试后，通过了lab2和lab3的所有测试。</p>
<h3 id="实现架构"><a href="#实现架构" class="headerlink" title="实现架构"></a>实现架构</h3><p>该KV数据库满足典型的客户端-服务器的CS实现架构，客户通过Client向发出读写请求，Server接收并执行Client请求，Raft负责实现副本之间操作顺序的共识，client/server/raft之间的具体结构以及交互关系如<a href="https://pdos.csail.mit.edu/6.824/notes/raft_diagram.pdf">MIT6.824课程资料中的raft_diagram.pdf</a>所示:</p>
<ol>
<li>多个Client，每个Client内部请求逐个发送，即单个Client内不会出现操作并发的情况（这一点很重要，一定程度降低了实现难度）</li>
<li>多个KV Server，每个Server对应一个Raft peer，不同Server存储相同内容，互为备份，通过Raft实现一致状态保证</li>
</ol>
<p>一个典型的写操作流程，可以定义为如下流程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    Client-&gt;&gt;Leader Server: 1.发送写请求</span><br><span class="line">    Leader Server-&gt;&gt;Leader Raft: 2.生成操作日志，发送到Raft层</span><br><span class="line">    Leader Raft -&gt;&gt; Leader Raft: 3.在Raft节点群内备份日志，达到多数后提交日志</span><br><span class="line">    Leader Raft -&gt;&gt; Leader Server: 5.提交日之后，通知server应用操作，修改状态</span><br><span class="line">    Leader Server -&gt;&gt; Client: 5.返回操作执行结果</span><br></pre></td></tr></table></figure>
<p>其中<strong>每个server对应一个raft peer</strong>，<strong>Leader Raft对应Leader Server</strong>:</p>
<h3 id="线性一致性和容错"><a href="#线性一致性和容错" class="headerlink" title="线性一致性和容错"></a>线性一致性和容错</h3><h4 id="读线性一致性"><a href="#读线性一致性" class="headerlink" title="读线性一致性"></a>读线性一致性</h4><p>基于Raft的WAL机制下，写请求自然是满足线性一致性的，但是对于读请求，如果不进行特殊处理，可能会读到过期数据</p>
<ul>
<li><p>例如：当发生网络分区，Client发送请求到了旧Leader，此时新Leader已经执行了部分更新操作，导致旧Leader返回过期数据，导致不满足读写的线性一致性</p>
</li>
<li><p>对于此问题，Raft论文中提出了在响应只读请求之前，与大部分raft peer交互同步，确实自己状态是否已经过期。</p>
<blockquote>
<p>Raft handles this by having the leader exchange heartbeat messages with a majority of the cluster before responding<br>to read-only requests.</p>
</blockquote>
</li>
</ul>
<p>对于raft读操作线性一致性保证，存在其他的效率更高的实现方式（以后进行总结）：</p>
<ol>
<li>Read Index</li>
<li>Lease Read</li>
</ol>
<p>同步的方法性能较差，但是实现起来较为简单，结合以上思路系统读写操作实现为：<strong>所有的读写操作均通过leader进行，在应用到本地状态机之前首先提交Raft日志，日志提交后才进行状态变更</strong></p>
<h4 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h4><blockquote>
<p>To achieve linearizability in Raft, servers must filter out duplicate requests. The basic idea is that servers save the results of client operations and use them to skip executing the same request multiple times. To implement this, each client is given a unique identifier, and clients assign unique serial numbers to every command. Each server’s state machine maintains a session for each client. The session tracks the latest serial number processed for the client, along with the associated response. If a server receives a command whose serial number has already been executed, it responds immediately without re-executing the request.  - raft论文中容错思路</p>
</blockquote>
<p>由于KV层状态机状态以及状态变更基于Raft容错，所以可认为是可靠的，此时需要解决的容错问题其实仅限于请求响应的丢失，即请求成功执行但是客户端未收到响应，针对此问题的容错方案为：</p>
<ol>
<li>Client在未收到正确的响应之前，不断重试发送请求</li>
<li>Server需要对成功执行的请求进行缓存，应对Client的请求重发（Server并不知道一个成功的请求响应是否被Client收到，必须记录）</li>
</ol>
<p>由于每个Client的请求串行执行，上述容错方案可以实现为：</p>
<ol>
<li>每个Client为每个操作编号，一个操作可以通过 Client号+Operation号唯一标识</li>
<li>Server为每个Client缓存最新操作的执行结果，接收到操作时通过缓存判断是否为已执行过操作</li>
</ol>
<p>上述机制存在一个漏洞即某操作对应Raft日志成功提交，但是可能由于Raft共识达成过于缓慢，在应用到状态机之前，Client认为请求执行超时，重新发送请求，此时同一个操作在Raft中对应两条日志项，针对此中情况增加过滤：</p>
<ul>
<li><strong>一条操作可以有多条日志，但是只有一条日志操作会应用到状态机上</strong>了，应用到状态机以后即缓存操作结果</li>
<li>在应用日志时，通过缓存判断是否为已执行过操作</li>
</ul>
<p><strong>小总结：缓存+双重过滤实现了响应丢失的容错，同时避免了重复执行一个相同操作破坏线性一致性</strong></p>
<p>另外由于实验中Client串行发送请求，导致Server只需要存储每个Client最新操作执行结果，如果Client能够并发发送操作请求，则缓存需要基于滑动窗口的方式(来自<a href="https://www.zhihu.com/question/278551592">知乎回答</a>),简单总结加深印象：</p>
<ul>
<li>Server为每个Client缓存可能需要的请求结果窗口:[op_uncheck1,op_uncheck2,……op_latest]</li>
<li>Client请求会携带其确认已经接收最大操作号，导致请求结果窗口左边界推进</li>
<li>Client新请求导致请求结果窗口右边界推进</li>
</ul>
<h3 id="功能实现-1"><a href="#功能实现-1" class="headerlink" title="功能实现"></a>功能实现</h3><p>功能实现部分主要分为两部分进行总结：</p>
<ol>
<li>KV Client 请求发送逻辑</li>
<li>KV Server 请求处理逻辑</li>
</ol>
<h4 id="KV-Client-请求发送逻辑"><a href="#KV-Client-请求发送逻辑" class="headerlink" title="KV Client 请求发送逻辑"></a>KV Client 请求发送逻辑</h4><p>Client由于请求串行执行，请求处理逻辑较为简单，主要是请求结果处理以及请求初始，以PutAppend操作代码为例：</p>
<ul>
<li>初始化请求体是设置操作编号，操作执行成功后才进行操作编号自增</li>
<li>请求server初始随机访问，按照轮询的方式寻找leader，直到操作成功执行</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ck *Clerk)</span> <span class="title">PutAppend</span><span class="params">(key <span class="keyword">string</span>, value <span class="keyword">string</span>, op <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">	<span class="comment">// You will have to modify this function.</span></span><br><span class="line">	args := PutAppendArgs&#123;ClientStamp: ck.clentStamp, OpStamp: ck.opStamp, Key: key, Value: value, Op: op&#125;</span><br><span class="line">	<span class="keyword">for</span> i := ck.leaderServer; ; i = (i + <span class="number">1</span>) % <span class="built_in">len</span>(ck.servers) &#123;</span><br><span class="line">		reply := PutAppendReply&#123;&#125;</span><br><span class="line">		ok := ck.servers[i].Call(<span class="string">&quot;KVServer.PutAppend&quot;</span>, &amp;args, &amp;reply)</span><br><span class="line">		<span class="keyword">if</span> ok &#123;</span><br><span class="line">			<span class="keyword">if</span> reply.Err == ErrTimeOut &#123;</span><br><span class="line">			&#125; <span class="keyword">else</span> <span class="keyword">if</span> reply.Err == ErrWrongLeader &#123;</span><br><span class="line">			&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">				<span class="keyword">if</span> reply.Err == ErrNoKey &#123;</span><br><span class="line">					<span class="keyword">break</span></span><br><span class="line">				&#125;</span><br><span class="line">				<span class="comment">//修改leaderServer</span></span><br><span class="line">				ck.leaderServer = i</span><br><span class="line">				<span class="keyword">break</span></span><br><span class="line">			&#125;</span><br><span class="line">		&#125; </span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//最后操作符加一</span></span><br><span class="line">	ck.opStamp++</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Client在初始化随机设置clientId:</p>
<ul>
<li>采用实验代码中提供的nrand()函数，由于随机数范围较大，出现重叠的概率较小</li>
<li>常用的分布式全局不重复ID生成方法为：DB自增、时间戳、snowflake算法（需要进一步学习）</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">MakeClerk</span><span class="params">(servers []*labrpc.ClientEnd)</span> *<span class="title">Clerk</span></span> &#123;</span><br><span class="line">	ck := <span class="built_in">new</span>(Clerk)</span><br><span class="line">	ck.servers = servers</span><br><span class="line">	<span class="comment">// You&#x27;ll have to add code here.</span></span><br><span class="line">	ck.clentStamp = nrand() <span class="comment">//随机生成当前client编号</span></span><br><span class="line">	ck.opStamp = <span class="number">0</span>          <span class="comment">//操作编号初始化为0</span></span><br><span class="line">	bigx, _ := rand.Int(rand.Reader, big.NewInt(<span class="keyword">int64</span>(<span class="built_in">len</span>(servers))))</span><br><span class="line">	ck.leaderServer = <span class="keyword">int</span>(bigx.Int64()) <span class="comment">//随机初始化当前leader，server</span></span><br><span class="line">	<span class="keyword">return</span> ck</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="KV-Server-处理逻辑"><a href="#KV-Server-处理逻辑" class="headerlink" title="KV Server 处理逻辑"></a>KV Server 处理逻辑</h4><p>Server端在接收请求，应用请求变更、返回请求结果之前，首先写入Raft日志：</p>
<ul>
<li>基本流程：缓存去重-&gt;提交日志-&gt;等待chan通知 <strong>或</strong> 超时-&gt;返回结果</li>
<li>其中接收到chan通知后，需要判断此时操作缓存中操作号是否为当前操作号<ul>
<li>原因：在我的实现中<strong>允许非leader向用户</strong>返回结果，即只要日志提交，且当前server有client在等待返回结果，即通知client</li>
<li>如此设计会导致一个bug：如果一个日志提交后，旧leader返回结果给client后崩溃，新leader此时上线，client向新leader发送新请求，建立通知通道，此时新leader执行历史请求，向新请求通知通道通知，导致<strong>执行历史请求通知了新请求的返回</strong></li>
<li>如何排除此错误：请求返回处理程度接收到通知后，判断缓存中操作是否为历史操作结果，<strong>若为历史操作继续循环等待通知</strong></li>
</ul>
</li>
<li><strong>总结</strong>：是否返回操作结果由<strong>日志是否在超时时间内提交</strong>决定，与leader节点状态是否变化无关（此操作规避了leader变换导致的相同操作日志重复提交问题）</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *KVServer)</span> <span class="title">processOperation</span><span class="params">(op Op)</span> <span class="params">(Err, <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> resultError Err</span><br><span class="line">	resultValue := <span class="string">&quot;&quot;</span></span><br><span class="line">	kv.mu.RLock()</span><br><span class="line">	opRes, ok := kv.opResStore[op.ClientStamp]</span><br><span class="line">	<span class="keyword">if</span> ok &amp;&amp; opRes.OpStamp == op.OpStamp &#123;</span><br><span class="line">		resultError, resultValue = opRes.Err, opRes.Value</span><br><span class="line">		kv.mu.RUnlock()</span><br><span class="line">		<span class="keyword">return</span> resultError, resultValue</span><br><span class="line">	&#125;</span><br><span class="line">	kv.mu.RUnlock()</span><br><span class="line">	kv.mu.Lock()</span><br><span class="line">	_, _, isLeader := kv.rf.Start(op)</span><br><span class="line">	<span class="keyword">if</span> !isLeader &#123;</span><br><span class="line">		kv.mu.Unlock()</span><br><span class="line">		resultError = ErrWrongLeader</span><br><span class="line">		<span class="keyword">return</span> resultError, resultValue</span><br><span class="line">	&#125;</span><br><span class="line">	notifyChan := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>, <span class="number">1</span>)</span><br><span class="line">	kv.notifyChanStore[op.ClientStamp] = notifyChan</span><br><span class="line">	kv.mu.Unlock()</span><br><span class="line">	timeout := time.Now().UnixMilli() + MaxWaitTime</span><br><span class="line">	<span class="keyword">for</span> time.Now().UnixMilli() &lt; timeout &#123;</span><br><span class="line">		<span class="keyword">select</span> &#123;</span><br><span class="line">		<span class="keyword">case</span> opRes := &lt;-notifyChan:</span><br><span class="line">			<span class="keyword">if</span> opRes.OpStamp == op.OpStamp &#123;</span><br><span class="line">				kv.mu.Lock()</span><br><span class="line">				kv.notifyChanStore[op.ClientStamp] = <span class="literal">nil</span></span><br><span class="line">				kv.mu.Unlock()</span><br><span class="line">				resultError = opRes.Err</span><br><span class="line">				resultValue = opRes.Value</span><br><span class="line">				<span class="keyword">return</span> resultError, resultValue</span><br><span class="line">			&#125;</span><br><span class="line">		<span class="keyword">default</span>:</span><br><span class="line">			<span class="comment">//休眠10微妙</span></span><br><span class="line">			time.Sleep(<span class="number">10</span> * time.Millisecond)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	resultError = ErrTimeOut</span><br><span class="line">	kv.mu.Lock()</span><br><span class="line">	kv.notifyChanStore[op.ClientStamp] = <span class="literal">nil</span></span><br><span class="line">	kv.mu.Unlock()</span><br><span class="line">	<span class="keyword">return</span> resultError, resultValue</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Server在初始化时，启动一个读取提交信息的线程，负责在日志提交后将操作应用到状态机上：</p>
<ul>
<li>log应用也<strong>需要过滤重复操作的原因</strong>：在一致性和容错部分描述过，即一条操作可能会有多条日志，但是只有一条日志操作会应用到状态机上</li>
<li><strong>进行快照时机</strong>：每次应用日志时，判断此时raft日志大小是否达到上限</li>
<li><strong>切换快照的时机</strong>：一旦接收到切换快照日志，即进行快照切换（2021版本实验需要用到CondInstallSnapshot函数，而2022版本里推荐不实现该函数，直接返回OK，其中涉及到的同步问题<strong>在raft层解决</strong>，见注意事项<a id="snapshotProblem">1</a>）</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *KVServer)</span> <span class="title">applyCommitLog</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">for</span> m := <span class="keyword">range</span> kv.applyCh &#123;</span><br><span class="line">		<span class="keyword">if</span> m.CommandValid &#123;</span><br><span class="line">			opCommand := m.Command.(Op)</span><br><span class="line">			kv.mu.Lock()</span><br><span class="line">			kv.lastAppliedIndex = m.CommandIndex</span><br><span class="line">			opRes, ok := kv.opResStore[opCommand.ClientStamp]</span><br><span class="line">			<span class="keyword">if</span> !ok || opRes.OpStamp != opCommand.OpStamp &#123;</span><br><span class="line">				notifyChan := kv.notifyChanStore[opCommand.ClientStamp]</span><br><span class="line">				kv.mu.Unlock()</span><br><span class="line">				<span class="keyword">if</span> notifyChan != <span class="literal">nil</span> &#123;</span><br><span class="line">					kv.notifyChanStore[opCommand.ClientStamp] = <span class="literal">nil</span></span><br><span class="line">					notifyChan &lt;- <span class="number">1</span></span><br><span class="line">				&#125;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">				kv.mu.Unlock()</span><br><span class="line">            &#125;</span><br><span class="line">			<span class="keyword">if</span> kv.maxraftstate != <span class="number">-1</span> &amp;&amp; kv.rf.GetLogSize() &gt; kv.maxraftstate &#123;</span><br><span class="line">				kv.snapshotStatus()</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> m.SnapshotValid &#123;</span><br><span class="line">			kv.loadSnapshot(m.SnapshotIndex, m.Snapshot)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="遇到的实现问题-1"><a href="#遇到的实现问题-1" class="headerlink" title="遇到的实现问题"></a>遇到的实现问题</h3><h5 id="1-InstallSnapshot引发的上层快照切换时机同步问题"><a href="#1-InstallSnapshot引发的上层快照切换时机同步问题" class="headerlink" title="1.InstallSnapshot引发的上层快照切换时机同步问题"></a><a href="#snapshotProblem">1</a>.InstallSnapshot引发的上层快照切换时机同步问题</h5><p>做lab4在测试多并发时遇到了这个问题，发现自己并没有仔细思考CondInstallSnapshot这个函数存在的意义，简单的认为2022版本丢弃后，直接返回true不做其他处理即可，导致了kv层切换快照和应用日志的不同步引发的系统不满足线性一致性问题</p>
<p><strong>切换日志同步问题</strong>：调用installSnapshot唤醒上层进行快照切换时，由于将快照切换消息发送到kv层时不占有锁，同时日志commit也在向kv层发送，两者存在争抢问题，即可能由于并发导致顺序出现错误，如下图所示例子</p>
<ol>
<li>正确顺序为raft发送Snapshot:10日志后，发送后续Command:11和12日志</li>
<li>错误情况1：raft层截断日志之后，发送Snapshot:10日志之前，提交日志进程读取新日志，抢先发送Command:11日志</li>
<li>错误情况2：raft层截断日志之前，提交日志进程正在准备发送Command:9-10的日志，此时raft接收到InstallSnapshot，截断日志，抢在提交日志进程发送之前，发送Snapshot:10日志</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">	subgraph 错误顺序2:不需要日志发送</span><br><span class="line">    7[Snapshot:10] --&gt; 8[/Command:9/]--&gt; 9[/Command:10/]</span><br><span class="line">    end</span><br><span class="line">	subgraph 错误顺序1:未来日志提前发</span><br><span class="line">    4[/Command:11/] --&gt; 5[Snapshot:10]--&gt; 6[/Command:12/]</span><br><span class="line">    end</span><br><span class="line">	subgraph 正确顺序:</span><br><span class="line">    1[Snapshot:10] --&gt; 2[/Command:11/]--&gt; 3[/Command:12/]</span><br><span class="line">    end</span><br></pre></td></tr></table></figure>
<p>切换日志同步问题并不一定会导致上层kv层出现状态不一致的问题，分情况讨论：</p>
<ul>
<li><p>错误情况1：会导致过期Snapshot和kv层跳过执行某些日志两种错误</p>
<ol>
<li><p>过期snapshot问题：该问题通过在kv层比较最后应用日志index和快照index来过滤过期快照</p>
</li>
<li><p>kv层跳过执行日志问题：按照上图，raft集群提交到了log:11，然而raft节点由于一定原因只执行到了log:8,此时raft leader向peer节点发送installsnapshot，然而发生了上图情况二，导致peer对应kv层直接从log:8跳到执行log:11，并且在1问题解决措施下，认为Snapshot:10过期并过滤，从而导致kv层漏执行log:9和log10，可能导致<strong>不一致问题</strong></p>
<img src="/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/image-20220904105055292.png" class="" title="image-20220904105055292">
</li>
</ol>
</li>
<li><p>错误情况2：导致kv层收到其认为执行过的历史日志</p>
<ul>
<li><p>通过比较日志应用index和raft层传入日志，过滤由于snapshot导致的“历史日志”</p>
</li>
<li><p>例如上图：在Snapshot:10日志读取后，更新日志应用index到10，之后接收到Command:9-10均认为过期，直接丢弃</p>
<img src="/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/image-20220904104626216.png" class="" title="image-20220904104626216">
</li>
</ul>
</li>
</ul>
<p>综上所属，错误1情况的<strong>跳过执行日志问题</strong>无法在kv层解决，需要在raft层避免情况1的出现，针对此设计一下同步思路：</p>
<ol>
<li><p>installSnapshot在释放锁之前，sendSnapshot标记+1，完成发送后sendSnapshot标记-1</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">//写入applych</span></span><br><span class="line">rf.startSendingSnapshot()</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">    rf.applyCh &lt;- ApplyMsg&#123;CommandValid: <span class="literal">false</span>, SnapshotValid: <span class="literal">true</span>, Snapshot: args.Data,</span><br><span class="line">			SnapshotIndex: args.LastIncludedIndex, SnapshotTerm: args.LastIncludedTerm&#125;</span><br><span class="line">	rf.finishSendingSnapshot()</span><br><span class="line">&#125;()</span><br><span class="line">kv.mu.Unlock()</span><br></pre></td></tr></table></figure>
</li>
<li><p>日志应用端，发送日志之前等待sendSnapshot为0</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> rf.isSendingSnapshot() &#123;</span><br><span class="line">    time.Sleep(<span class="number">5</span> * time.Millisecond)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="built_in">len</span>(applyEntries); i++ &#123;</span><br><span class="line">    rf.applyCh &lt;- ApplyMsg&#123;CommandValid: <span class="literal">true</span>, SnapshotValid: <span class="literal">false</span>, Command: applyEntries[i].Command, CommandIndex: applyEntries[i].Index&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>标记采用原子性操作的整数（没必要使用锁）</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">isSendingSnapshot</span><span class="params">()</span> <span class="title">bool</span></span> &#123;</span><br><span class="line">	z := atomic.LoadInt32(&amp;rf.snapshotMsgSending)</span><br><span class="line">	<span class="keyword">return</span> z == <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">startSendingSnapshot</span><span class="params">()</span></span> &#123;</span><br><span class="line">	atomic.AddInt32(&amp;rf.snapshotMsgSending, <span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">finishSendingSnapshot</span><span class="params">()</span></span> &#123;</span><br><span class="line">	atomic.AddInt32(&amp;rf.snapshotMsgSending, <span class="number">-1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>上述不基于锁实现的同步机制，保证了日志应用端在<strong>确定发送日志之前</strong>执行的InstallSnapshot消息<strong>均能够发送</strong>，问题是<strong>有可能后发生的InstallSnapshot会阻碍与其无关之前日志应用端日志发送</strong>，由于installsnapshot调用频率较低，且执行较快，所以该问题影响不大</p>
<h3 id="测试和总结-1"><a href="#测试和总结-1" class="headerlink" title="测试和总结"></a>测试和总结</h3><p>由于Raft底层保证+单个Client串行执行操作，实现kv服务的难度并不大，主要难点在于请求去重和遗留bug处理，经过三天时间终于写完并且测试通过代码（测试一百轮）。</p>
<img src="/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/image-20220822155923816.png" class="" title="image-20220822155923816">
<img src="/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/image-20220822160313514.png" class="" title="image-20220822160313514">
<h2 id="Lab4-基于Raft的Sharded-KV-数据库实现"><a href="#Lab4-基于Raft的Sharded-KV-数据库实现" class="headerlink" title="Lab4:基于Raft的Sharded KV 数据库实现"></a>Lab4:基于Raft的Sharded KV 数据库实现</h2><p>lab4主要是在原来的kvServer基础上，添加分片(Shard)机制，从而实现一个真正的分布式容错高性能KV数据库，实现过程中的主要难点在于Shard在不同replicate group之间的交互过程。首先系统主要的功能点可以总结为：</p>
<ol>
<li>提供包括<code>put(key, value), append(key, value), get(key)</code>的基本kv数据库功能</li>
<li>基于Raft共识算法的多服务器备份，实现一致性备份存储，实现了系统容错功能</li>
<li>基于Raft日志的WAL机制以及系统快照机制，允许系统在失效后通过日志重新执行、加载快照等，快速恢复数据</li>
<li>通过数据分片和多复制服务器组存储方式，实现了系统的高并发访问性能</li>
<li>支持存储服务器的动态配置，即可以动态的增加删除存储服务器</li>
</ol>
<h3 id="基本实现架构"><a href="#基本实现架构" class="headerlink" title="基本实现架构"></a>基本实现架构</h3><p>如下图所示，系统按照标准的CS架构实现，其中Server端包括一个配置管理集群（Shard Manager）以及多个数据片存储管理集群（KV Server Group）；Client端包括两种类型身份的Client：一种为发送KV数据操作请求的客户端（KV Client）,一种为管理分片信息以及数据片存储集群的客户端（Shard Manage Client）</p>
<ul>
<li>Shard Manager Server负责kv server group、数据分片以及分片到kv server映射信息等系统元数据的管理（类似于HDFS Master）</li>
<li>KV Server Group负责按照分片配置存储对应分片数据以及执行和响应KV客户端操作</li>
</ul>
<img src="/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/image-20220904152037470.png" class="" title="image-20220904152037470">
<p>其中<strong>Shard Manager</strong>和每个<strong>KV Server Group</strong>，通过多服务器备份的方式实现数据的可靠性，具体实现架构如下图所示（以KV Server Group为例）：</p>
<ul>
<li>每个KV Server Group以及ShardManager内包括三个服务器实例，互为备份服务器</li>
<li>通过基于Raft日志的WAL机制，保证不同副本之间的状态一致性以及错误恢复（KV Server中状态机为存储数据片、Shard Manager中状态机为系统shard元数据）</li>
</ul>
<img src="/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/image-20220904161454555.png" class="" title="image-20220904161454555">
<h3 id="分片以及分片分配-sharding"><a href="#分片以及分片分配-sharding" class="headerlink" title="分片以及分片分配(sharding)"></a>分片以及分片分配(sharding)</h3><p>kv数据库中的每个key对相当于关系数据库中表中的条目，且value为单值，区别于关系型数据库条目由多个属性组成，采用Horiziontal Partitioning策略，基于hash的方法对数据进行分片，分片方式如下：</p>
<ul>
<li>shardNum为配置的固定分片数量，确定后即不会改变</li>
</ul>
<script type="math/tex; mode=display">
shardId := hash(key) \% shardNum</script><ul>
<li>确定分片后，根据KV Server Group数量将shard<strong>均匀分配</strong>到KV Server Gruop上，由Shard Manager维护映射关系<script type="math/tex; mode=display">
shadId->GruopId</script></li>
</ul>
<p>上述方法可以总结为：<strong>固定分片策略，动态分配方法</strong>相，优缺点为：</p>
<ul>
<li>优点：当KV Server Group配置改变时，涉及到数据迁移时，以shard为单位进行迁移，较于以key直接映射KV Serve Group(如下公式)，降低了涉及到的数据迁移通信量</li>
<li>缺点：根据key分布进行划分，当key分布不均有或者某些热点key访问量较高时，无法保证不同KV Serve Group之间的负载均衡</li>
</ul>
<script type="math/tex; mode=display">
shardId := hash(key) \% Server Group Num</script><p>未来可以优化的点：</p>
<ul>
<li>采用复合划分Composite partitioning策略即：首先基于哈希方法划分，在根据key的分布规律和请求访问，进行基于列表划分（List Partitionning）的二次细粒度划分</li>
<li>可以基于一致性哈希实现shard-&gt;KV Serve Group的映射管理，降低由于KV Serve Group的增加或者减少shard重分配导致的数据迁移</li>
</ul>
<h4 id="分片分配机制"><a href="#分片分配机制" class="headerlink" title="分片分配机制"></a>分片分配机制</h4><p>Shard Manager作为<script type="math/tex">key->kvServerGroup</script>的配置管理查询服务，支持动态增加/删除存储服务器，相关接口如下</p>
<ul>
<li><code>Join(servers)</code>：批量增加存储服务器组</li>
<li><code>Leave(gruopIds)</code>：批量删除存储服务器组</li>
</ul>
<p>当kvServerGroup配置发生改变时，Shard Manager会重新在剩余可用Gruop进行shard重分配，基本原则如下：</p>
<ul>
<li>保证shard在所有server Group上的<strong>均匀分配</strong>：一部分存储平均数个shard，一部分存储平均数+1个shard</li>
<li>尽量<strong>减少shard迁移</strong>：重新计算平均数，存储shard大于平均数的group移动到小于平均数的group</li>
</ul>
<p>重分配关键代码如下所示：</p>
<ul>
<li>根据group数量，计算平均值<code>averagerShard</code>和余数<code>remindShard</code>，<strong>最终分配结果为</strong>：<code>remindShard</code>个server存储<code>averagerShard + 1</code>个shard，其余group存储<code>averagerShard</code>个shard</li>
<li>分为统计重分配shard和重新分配两个步骤，两步骤思路基本相同</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">//首先统计当前gruop数量</span></span><br><span class="line"><span class="keyword">for</span> key, _ := <span class="keyword">range</span> newConfig.Groups &#123;</span><br><span class="line">	groupShardNumMap[key] = <span class="number">0</span></span><br><span class="line">	allGroupId = <span class="built_in">append</span>(allGroupId, key)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//将shard超过average的日志重新分配</span></span><br><span class="line">reallocateShards := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">0</span>)</span><br><span class="line">averageShard := <span class="built_in">len</span>(newConfig.Shards) / <span class="built_in">len</span>(allGroupId)</span><br><span class="line">remindShard := <span class="built_in">len</span>(newConfig.Shards) % <span class="built_in">len</span>(allGroupId)</span><br><span class="line"><span class="keyword">for</span> shard, group := <span class="keyword">range</span> newConfig.Shards &#123;</span><br><span class="line">	_, ok := newConfig.Groups[group]</span><br><span class="line">	<span class="keyword">if</span> !ok &#123;</span><br><span class="line">		<span class="comment">//shard所属gruop被删除的情况</span></span><br><span class="line">		reallocateShards = <span class="built_in">append</span>(reallocateShards, shard)</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		numShard := groupShardNumMap[group]</span><br><span class="line">		<span class="comment">//如何判断一个group是否超量（关键）：管理shard数量 &gt; averageShard 或者 管理shard数量 == averageShard 且此时可以管理averageShard + 1个shard的机会已经用尽</span></span><br><span class="line">		<span class="keyword">if</span> numShard &gt; averageShard || (numShard == averageShard &amp;&amp; remindShard == <span class="number">0</span>) &#123;</span><br><span class="line">			reallocateShards = <span class="built_in">append</span>(reallocateShards, shard)</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="comment">//如果当前group管理shard数量为averageShard，再分配一个shard，当前group管理了averageShard + 1个shard，则需要占用一个管理averageShard + 1的名额</span></span><br><span class="line">			<span class="keyword">if</span> numShard == averageShard &#123;</span><br><span class="line">				remindShard--</span><br><span class="line">			&#125;</span><br><span class="line">			groupShardNumMap[group] += <span class="number">1</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//开始重新分配</span></span><br><span class="line"><span class="comment">//。。。。。。省略重新分配代码，和上部分差异不大</span></span><br></pre></td></tr></table></figure>
<h3 id="分片迁移实现"><a href="#分片迁移实现" class="headerlink" title="分片迁移实现"></a>分片迁移实现</h3><p>当系统发生Server Gruop的增加或者删除时，会触发shard在不同server之间的变更，虽然整体上思考较为复杂，但是从单个shard迁移的角度考虑，可以将变更过程定义为：多个同时进行的<strong>shard从一个server group 到 另一个server group</strong>的过程，如下图所示：</p>
<ul>
<li>对于每个server group来说，在每个配置变更期，要么有一定量移出的shard，要么有一定量等待移入的shard，要么shard不变</li>
</ul>
<img src="/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/image-20220905112323707.png" class="" title="image-20220905112323707">
<p>在shard迁移过程中，我们必须保证一下几点：</p>
<ol>
<li>shard不能丢失：需要迁出shard的server group只有在确保对方成功接收对应shard后，才能安全删除</li>
<li>shard一旦迁出，不能再提供服务：server group在迁出shard后，不能再服务shard上的数据操作（client端可能为获取到最新配置，导致该问题的出现）</li>
<li>在配置切换过程中，系统能够继续提供服务</li>
</ol>
<p>针对上述问题，为shard定义以下几个状态，迁入迁出过程可以通过状态变更实现：</p>
<ul>
<li><code>Normal</code>：默认正常状态（<strong>正常访问操作</strong>）</li>
<li><code>WaitIn</code>：等待迁入状态（无法访问操作）</li>
<li><code>In</code>：已经迁入，但还向发送端确认状态（<strong>正常访问操作</strong>）</li>
<li><code>out</code>：等待迁出状态（无法访问操作）</li>
<li><code>Delelte</code>：迁出完毕状态，可以进行垃圾回收（无法访问操作）</li>
</ul>
<p>如下图以一个shard迁移过程中的状态变更为例，分析变更流程：</p>
<ul>
<li>shard接收端读取到新配置后，创建空shard，并设置状态为 <code>WaitIn</code></li>
<li>不断地向发送端，拉取shard(<strong>由发送端推也一样，只是选择实现了拉</strong>)，不断重试，直到获取到shard，修改状态： <code>WaitIn -&gt; In</code></li>
<li>完成状态转换后，向发动端发送确认收到RPC，发动端修改shard状态： <code>Out -&gt; Delete</code></li>
<li>接收端不断地发送确认收到RPC，直到RPC请求返回，携带有发送端已经将对应shard状态变更为<code>Delete</code>或者删除的信息后，停止发送，修改状态：<code>In -&gt; Normal</code></li>
<li>当server Group的所有shard状态变为<code>Normal</code>或者<code>Delete</code>时，<strong>完成配置变更</strong></li>
<li>上述所有状态变更操作均首先提交到raft，在<strong>操作日志成功提交后</strong>，执行对应状态变更</li>
</ul>
<img src="/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/image-20220905145204954.png" class="" title="image-20220905145204954">
<p>上述设计思路的原因：</p>
<ol>
<li><p>为什么需要确认消息，才能将对应shard状态变更<code>Out -&gt; Delete</code></p>
<ul>
<li>由于发送端是被拉取方，在接收端发送确认收到消息后，发送端才能保证shard已经发送到接收端且成功存储可以<strong>删除</strong></li>
</ul>
</li>
<li><p>为什么确认收到消息，返回需要携带发动端是否将对应shard状态变更为delete</p>
<ul>
<li>接受端发送确认消息的目的是为了通知发送端自己确认收到shard，接收端需要<strong>保证</strong>发送端收到并且成功记录的自己的确认消息，当发送端shard状态变为delete时，接收端可以确定自己的确认消息成功执行</li>
<li><p>若不按照上述方式执行，可能存在第一次消息确认成功返回，接收端停止发送，但是发送端由于leader切换等，消息确认操作log未成功提交，导致发送端<strong>无限期等待接收端的确认消息</strong></p>
</li>
<li><p>上述设计来源于假设：即使请求成功返回，对应操作不一定成功执行，只有操作结果出现（raft保证操作结果不丢失），才能保证操作执行</p>
</li>
</ul>
</li>
</ol>
<ul>
<li><strong>简单总结</strong>：发送端需要保证接收端接收到才能删除shard-&gt;接收端需要通知发送端自己收到了-&gt;接收端需要保证接收端知道了自己成功接收，才能停止通知</li>
</ul>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>在具体设计实现过程中，并未采用状态变更与通信绑定的操作，即一个线程执行了状态变更后，进行对应发送请求，具体考虑如下：</p>
<ul>
<li>通信可能失败，需要不断重试，状态变更线程不应等待通信，应该继续执行其他操作</li>
<li>状态变更线程由于互斥需要，往往需要持有锁，由于通信的不确定性（延迟、失败），持有锁时进行RPC通信，可能导致系统性能大幅下降</li>
</ul>
<p>综合考虑上述设计问题，采用了状态变更线程+周期性状态检测线程的思路</p>
<ul>
<li>状态变更线程：负责读取raft日志，根据日志中操作变更shard状态</li>
<li>周期性状态检测线程：周期性遍历shard，根据shard状态按照上述交互图，发送消息</li>
</ul>
<p><strong>状态变更线程</strong>代码如下所示：</p>
<ul>
<li>只负责根据日志操作进行状态变更，不负责状态变更后的操作</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">processConfigOp</span><span class="params">(commandInterface <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">	<span class="keyword">switch</span> opCommand := commandInterface.(<span class="keyword">type</span>) &#123;</span><br><span class="line">	<span class="keyword">case</span> ConfigOp:</span><br><span class="line">		<span class="comment">//过滤重复的修改配置操作（因为从写入日志到日志提交存在时间差，可能重复提交日志）</span></span><br><span class="line">		<span class="keyword">if</span> opCommand.Config.Num &gt; kv.config.Num &#123;</span><br><span class="line">            <span class="comment">//省略根据配置信息修改shard状态代码</span></span><br><span class="line">		&#125;</span><br><span class="line">	<span class="keyword">case</span> InShardOp:</span><br><span class="line">		<span class="keyword">if</span> opCommand.ConfigNum == kv.config.Num &#123;</span><br><span class="line">			<span class="comment">//省略添加shard(去重)</span></span><br><span class="line">		&#125;</span><br><span class="line">	<span class="keyword">case</span> DelShardOp:</span><br><span class="line">		<span class="keyword">if</span> opCommand.ConfigNum == kv.config.Num &#123;</span><br><span class="line">			<span class="comment">//省略修改shard状态为out-&gt;delete(需要去重)</span></span><br><span class="line">		&#125;</span><br><span class="line">	<span class="keyword">case</span> AckShardOp:</span><br><span class="line">		<span class="keyword">if</span> opCommand.ConfigNum == kv.config.Num &#123;</span><br><span class="line">			<span class="comment">//省略从in状态修改为normal状态(需要去重)</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>周期性检测线程</strong>代码如下所示：</p>
<ul>
<li>遍历所有shard，启动单独线程负责通信，主线程等待所有通信线程退出</li>
<li>所有通信线程推出后，主线程遍历所有shard，判断是否退出配置切换状态</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">updateShardState</span><span class="params">(updateFunc <span class="keyword">func</span>(<span class="keyword">int</span>, <span class="keyword">int</span>, []<span class="keyword">string</span>)</span>, <span class="title">chaeckStatus</span> <span class="title">string</span>)</span> &#123;</span><br><span class="line">	kv.mu.RLock()</span><br><span class="line">	_, isLeader := kv.rf.GetState()</span><br><span class="line">	<span class="comment">//如果在配置</span></span><br><span class="line">	<span class="keyword">if</span> kv.isConfiging() &amp;&amp; isLeader &#123;</span><br><span class="line">		<span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line">        <span class="comment">//遍历所有shard，根据状态执行对应操作（如：WaitIn状态发起拉取RPC请求，In状态发起确认RPC）</span></span><br><span class="line">		<span class="keyword">for</span> shardId, shard := <span class="keyword">range</span> kv.allShards &#123;</span><br><span class="line">			<span class="keyword">if</span> shard.ShardStatus == chaeckStatus &#123;</span><br><span class="line">				wg.Add(<span class="number">1</span>)</span><br><span class="line">				<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(shardId <span class="keyword">int</span>, configNum <span class="keyword">int</span>, allServers []<span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">					<span class="keyword">defer</span> wg.Done()</span><br><span class="line">					updateFunc(shardId, configNum, allServers)</span><br><span class="line">				&#125;(shardId, kv.config.Num, kv.preConfig.Groups[kv.preConfig.Shards[shardId]])</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//释放锁，并等待</span></span><br><span class="line">		kv.mu.RUnlock()</span><br><span class="line">		wg.Wait()</span><br><span class="line">        <span class="comment">//上锁，判断当前状态是否可以退出配置状态</span></span><br><span class="line">		kv.mu.RLock()</span><br><span class="line">		completeFlag := <span class="literal">true</span></span><br><span class="line">		<span class="keyword">for</span> _, shard := <span class="keyword">range</span> kv.allShards &#123;</span><br><span class="line">			<span class="keyword">if</span> shard.ShardStatus != ShardNormal &amp;&amp; shard.ShardStatus != ShardDelete &#123;</span><br><span class="line">				completeFlag = <span class="literal">false</span></span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		kv.mu.RUnlock()</span><br><span class="line">		<span class="keyword">if</span> completeFlag &#123;</span><br><span class="line">			kv.changeConfigState(<span class="literal">false</span>)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		kv.mu.RUnlock()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于后台存在多个周期性运行函数（状态检测、垃圾回收），抽取一个<strong>公用的周期循环</strong>方法：</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">backRoutine</span><span class="params">(operation <span class="keyword">func</span>()</span>, <span class="title">interval</span> <span class="title">int</span>)</span> &#123;</span><br><span class="line">	<span class="keyword">for</span> !kv.killed() &#123;</span><br><span class="line">		<span class="comment">//执行具体操作</span></span><br><span class="line">		operation()</span><br><span class="line">		time.Sleep(time.Duration(interval) * time.Millisecond)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//传入需要周期运行的方法</span></span><br><span class="line"><span class="keyword">go</span> kv.backRoutine(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123; kv.updateShardState(kv.callMigrateShard, ShardWaitIn) &#125;, UpdateShardInterval)</span><br><span class="line"><span class="comment">//启动确认收到对应shard的线程</span></span><br><span class="line"><span class="keyword">go</span> kv.backRoutine(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123; kv.updateShardState(kv.callMigrateShardAck, ShardIn) &#125;, UpdateShardInterval)</span><br></pre></td></tr></table></figure>
<h4 id="垃圾回收实现"><a href="#垃圾回收实现" class="headerlink" title="垃圾回收实现"></a>垃圾回收实现</h4><p>根据分片迁移实现部分逻辑，仅仅需要回收状态为delete状态的shard，实现逻辑较为简单，采用周期性回收线程的方式，关键代码如下：</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">garbageCollect</span><span class="params">()</span></span> &#123;</span><br><span class="line">	kv.mu.Lock()</span><br><span class="line">	<span class="keyword">for</span> shardId, shard := <span class="keyword">range</span> kv.allShards &#123;</span><br><span class="line">		<span class="keyword">if</span> shard.ShardStatus == ShardDelete &#123;</span><br><span class="line">			<span class="comment">//删除对应状态的shard</span></span><br><span class="line">			<span class="built_in">delete</span>(kv.allShards, shardId)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	kv.mu.Unlock()</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//启动垃圾回收线程</span></span><br><span class="line"><span class="keyword">go</span> kv.backRoutine(kv.garbageCollect, GCInterval)</span><br></pre></td></tr></table></figure>
<h3 id="遇到的实现问题-2"><a href="#遇到的实现问题-2" class="headerlink" title="遇到的实现问题"></a>遇到的实现问题</h3><h5 id="1-同一个类型中的RPC请求-结构体中，由于条件不同请求参数不同，导致大量无用参数"><a href="#1-同一个类型中的RPC请求-结构体中，由于条件不同请求参数不同，导致大量无用参数" class="headerlink" title="1.同一个类型中的RPC请求/结构体中，由于条件不同请求参数不同，导致大量无用参数"></a>1.同一个类型中的RPC请求/结构体中，由于条件不同请求参数不同，导致大量无用参数</h5><p>在实现过程中遇到了一个操作请求对应多种不同操作的情况，不同操作需要携带不同的操作参数，如下代码所示：</p>
<ul>
<li>一种操作对应四种类型的操作，传输其他操作时需要占用其他三种操作参数的空间</li>
</ul>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> ShardOp <span class="keyword">struct</span> &#123;</span><br><span class="line">	<span class="comment">//公共参数</span></span><br><span class="line">	OpType    <span class="keyword">string</span></span><br><span class="line">	ConfigNum <span class="keyword">int</span></span><br><span class="line">	<span class="comment">//修改配置操作参数</span></span><br><span class="line">	Config shardctrler.Config</span><br><span class="line">	<span class="comment">//添加shard操作参数</span></span><br><span class="line">	AddShard <span class="keyword">int</span></span><br><span class="line">	Shard    ShardData</span><br><span class="line">	<span class="comment">//删除操作参数</span></span><br><span class="line">	DelShard <span class="keyword">int</span></span><br><span class="line">	<span class="comment">//确认shard操作参数</span></span><br><span class="line">	AckShard <span class="keyword">int</span> <span class="comment">//迁移进入shard的参数</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>针对以上情况，想出了三种解决方案</p>
<ol>
<li><p>发送不特殊处理，接收端根据opType进行处理（不做处理），缺点是：多余参数占用空间</p>
</li>
<li><p>修改结构体，使用byte[]存储编码后的参数，发送端编码，接收端根据OpType进行解码，缺点：编解码浪费时间，发送接收端需要确定编码顺序</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> ShardOp <span class="keyword">struct</span> &#123;</span><br><span class="line">	<span class="comment">//公共参数</span></span><br><span class="line">	OpType    <span class="keyword">string</span></span><br><span class="line">	ConfigNum <span class="keyword">int</span></span><br><span class="line">	<span class="comment">//修改配置操作参数</span></span><br><span class="line">	parameters []<span class="keyword">byte</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将结构体拆分，传输不同的结构体，在接收端基于golang反射进行操作，缺点：反射的运行效率较低，影响系统运行效率</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> DelShardOp <span class="keyword">struct</span> &#123;</span><br><span class="line">	ConfigNum <span class="keyword">int</span> <span class="comment">//非切换配置需要使用的去重参数</span></span><br><span class="line">	DelShard  <span class="keyword">int</span> <span class="comment">//迁移出删除shard的参数</span></span><br><span class="line"></span><br><span class="line">	AddShard <span class="keyword">int</span>       <span class="comment">//迁移进入shard的参数</span></span><br><span class="line">	Shard    ShardData <span class="comment">//迁移进入shard的参数</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">type</span> InShardOp <span class="keyword">struct</span> &#123;</span><br><span class="line">	ConfigNum <span class="keyword">int</span>       <span class="comment">//非切换配置需要使用的去重参数</span></span><br><span class="line">	AddShard  <span class="keyword">int</span>       <span class="comment">//迁移进入shard的参数</span></span><br><span class="line">	Shard     ShardData <span class="comment">//迁移进入shard的参数</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">type</span> AckShardOp <span class="keyword">struct</span> &#123;</span><br><span class="line">	ConfigNum <span class="keyword">int</span> <span class="comment">//非切换配置需要使用的去重参数</span></span><br><span class="line">	AckShard  <span class="keyword">int</span> <span class="comment">//迁移进入shard的参数</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//接收端执行操作</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kv *ShardKV)</span> <span class="title">processConfigOp</span><span class="params">(commandInterface <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">	<span class="keyword">switch</span> opCommand := commandInterface.(<span class="keyword">type</span>) &#123;</span><br><span class="line">	<span class="keyword">case</span> ConfigOp:</span><br><span class="line">	<span class="keyword">case</span> InShardOp:</span><br><span class="line">	<span class="keyword">case</span> DelShardOp:</span><br><span class="line">	<span class="keyword">case</span> AckShardOp:</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>最后综合考虑<strong>采用第三种方法</strong>，虽然执行效率低，但是实现逻辑上更加清晰，相较于第一种方式减少了空间浪费，降低了网络通信代价</p>
<h3 id="测试与总结"><a href="#测试与总结" class="headerlink" title="测试与总结"></a>测试与总结</h3><p>测试过程按照以下方式进行：</p>
<ul>
<li><p>执行测试脚本，测试200次，每次输出结果写入到文件中</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for ((i = 0; i &lt; 200; i++)); do echo $i; (go test) &gt; ./res/$i; grep -nr &quot;FAIL.*&quot; res;  done</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行完毕，统计通过数量</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line">grep -nr <span class="string">&quot;PASS&quot;</span> res |wc -l</span><br></pre></td></tr></table></figure>
</li>
<li><p>重复执行三轮，共计测试600次</p>
</li>
</ul>
<p>测试结果为：</p>
<ul>
<li>测试所有轮次均通过</li>
</ul>
<img src="/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/image-20220905161251235.png" class="" title="image-20220905161251235">
<ul>
<li>其中一次的测试输出为：</li>
</ul>
<img src="/2022/09/06/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/MIT6.824%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/image-20220905161743796.png" class="" title="image-20220905161743796">
<h5 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h5><p>终于经过了一个多月的视频学习和接近一个月的实验实现，终于完成MIT6.824的学习，现在回看自己的收获可以总结为以下几点：</p>
<ol>
<li>对于分布式系统概念以及涉及到的知识点，有了广泛但不一定深入的了解</li>
<li>掌握了基本golang开发和调试的能力，对于golang的特性和语法有了一定程度的理解</li>
<li>对于并发编程，RPC通信，线程和进程有了更深的理解</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>在实现过程中，由于存在部分知识点理解不够透彻，漏看某些实验条件和实验约束，导致部分实验卡壳，实现过程中参考了部分其他实现方案，具体参考如下：</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/463146544">知乎：MIT6.824-2021 Lab4 : MultiRaft</a> 主要参考了shard封装和状态的思路，并从博主其他博客中了解到了其他可用来帮助加深理解Raft等算法的资料</li>
<li><a href="https://www.cnblogs.com/sun-lingyu/p/14591757.html">博客园：MIT6.824 spring21 Lab2D总结记录</a> 根据博客中快照同步的讲解理解了为什么会需要快照同步，不进行快照同步可能带来的bug</li>
<li><a href="">知乎：MIT6.824_2021_labs</a> 主要和他实现性能进行对比(因为只有他放了结果截图)，基本所有lab实现测试时间小于他的水平（<del>达到心理的满足</del>）</li>
</ol>
<p>除此之外，参考其他的大多是golang相关的问题，四个实验从设计到实现<strong>基本上独立完成</strong>，参考较少，没有进行过代码copy。</p>
]]></content>
      <categories>
        <category>科研学习</category>
        <category>分布式理论</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>MIT6.824</tag>
      </tags>
  </entry>
</search>
